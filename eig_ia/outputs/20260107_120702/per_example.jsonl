{"example_id": "0", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4620790510502866, 0.5379209489497133], "posterior_probs": [0.4620790510502866, 0.5379209489497133], "prior_entropy": 0.690268420337182, "posterior_entropy": 0.690268420337182, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5379209489497133, "accuracy": 1, "tokens_in": 60, "tokens_out": 20, "tokens_total": 80, "latency_total": 0.17460213100002875, "latency_per_module": {"scorer": 0.17460213100002875}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.", "hypotheses": ["Ron ignores his bosses's orders and called him an idiot.", "Ron's boss called him an idiot."]}
{"example_id": "1", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.49074052349518316, 0.5092594765048167], "posterior_probs": [0.49074052349518316, 0.5092594765048167], "prior_entropy": 0.6929756949449963, "posterior_entropy": 0.6929756949449963, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5092594765048167, "accuracy": 1, "tokens_in": 38, "tokens_out": 12, "tokens_total": 50, "latency_total": 0.01035069600038696, "latency_per_module": {"scorer": 0.01035069600038696}, "tokens_per_module": {"scorer": {"tokens_in": 38, "tokens_out": 12, "tokens_total": 50}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sandy lived in New York. Sandy was prepared.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sandy lived in New York. Sandy was prepared.", "hypotheses": ["It stormed in New York.", "She partied all night."]}
{"example_id": "2", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5653405163187792, 0.4346594836812207], "posterior_probs": [0.5653405163187792, 0.4346594836812207], "prior_entropy": 0.6845839433504841, "posterior_entropy": 0.6845839433504841, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5653405163187792, "accuracy": 0, "tokens_in": 68, "tokens_out": 23, "tokens_total": 91, "latency_total": 0.008415792999585392, "latency_per_module": {"scorer": 0.008415792999585392}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 23, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!", "hypotheses": ["Mary and her mom decided to make chocolate covered frozen bananas to avoid waste.", "So Mary made pineapple splits for everyone."]}
{"example_id": "3", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5590702485504583, 0.44092975144954183], "posterior_probs": [0.5590702485504583, 0.44092975144954183], "prior_entropy": 0.6861522671508851, "posterior_entropy": 0.6861522671508851, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5590702485504583, "accuracy": 0, "tokens_in": 52, "tokens_out": 16, "tokens_total": 68, "latency_total": 0.007741903999885835, "latency_per_module": {"scorer": 0.007741903999885835}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jim was working on a project. Luckily, he found it on a nearby shelf.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was working on a project. Luckily, he found it on a nearby shelf.", "hypotheses": ["Jim found he was missing an item.", "Jim needed a certain animal for it."]}
{"example_id": "4", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5075695691361044, 0.49243043086389565], "posterior_probs": [0.5075695691361044, 0.49243043086389565], "prior_entropy": 0.6930325794262594, "posterior_entropy": 0.6930325794262594, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5075695691361044, "accuracy": 0, "tokens_in": 60, "tokens_out": 20, "tokens_total": 80, "latency_total": 0.007827294999515289, "latency_per_module": {"scorer": 0.007827294999515289}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sean was sitting at his desk. After a minute, he was able to put the chair back together.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sean was sitting at his desk. After a minute, he was able to put the chair back together.", "hypotheses": ["He noticed the chair leg was falling off.", "He leaned too far back and his chair tipped over."]}
{"example_id": "5", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6068721153854889, 0.39312788461451104], "posterior_probs": [0.6068721153854889, 0.39312788461451104], "prior_entropy": 0.6701266849595834, "posterior_entropy": 0.6701266849595834, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6068721153854889, "accuracy": 0, "tokens_in": 44, "tokens_out": 21, "tokens_total": 65, "latency_total": 0.007631596999999601, "latency_per_module": {"scorer": 0.007631596999999601}, "tokens_per_module": {"scorer": {"tokens_in": 44, "tokens_out": 21, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Pablo likes to eat worms. Pablo does not enjoy eating worms.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pablo likes to eat worms. Pablo does not enjoy eating worms.", "hypotheses": ["Pablo thought that worms were a delicious source of protein.", "Pablo then learned what worms really are."]}
{"example_id": "6", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5630861110677394, 0.4369138889322606], "posterior_probs": [0.5630861110677394, 0.4369138889322606], "prior_entropy": 0.6851662110791645, "posterior_entropy": 0.6851662110791645, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5630861110677394, "accuracy": 0, "tokens_in": 52, "tokens_out": 19, "tokens_total": 71, "latency_total": 0.008067209000728326, "latency_per_module": {"scorer": 0.008067209000728326}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.", "hypotheses": ["The scientist collected samples of the bacteria and tested them.", "He collected the bacteria and froze it."]}
{"example_id": "7", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6387544124977744, 0.36124558750222546], "posterior_probs": [0.6387544124977744, 0.36124558750222546], "prior_entropy": 0.6541314959287188, "posterior_entropy": 0.6541314959287188, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6387544124977744, "accuracy": 0, "tokens_in": 58, "tokens_out": 21, "tokens_total": 79, "latency_total": 0.007925743000669172, "latency_per_module": {"scorer": 0.007925743000669172}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 21, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I joined the Navy. That angered me so I hit him and was arrested by the military police.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I joined the Navy. That angered me so I hit him and was arrested by the military police.", "hypotheses": ["My commanding officer told me I wasn't doing bad at my job.", "My drill sergeant insulted my mother."]}
{"example_id": "8", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5659128800175401, 0.43408711998245986], "posterior_probs": [0.5659128800175401, 0.43408711998245986], "prior_entropy": 0.6844328221404599, "posterior_entropy": 0.6844328221404599, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5659128800175401, "accuracy": 0, "tokens_in": 46, "tokens_out": 17, "tokens_total": 63, "latency_total": 0.007629953000105161, "latency_per_module": {"scorer": 0.007629953000105161}, "tokens_per_module": {"scorer": {"tokens_in": 46, "tokens_out": 17, "tokens_total": 63}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dotty was being very grumpy. She felt much better afterwards.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dotty was being very grumpy. She felt much better afterwards.", "hypotheses": ["Dotty ate something bad.", "Dotty call some close friends to chat."]}
{"example_id": "9", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5599978275854001, 0.4400021724145999], "posterior_probs": [0.5599978275854001, 0.4400021724145999], "prior_entropy": 0.6859303241447694, "posterior_entropy": 0.6859303241447694, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5599978275854001, "accuracy": 0, "tokens_in": 66, "tokens_out": 17, "tokens_total": 83, "latency_total": 0.007657832999939274, "latency_per_module": {"scorer": 0.007657832999939274}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 17, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.", "hypotheses": ["Ali did not want to take karate.", "Ali did horribly in her last class."]}
{"example_id": "10", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4883579795936534, 0.5116420204063465], "posterior_probs": [0.4883579795936534, 0.5116420204063465], "prior_entropy": 0.6928760827807745, "posterior_entropy": 0.6928760827807745, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5116420204063465, "accuracy": 1, "tokens_in": 64, "tokens_out": 25, "tokens_total": 89, "latency_total": 0.007904561000032118, "latency_per_module": {"scorer": 0.007904561000032118}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 25, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.", "hypotheses": ["Cory was teased by some of the kids in his classroom.", "Cory ran away from home as fast as he could."]}
{"example_id": "11", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5310383771489922, 0.4689616228510078], "posterior_probs": [0.5310383771489922, 0.4689616228510078], "prior_entropy": 0.6912191794642322, "posterior_entropy": 0.6912191794642322, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5310383771489922, "accuracy": 0, "tokens_in": 62, "tokens_out": 18, "tokens_total": 80, "latency_total": 0.007649328000297828, "latency_per_module": {"scorer": 0.007649328000297828}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.", "hypotheses": ["People went to watch the band play.", "Dennis has been a member for ten seconds."]}
{"example_id": "12", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.42302664764035913, 0.5769733523596409], "posterior_probs": [0.42302664764035913, 0.5769733523596409], "prior_entropy": 0.6812501313089362, "posterior_entropy": 0.6812501313089362, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5769733523596409, "accuracy": 1, "tokens_in": 52, "tokens_out": 17, "tokens_total": 69, "latency_total": 0.007064767999509058, "latency_per_module": {"scorer": 0.007064767999509058}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 17, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Deb wanted to go shopping. She found everything she needed and had money left over.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Deb wanted to go shopping. She found everything she needed and had money left over.", "hypotheses": ["Deb went to a matinee movie instead.", "Deb had a lot of coupons."]}
{"example_id": "13", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.617982844061609, 0.38201715593839114], "posterior_probs": [0.617982844061609, 0.38201715593839114], "prior_entropy": 0.6650429923305812, "posterior_entropy": 0.6650429923305812, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.617982844061609, "accuracy": 0, "tokens_in": 54, "tokens_out": 18, "tokens_total": 72, "latency_total": 0.007615651999913098, "latency_per_module": {"scorer": 0.007615651999913098}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My cousin Kory was working at the airport. He is now serving out his sentence.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My cousin Kory was working at the airport. He is now serving out his sentence.", "hypotheses": ["Kory stole from the airport.", "He got caught anti-shoplifting from passengers."]}
{"example_id": "14", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.3696246176351904, 0.6303753823648096], "posterior_probs": [0.3696246176351904, 0.6303753823648096], "prior_entropy": 0.6587555935763323, "posterior_entropy": 0.6587555935763323, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6303753823648096, "accuracy": 1, "tokens_in": 80, "tokens_out": 19, "tokens_total": 99, "latency_total": 0.007796971000061603, "latency_per_module": {"scorer": 0.007796971000061603}, "tokens_per_module": {"scorer": {"tokens_in": 80, "tokens_out": 19, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!", "hypotheses": ["He opened a lemonade stand.", "Daniel stayed home and didn't want to buy a plane."]}
{"example_id": "15", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5233176960502864, 0.47668230394971345], "posterior_probs": [0.5233176960502864, 0.47668230394971345], "prior_entropy": 0.6920593561485264, "posterior_entropy": 0.6920593561485264, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5233176960502864, "accuracy": 0, "tokens_in": 60, "tokens_out": 16, "tokens_total": 76, "latency_total": 0.007566180000139866, "latency_per_module": {"scorer": 0.007566180000139866}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 16, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.", "hypotheses": ["Her neck pain stopped because of this.", "Jenna pulled a muscle lifting weights."]}
{"example_id": "16", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4115371464631088, 0.5884628535368912], "posterior_probs": [0.4115371464631088, 0.5884628535368912], "prior_entropy": 0.6774131328021895, "posterior_entropy": 0.6774131328021895, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5884628535368912, "accuracy": 1, "tokens_in": 56, "tokens_out": 17, "tokens_total": 73, "latency_total": 0.007563400999970327, "latency_per_module": {"scorer": 0.007563400999970327}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.", "hypotheses": ["Kat went to get a salad.", "Kat decided to take a nap instead of eating."]}
{"example_id": "17", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.43587771834479677, 0.5641222816552033], "posterior_probs": [0.43587771834479677, 0.5641222816552033], "prior_entropy": 0.6849011558642406, "posterior_entropy": 0.6849011558642406, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5641222816552033, "accuracy": 1, "tokens_in": 50, "tokens_out": 19, "tokens_total": 69, "latency_total": 0.007472172000234423, "latency_per_module": {"scorer": 0.007472172000234423}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 19, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Cosmo was a pudgy cat. Now he's fit and muscular!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cosmo was a pudgy cat. Now he's fit and muscular!", "hypotheses": ["His owner gave him a lower fat cat food.", "The vet put Cosmo on a treadmill."]}
{"example_id": "18", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4609254882025264, 0.5390745117974737], "posterior_probs": [0.4609254882025264, 0.5390745117974737], "prior_entropy": 0.6900904297666779, "posterior_entropy": 0.6900904297666779, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5390745117974737, "accuracy": 1, "tokens_in": 72, "tokens_out": 14, "tokens_total": 86, "latency_total": 0.007770349000566057, "latency_per_module": {"scorer": 0.007770349000566057}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 14, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.", "hypotheses": ["Tim became very sick one day.", "Tim could not find his socks."]}
{"example_id": "19", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4207589138170007, 0.5792410861829993], "posterior_probs": [0.4207589138170007, 0.5792410861829993], "prior_entropy": 0.6805357754094549, "posterior_entropy": 0.6805357754094549, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5792410861829993, "accuracy": 1, "tokens_in": 50, "tokens_out": 21, "tokens_total": 71, "latency_total": 0.007838559999981953, "latency_per_module": {"scorer": 0.007838559999981953}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 21, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: One day Adam bought two BB guns. Adam took the gun away from Christian.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "One day Adam bought two BB guns. Adam took the gun away from Christian.", "hypotheses": ["Adam's brother Christian was afraid of the guns.", "Christian grabbed the gun and shot Adam in the eye."]}
{"example_id": "20", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.41679212274760574, 0.5832078772523943], "posterior_probs": [0.41679212274760574, 0.5832078772523943], "prior_entropy": 0.6792354461007641, "posterior_entropy": 0.6792354461007641, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5832078772523943, "accuracy": 1, "tokens_in": 48, "tokens_out": 22, "tokens_total": 70, "latency_total": 0.00739196999984415, "latency_per_module": {"scorer": 0.00739196999984415}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 22, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My friend is a hunter. The elk was nowhere to be found.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend is a hunter. The elk was nowhere to be found.", "hypotheses": ["She set up a hunting blind in the woods.", "My friend who is a hunter found lots of elk."]}
{"example_id": "21", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4435230981074238, 0.5564769018925761], "posterior_probs": [0.4435230981074238, 0.5564769018925761], "prior_entropy": 0.6867542648829669, "posterior_entropy": 0.6867542648829669, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5564769018925761, "accuracy": 1, "tokens_in": 42, "tokens_out": 16, "tokens_total": 58, "latency_total": 0.006883158999698935, "latency_per_module": {"scorer": 0.006883158999698935}, "tokens_per_module": {"scorer": {"tokens_in": 42, "tokens_out": 16, "tokens_total": 58}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I walked into my math class. I ended up failing.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I walked into my math class. I ended up failing.", "hypotheses": ["I saw the string by the door.", "I didn't study for the test."]}
{"example_id": "22", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5059038040919378, 0.4940961959080622], "posterior_probs": [0.5059038040919378, 0.4940961959080622], "prior_entropy": 0.693077469132524, "posterior_entropy": 0.693077469132524, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5059038040919378, "accuracy": 0, "tokens_in": 68, "tokens_out": 17, "tokens_total": 85, "latency_total": 0.007411202000184858, "latency_per_module": {"scorer": 0.007411202000184858}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 17, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.", "hypotheses": ["we bought the owners grandmother a new pc.", "Our founder Rachel only uses the PC."]}
{"example_id": "23", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.432870459419034, 0.5671295405809661], "posterior_probs": [0.432870459419034, 0.5671295405809661], "prior_entropy": 0.6841071564298835, "posterior_entropy": 0.6841071564298835, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5671295405809661, "accuracy": 1, "tokens_in": 56, "tokens_out": 16, "tokens_total": 72, "latency_total": 0.007999480999387742, "latency_per_module": {"scorer": 0.007999480999387742}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 16, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary doesn't like cold weather. At least until she can afford to move to warmer state.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary doesn't like cold weather. At least until she can afford to move to warmer state.", "hypotheses": ["Mary wears two jackets.", "It seemed that the cold weather stopped for two months."]}
{"example_id": "24", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5128077854715616, 0.48719221452843847], "posterior_probs": [0.5128077854715616, 0.48719221452843847], "prior_entropy": 0.6928190659326015, "posterior_entropy": 0.6928190659326015, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5128077854715616, "accuracy": 0, "tokens_in": 66, "tokens_out": 45, "tokens_total": 111, "latency_total": 0.007986751000316872, "latency_per_module": {"scorer": 0.007986751000316872}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 45, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.", "hypotheses": ["They started getting followed by a policeman, ran, and hid behind a building.", "The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers."]}
{"example_id": "25", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47422526177721863, 0.5257747382227814], "posterior_probs": [0.47422526177721863, 0.5257747382227814], "prior_entropy": 0.6918179172122214, "posterior_entropy": 0.6918179172122214, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5257747382227814, "accuracy": 1, "tokens_in": 58, "tokens_out": 13, "tokens_total": 71, "latency_total": 0.007366834000094968, "latency_per_module": {"scorer": 0.007366834000094968}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 13, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob's parents grounded him. He came back home but his parents didn't even know he left.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob's parents grounded him. He came back home but his parents didn't even know he left.", "hypotheses": ["Bob got caught sneaking out.", "Bob got away with sneaking out."]}
{"example_id": "26", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.48297515281453285, 0.5170248471854672], "posterior_probs": [0.48297515281453285, 0.5170248471854672], "prior_entropy": 0.6925673776487646, "posterior_entropy": 0.6925673776487646, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5170248471854672, "accuracy": 1, "tokens_in": 62, "tokens_out": 27, "tokens_total": 89, "latency_total": 0.007723390000137442, "latency_per_module": {"scorer": 0.007723390000137442}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 27, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.", "hypotheses": ["Amy won an award for how much work she accomplished and was given the same quota.", "Amy's boss said she needed to do more."]}
{"example_id": "27", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6126871071764336, 0.3873128928235664], "posterior_probs": [0.6126871071764336, 0.3873128928235664], "prior_entropy": 0.6675309227274598, "posterior_entropy": 0.6675309227274598, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6126871071764336, "accuracy": 0, "tokens_in": 64, "tokens_out": 20, "tokens_total": 84, "latency_total": 0.006681262999336468, "latency_per_module": {"scorer": 0.006681262999336468}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 20, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.", "hypotheses": ["He didn't let his inspiration go to waste, he trained and trained.", "Jason learned to knit."]}
{"example_id": "28", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4758078557354764, 0.5241921442645237], "posterior_probs": [0.4758078557354764, 0.5241921442645237], "prior_entropy": 0.6919762037360642, "posterior_entropy": 0.6919762037360642, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5241921442645237, "accuracy": 1, "tokens_in": 48, "tokens_out": 25, "tokens_total": 73, "latency_total": 0.006517284999972617, "latency_per_module": {"scorer": 0.006517284999972617}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 25, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Erin tried to learn how to draw. So she joined a drawing class.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erin tried to learn how to draw. So she joined a drawing class.", "hypotheses": ["Erin, practiced drawing at home with no luck.", "Erin, practiced drawing at home and became recognized for her talent."]}
{"example_id": "29", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47143509427231317, 0.5285649057276869], "posterior_probs": [0.47143509427231317, 0.5285649057276869], "prior_entropy": 0.6915143840109692, "posterior_entropy": 0.6915143840109692, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5285649057276869, "accuracy": 1, "tokens_in": 50, "tokens_out": 15, "tokens_total": 65, "latency_total": 0.006366524999975809, "latency_per_module": {"scorer": 0.006366524999975809}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 15, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jon decided to steal a police car. Jon went to prison for three years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jon decided to steal a police car. Jon went to prison for three years.", "hypotheses": ["Jon crashed the police car into a telephone poll.", "Jon wasn't caught."]}
{"example_id": "30", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47574630971646853, 0.5242536902835314], "posterior_probs": [0.47574630971646853, 0.5242536902835314], "prior_entropy": 0.6919702357676847, "posterior_entropy": 0.6919702357676847, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5242536902835314, "accuracy": 1, "tokens_in": 54, "tokens_out": 18, "tokens_total": 72, "latency_total": 0.006260574999942037, "latency_per_module": {"scorer": 0.006260574999942037}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I used to procrastinate about studying. Now, I never procrastinate studying.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used to procrastinate about studying. Now, I never procrastinate studying.", "hypotheses": ["I failed a big test.", "After getting a good grade, I learned an easy lesson."]}
{"example_id": "31", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.499290943621067, 0.500709056378933], "posterior_probs": [0.499290943621067, 0.500709056378933], "prior_entropy": 0.6931461750357113, "posterior_entropy": 0.6931461750357113, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.500709056378933, "accuracy": 1, "tokens_in": 60, "tokens_out": 15, "tokens_total": 75, "latency_total": 0.0070268749996103, "latency_per_module": {"scorer": 0.0070268749996103}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.", "hypotheses": ["Jacob decided to buy himself a car.", "Jacob couldn't afford a car."]}
{"example_id": "32", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5412382362692145, 0.4587617637307855], "posterior_probs": [0.5412382362692145, 0.4587617637307855], "prior_entropy": 0.6897421297482087, "posterior_entropy": 0.6897421297482087, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5412382362692145, "accuracy": 0, "tokens_in": 60, "tokens_out": 17, "tokens_total": 77, "latency_total": 0.006712473999868962, "latency_per_module": {"scorer": 0.006712473999868962}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.", "hypotheses": ["He yelled at the players for every home run.", "Roy made the other team uncomfortable."]}
{"example_id": "33", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5939922725940271, 0.40600772740597296], "posterior_probs": [0.5939922725940271, 0.40600772740597296], "prior_entropy": 0.6753725208367414, "posterior_entropy": 0.6753725208367414, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5939922725940271, "accuracy": 0, "tokens_in": 66, "tokens_out": 23, "tokens_total": 89, "latency_total": 0.006568431999767199, "latency_per_module": {"scorer": 0.006568431999767199}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 23, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.", "hypotheses": ["Stan was out of school for a week with the stomach ache.", "The school nurse sent Stan home from school."]}
{"example_id": "34", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6792737606353848, 0.32072623936461514], "posterior_probs": [0.6792737606353848, 0.32072623936461514], "prior_entropy": 0.6274156648989593, "posterior_entropy": 0.6274156648989593, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6792737606353848, "accuracy": 0, "tokens_in": 46, "tokens_out": 26, "tokens_total": 72, "latency_total": 0.006544361000123899, "latency_per_module": {"scorer": 0.006544361000123899}, "tokens_per_module": {"scorer": {"tokens_in": 46, "tokens_out": 26, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lisa and Tim had been married for a long time. It worked.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa and Tim had been married for a long time. It worked.", "hypotheses": ["Lisa and Tim went to a fertility clinic to get pregnant.", "They decided to try the advice given in a book about guitar playing."]}
{"example_id": "35", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6086536919959997, 0.3913463080040002], "posterior_probs": [0.6086536919959997, 0.3913463080040002], "prior_entropy": 0.6693464989756049, "posterior_entropy": 0.6693464989756049, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6086536919959997, "accuracy": 0, "tokens_in": 64, "tokens_out": 27, "tokens_total": 91, "latency_total": 0.006658075999439461, "latency_per_module": {"scorer": 0.006658075999439461}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 27, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.", "hypotheses": ["Adam made himself a sandwich using bread, turkey, and a slice of American cheese.", "Adam made himself a pb&j sandwich."]}
{"example_id": "36", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5647430126639348, 0.4352569873360652], "posterior_probs": [0.5647430126639348, 0.4352569873360652], "prior_entropy": 0.6847402799808633, "posterior_entropy": 0.6847402799808633, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5647430126639348, "accuracy": 0, "tokens_in": 44, "tokens_out": 18, "tokens_total": 62, "latency_total": 0.00649929399969551, "latency_per_module": {"scorer": 0.00649929399969551}, "tokens_per_module": {"scorer": {"tokens_in": 44, "tokens_out": 18, "tokens_total": 62}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was painting his fence. Tom left his fence half painted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was painting his fence. Tom left his fence half painted.", "hypotheses": ["Tom got tired of painting after he finished.", "Tom heard a game was on and left."]}
{"example_id": "37", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6175754766923246, 0.3824245233076754], "posterior_probs": [0.6175754766923246, 0.3824245233076754], "prior_entropy": 0.6652385826527232, "posterior_entropy": 0.6652385826527232, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6175754766923246, "accuracy": 0, "tokens_in": 56, "tokens_out": 18, "tokens_total": 74, "latency_total": 0.006893728999784798, "latency_per_module": {"scorer": 0.006893728999784798}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.", "hypotheses": ["She would be with her friends out there.", "Amy wanted to live by the beadch."]}
{"example_id": "38", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.40261986142381756, 0.5973801385761824], "posterior_probs": [0.40261986142381756, 0.5973801385761824], "prior_entropy": 0.6740596404089629, "posterior_entropy": 0.6740596404089629, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5973801385761824, "accuracy": 1, "tokens_in": 70, "tokens_out": 24, "tokens_total": 94, "latency_total": 0.006781531999877188, "latency_per_module": {"scorer": 0.006781531999877188}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 24, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.", "hypotheses": ["Roger overslept and lounged most the day.", "Roger tried but he wasn't as good as his idol."]}
{"example_id": "39", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5156470688694731, 0.484352931130527], "posterior_probs": [0.5156470688694731, 0.484352931130527], "prior_entropy": 0.6926574390754046, "posterior_entropy": 0.6926574390754046, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5156470688694731, "accuracy": 0, "tokens_in": 58, "tokens_out": 22, "tokens_total": 80, "latency_total": 0.006456768999669293, "latency_per_module": {"scorer": 0.006456768999669293}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 22, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.", "hypotheses": ["Barry did not tell anyone that Julie farted.", "Barry laughed at Julie's unzipped pants."]}
{"example_id": "40", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.44667890952075856, 0.5533210904792414], "posterior_probs": [0.44667890952075856, 0.5533210904792414], "prior_entropy": 0.6874500759315252, "posterior_entropy": 0.6874500759315252, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5533210904792414, "accuracy": 1, "tokens_in": 54, "tokens_out": 28, "tokens_total": 82, "latency_total": 0.006638859999839042, "latency_per_module": {"scorer": 0.006638859999839042}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 28, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.", "hypotheses": ["Bob stopped in the middle of the hike because he had no bug spray.", "Bob stopped in the middle of his hike to tie his shoes."]}
{"example_id": "41", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5715470914982475, 0.42845290850175255], "posterior_probs": [0.5715470914982475, 0.42845290850175255], "prior_entropy": 0.6828739799192332, "posterior_entropy": 0.6828739799192332, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5715470914982475, "accuracy": 0, "tokens_in": 70, "tokens_out": 18, "tokens_total": 88, "latency_total": 0.006912429000294651, "latency_per_module": {"scorer": 0.006912429000294651}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 18, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.", "hypotheses": ["Lucy decided to make the pizzas at home.", "Lucy started ordering the pizza."]}
{"example_id": "42", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.49518695439491683, 0.5048130456050831], "posterior_probs": [0.49518695439491683, 0.5048130456050831], "prior_entropy": 0.6931008490264108, "posterior_entropy": 0.6931008490264108, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5048130456050831, "accuracy": 1, "tokens_in": 58, "tokens_out": 18, "tokens_total": 76, "latency_total": 0.006497256999864476, "latency_per_module": {"scorer": 0.006497256999864476}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 18, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was a very windy day. Jim wished he hadn't gone out in his new hat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was a very windy day. Jim wished he hadn't gone out in his new hat.", "hypotheses": ["Jim found his new hat in a storm.", "Jim's hat blew away in the wind."]}
{"example_id": "43", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5889699375363152, 0.4110300624636848], "posterior_probs": [0.5889699375363152, 0.4110300624636848], "prior_entropy": 0.6772312612503355, "posterior_entropy": 0.6772312612503355, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5889699375363152, "accuracy": 0, "tokens_in": 64, "tokens_out": 17, "tokens_total": 81, "latency_total": 0.006611972999962745, "latency_per_module": {"scorer": 0.006611972999962745}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 17, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.", "hypotheses": ["The water was perfect for all levels of fishing.", "The water was spitting up poles."]}
{"example_id": "44", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5439580116016066, 0.4560419883983934], "posterior_probs": [0.5439580116016066, 0.4560419883983934], "prior_entropy": 0.6892775731215821, "posterior_entropy": 0.6892775731215821, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5439580116016066, "accuracy": 0, "tokens_in": 70, "tokens_out": 30, "tokens_total": 100, "latency_total": 0.006982373000028019, "latency_per_module": {"scorer": 0.006982373000028019}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 30, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.", "hypotheses": ["I went to visit her and stepped out onto the balcony of her apartment with a great view.", "I looked down from her balcony to see the clouds."]}
{"example_id": "45", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5420201553935746, 0.45797984460642543], "posterior_probs": [0.5420201553935746, 0.45797984460642543], "prior_entropy": 0.689611624953167, "posterior_entropy": 0.689611624953167, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5420201553935746, "accuracy": 0, "tokens_in": 64, "tokens_out": 19, "tokens_total": 83, "latency_total": 0.0077370039998641005, "latency_per_module": {"scorer": 0.0077370039998641005}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 19, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.", "hypotheses": ["Allister was still a novice at the bow.", "Allister was a pro at the bow."]}
{"example_id": "46", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.36029998877311387, 0.6397000112268861], "posterior_probs": [0.36029998877311387, 0.6397000112268861], "prior_entropy": 0.6535906023014595, "posterior_entropy": 0.6535906023014595, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6397000112268861, "accuracy": 1, "tokens_in": 62, "tokens_out": 18, "tokens_total": 80, "latency_total": 0.007884342000579636, "latency_per_module": {"scorer": 0.007884342000579636}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.", "hypotheses": ["Billy played games and forgot about cleaning until 5PM.", "Billy got home from work early."]}
{"example_id": "47", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5429731534759819, 0.4570268465240181], "posterior_probs": [0.5429731534759819, 0.4570268465240181], "prior_entropy": 0.6894492362019962, "posterior_entropy": 0.6894492362019962, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5429731534759819, "accuracy": 0, "tokens_in": 68, "tokens_out": 19, "tokens_total": 87, "latency_total": 0.0072589669998706086, "latency_per_module": {"scorer": 0.0072589669998706086}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 19, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.", "hypotheses": ["She ended up falling into the river.", "Maya slipped on some rocks and broke her back."]}
{"example_id": "48", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4976706673336924, 0.5023293326663075], "posterior_probs": [0.4976706673336924, 0.5023293326663075], "prior_entropy": 0.6931363289373521, "posterior_entropy": 0.6931363289373521, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5023293326663075, "accuracy": 1, "tokens_in": 52, "tokens_out": 16, "tokens_total": 68, "latency_total": 0.0067663239997273195, "latency_per_module": {"scorer": 0.0067663239997273195}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Susan was making a soup. She did her best to cut away the bad parts.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Susan was making a soup. She did her best to cut away the bad parts.", "hypotheses": ["She had to put in some broth.", "She had to put in some chicken."]}
{"example_id": "49", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5997206215358223, 0.40027937846417777], "posterior_probs": [0.5997206215358223, 0.40027937846417777], "prior_entropy": 0.6731247826300334, "posterior_entropy": 0.6731247826300334, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5997206215358223, "accuracy": 0, "tokens_in": 60, "tokens_out": 32, "tokens_total": 92, "latency_total": 0.0070125540005392395, "latency_per_module": {"scorer": 0.0070125540005392395}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 32, "tokens_total": 92}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.", "hypotheses": ["Lulu's daughter was going to go to school for the first time.", "Lulu's mom was thinking of sending her to a new house despite her objections."]}
{"example_id": "50", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4704830929730231, 0.529516907026977], "posterior_probs": [0.4704830929730231, 0.529516907026977], "prior_entropy": 0.6914036714465945, "posterior_entropy": 0.6914036714465945, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.529516907026977, "accuracy": 1, "tokens_in": 54, "tokens_out": 20, "tokens_total": 74, "latency_total": 0.0067390989997875295, "latency_per_module": {"scorer": 0.0067390989997875295}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I wanted to create a video game. Indeed, Java was terrible for programming video games.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I wanted to create a video game. Indeed, Java was terrible for programming video games.", "hypotheses": ["I programmed with Java for robot game.", "I programmed with Java for robot game because it was easy."]}
{"example_id": "51", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5536741528365954, 0.4463258471634046], "posterior_probs": [0.5536741528365954, 0.4463258471634046], "prior_entropy": 0.6873742336400734, "posterior_entropy": 0.6873742336400734, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5536741528365954, "accuracy": 0, "tokens_in": 70, "tokens_out": 16, "tokens_total": 86, "latency_total": 0.0065611250001893495, "latency_per_module": {"scorer": 0.0065611250001893495}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 16, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.", "hypotheses": ["My best friend visited me on a vacation.", "I went with her to celebrate."]}
{"example_id": "52", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6080432574413293, 0.39195674255867086], "posterior_probs": [0.6080432574413293, 0.39195674255867086], "prior_entropy": 0.6696153193660608, "posterior_entropy": 0.6696153193660608, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6080432574413293, "accuracy": 0, "tokens_in": 60, "tokens_out": 17, "tokens_total": 77, "latency_total": 0.006611253999835753, "latency_per_module": {"scorer": 0.006611253999835753}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.", "hypotheses": ["Tom was an elitist.", "Tom bought costly ones but they broke right away."]}
{"example_id": "53", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.531998472862934, 0.46800152713706594], "posterior_probs": [0.531998472862934, 0.46800152713706594], "prior_entropy": 0.691097975897329, "posterior_entropy": 0.691097975897329, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.531998472862934, "accuracy": 0, "tokens_in": 72, "tokens_out": 25, "tokens_total": 97, "latency_total": 0.00686001799931546, "latency_per_module": {"scorer": 0.00686001799931546}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 25, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.", "hypotheses": ["At the store, Kaya saw a very beautiful vase.", "Kaya could not find a single thing at the store,."]}
{"example_id": "54", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5717724395920868, 0.4282275604079132], "posterior_probs": [0.5717724395920868, 0.4282275604079132], "prior_entropy": 0.6828089385588626, "posterior_entropy": 0.6828089385588626, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5717724395920868, "accuracy": 0, "tokens_in": 66, "tokens_out": 18, "tokens_total": 84, "latency_total": 0.006711403000736027, "latency_per_module": {"scorer": 0.006711403000736027}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 18, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.", "hypotheses": ["Sam put a towel under the leaky fridge.", "Sam dishwasher broke and was leaking."]}
{"example_id": "55", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.46141858598069874, 0.5385814140193013], "posterior_probs": [0.46141858598069874, 0.5385814140193013], "prior_entropy": 0.6901671682063506, "posterior_entropy": 0.6901671682063506, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5385814140193013, "accuracy": 1, "tokens_in": 52, "tokens_out": 26, "tokens_total": 78, "latency_total": 0.006701219000206038, "latency_per_module": {"scorer": 0.006701219000206038}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 26, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Anna had a bad headache. Thankfully, when she awoke, the headache was gone.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Anna had a bad headache. Thankfully, when she awoke, the headache was gone.", "hypotheses": ["Anna took a few asprins and laid down and took a nap.", "She went to a concert to get rid of it."]}
{"example_id": "56", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5567255632063237, 0.44327443679367623], "posterior_probs": [0.5567255632063237, 0.44327443679367623], "prior_entropy": 0.6866977243851222, "posterior_entropy": 0.6866977243851222, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5567255632063237, "accuracy": 0, "tokens_in": 50, "tokens_out": 20, "tokens_total": 70, "latency_total": 0.006217573999492743, "latency_per_module": {"scorer": 0.006217573999492743}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 20, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Last night I had a dream about biking. I woke up, bitterly disappointed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Last night I had a dream about biking. I woke up, bitterly disappointed.", "hypotheses": ["my dream was very real and I was on a fun bike tour.", "I actually have a bike."]}
{"example_id": "57", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4345182363492293, 0.5654817636507706], "posterior_probs": [0.4345182363492293, 0.5654817636507706], "prior_entropy": 0.6845467737315923, "posterior_entropy": 0.6845467737315923, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5654817636507706, "accuracy": 1, "tokens_in": 56, "tokens_out": 23, "tokens_total": 79, "latency_total": 0.006541046999700484, "latency_per_module": {"scorer": 0.006541046999700484}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 23, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was starting to get late outside. Her parents grounded her for a week for being late.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was starting to get late outside. Her parents grounded her for a week for being late.", "hypotheses": ["She forgot what time it was and was home late.", "She was supposed to be home an hour after she arrived."]}
{"example_id": "58", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.574938188784644, 0.425061811215356], "posterior_probs": [0.574938188784644, 0.425061811215356], "prior_entropy": 0.6818732852598134, "posterior_entropy": 0.6818732852598134, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.574938188784644, "accuracy": 0, "tokens_in": 60, "tokens_out": 17, "tokens_total": 77, "latency_total": 0.006693784999697527, "latency_per_module": {"scorer": 0.006693784999697527}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.", "hypotheses": ["Kim needed more money than she could get.", "Kim applied for jobs to make money."]}
{"example_id": "59", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.49453006156178386, 0.5054699384382161], "posterior_probs": [0.49453006156178386, 0.5054699384382161], "prior_entropy": 0.6930873389112259, "posterior_entropy": 0.6930873389112259, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5054699384382161, "accuracy": 1, "tokens_in": 56, "tokens_out": 18, "tokens_total": 74, "latency_total": 0.006692030000522209, "latency_per_module": {"scorer": 0.006692030000522209}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!", "hypotheses": ["Joe had plenty of time to cook something.", "Joe didn't have time to cook something."]}
{"example_id": "60", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4444638647440158, 0.5555361352559842], "posterior_probs": [0.4444638647440158, 0.5555361352559842], "prior_entropy": 0.6869659093462148, "posterior_entropy": 0.6869659093462148, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5555361352559842, "accuracy": 1, "tokens_in": 52, "tokens_out": 20, "tokens_total": 72, "latency_total": 0.006587556999875233, "latency_per_module": {"scorer": 0.006587556999875233}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.", "hypotheses": ["He went to the near by super market.", "Tim looked for a long time in the messy fridge."]}
{"example_id": "61", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6524055812220552, 0.3475944187779449], "posterior_probs": [0.6524055812220552, 0.3475944187779449], "prior_entropy": 0.6459447581254353, "posterior_entropy": 0.6459447581254353, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6524055812220552, "accuracy": 0, "tokens_in": 60, "tokens_out": 16, "tokens_total": 76, "latency_total": 0.0064757350000945735, "latency_per_module": {"scorer": 0.0064757350000945735}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 16, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.", "hypotheses": ["The rap tape was mean and rude.", "Gina  take their new tape."]}
{"example_id": "62", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5428062460011205, 0.45719375399887957], "posterior_probs": [0.5428062460011205, 0.45719375399887957], "prior_entropy": 0.6894779411916492, "posterior_entropy": 0.6894779411916492, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5428062460011205, "accuracy": 0, "tokens_in": 54, "tokens_out": 16, "tokens_total": 70, "latency_total": 0.006298901000263868, "latency_per_module": {"scorer": 0.006298901000263868}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 16, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.", "hypotheses": ["We ordered a dessert everyone would like.", "We ordered appetizers everyone would like."]}
{"example_id": "63", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5766461630064038, 0.4233538369935963], "posterior_probs": [0.5766461630064038, 0.4233538369935963], "prior_entropy": 0.6813514588057896, "posterior_entropy": 0.6813514588057896, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5766461630064038, "accuracy": 0, "tokens_in": 54, "tokens_out": 21, "tokens_total": 75, "latency_total": 0.006584552999811422, "latency_per_module": {"scorer": 0.006584552999811422}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 21, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ora had always been overweight. With their help, Ora lost over twenty pounds!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora had always been overweight. With their help, Ora lost over twenty pounds!", "hypotheses": ["Ora decided to eat healthy for a month.", "Ora decided she wanted to maintainer her weight."]}
{"example_id": "64", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4374002057690711, 0.562599794230929], "posterior_probs": [0.4374002057690711, 0.562599794230929], "prior_entropy": 0.6852891073100924, "posterior_entropy": 0.6852891073100924, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.562599794230929, "accuracy": 1, "tokens_in": 52, "tokens_out": 19, "tokens_total": 71, "latency_total": 0.006294666999565379, "latency_per_module": {"scorer": 0.006294666999565379}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Priya decided to try a new restaurant. Priya thought her food was delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Priya decided to try a new restaurant. Priya thought her food was delicious.", "hypotheses": ["She ordered two shrimp dishes.", "The food that Priya ordered was microwaved and precooked."]}
{"example_id": "65", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.43257818279028354, 0.5674218172097164], "posterior_probs": [0.43257818279028354, 0.5674218172097164], "prior_entropy": 0.6840280241266522, "posterior_entropy": 0.6840280241266522, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5674218172097164, "accuracy": 1, "tokens_in": 54, "tokens_out": 23, "tokens_total": 77, "latency_total": 0.006582190999324666, "latency_per_module": {"scorer": 0.006582190999324666}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 23, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jamie and Candice were going on a date. Finally, they settled on ice cream!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jamie and Candice were going on a date. Finally, they settled on ice cream!", "hypotheses": ["Jamie and Candice did not know what movie to see.", "Jamie and Candice couldn't decide what to do."]}
{"example_id": "66", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6046598668084326, 0.39534013319156736], "posterior_probs": [0.6046598668084326, 0.39534013319156736], "prior_entropy": 0.6710769560047052, "posterior_entropy": 0.6710769560047052, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6046598668084326, "accuracy": 0, "tokens_in": 74, "tokens_out": 27, "tokens_total": 101, "latency_total": 0.006984867000028316, "latency_per_module": {"scorer": 0.006984867000028316}, "tokens_per_module": {"scorer": {"tokens_in": 74, "tokens_out": 27, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.", "hypotheses": ["Scott found a job in New York.", "The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living."]}
{"example_id": "67", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.42338107670920266, 0.5766189232907972], "posterior_probs": [0.42338107670920266, 0.5766189232907972], "prior_entropy": 0.6813598749171441, "posterior_entropy": 0.6813598749171441, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5766189232907972, "accuracy": 1, "tokens_in": 60, "tokens_out": 15, "tokens_total": 75, "latency_total": 0.006523698999444605, "latency_per_module": {"scorer": 0.006523698999444605}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I went to the store one day to buy clothes. I went home and the jeans fit much better.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I went to the store one day to buy clothes. I went home and the jeans fit much better.", "hypotheses": ["I got larger sizes.", "I bought jeans thought they were a bit expensive."]}
{"example_id": "68", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5093751289457198, 0.49062487105428026], "posterior_probs": [0.5093751289457198, 0.49062487105428026], "prior_entropy": 0.6929713841707495, "posterior_entropy": 0.6929713841707495, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5093751289457198, "accuracy": 0, "tokens_in": 68, "tokens_out": 27, "tokens_total": 95, "latency_total": 0.006712090000291937, "latency_per_module": {"scorer": 0.006712090000291937}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 27, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.", "hypotheses": ["They ran into a cute friend of Sam's on the way to dinner.", "The date went bad, they went home on good terms."]}
{"example_id": "69", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5997088313230708, 0.4002911686769291], "posterior_probs": [0.5997088313230708, 0.4002911686769291], "prior_entropy": 0.6731295491372684, "posterior_entropy": 0.6731295491372684, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5997088313230708, "accuracy": 0, "tokens_in": 70, "tokens_out": 21, "tokens_total": 91, "latency_total": 0.006654572000115877, "latency_per_module": {"scorer": 0.006654572000115877}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 21, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.", "hypotheses": ["Pam tried her best to be as honest as possible.", "Pam wasted time with doing her surveys."]}
{"example_id": "70", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4946844792302691, 0.505315520769731], "posterior_probs": [0.4946844792302691, 0.505315520769731], "prior_entropy": 0.6930906699713483, "posterior_entropy": 0.6930906699713483, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.505315520769731, "accuracy": 1, "tokens_in": 48, "tokens_out": 22, "tokens_total": 70, "latency_total": 0.006567635999999766, "latency_per_module": {"scorer": 0.006567635999999766}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 22, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lydia had a strange dream last night. Lydia wished the dream were real.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lydia had a strange dream last night. Lydia wished the dream were real.", "hypotheses": ["In Lydia's dream, she was poor and lonely.", "In Lydia's dream, she was rich and famous."]}
{"example_id": "71", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.486309255929223, 0.5136907440707771], "posterior_probs": [0.486309255929223, 0.5136907440707771], "prior_entropy": 0.6927722607542264, "posterior_entropy": 0.6927722607542264, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5136907440707771, "accuracy": 1, "tokens_in": 50, "tokens_out": 21, "tokens_total": 71, "latency_total": 0.006515660999866668, "latency_per_module": {"scorer": 0.006515660999866668}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 21, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dana always wanted to ride a bike. They were riding around together within minutes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dana always wanted to ride a bike. They were riding around together within minutes.", "hypotheses": ["Dana asked a neighbor to ride with her.", "Her friend asked Dana to teach her how to ride."]}
{"example_id": "72", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5442938108983753, 0.4557061891016247], "posterior_probs": [0.5442938108983753, 0.4557061891016247], "prior_entropy": 0.6892181487243876, "posterior_entropy": 0.6892181487243876, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5442938108983753, "accuracy": 0, "tokens_in": 50, "tokens_out": 18, "tokens_total": 68, "latency_total": 0.006105405000198516, "latency_per_module": {"scorer": 0.006105405000198516}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 18, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.", "hypotheses": ["Kelly tried out for the soccer team but was cut.", "Kelly made it onto the team."]}
{"example_id": "73", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4592465255863792, 0.5407534744136208], "posterior_probs": [0.4592465255863792, 0.5407534744136208], "prior_entropy": 0.6898218015182185, "posterior_entropy": 0.6898218015182185, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5407534744136208, "accuracy": 1, "tokens_in": 56, "tokens_out": 18, "tokens_total": 74, "latency_total": 0.006469250999543874, "latency_per_module": {"scorer": 0.006469250999543874}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.", "hypotheses": ["Tom didn't check the grills gas.", "Tom didn't check the grill's safety."]}
{"example_id": "74", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.33492383062893677, 0.6650761693710633], "posterior_probs": [0.33492383062893677, 0.6650761693710633], "prior_entropy": 0.6376109297354657, "posterior_entropy": 0.6376109297354657, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6650761693710633, "accuracy": 1, "tokens_in": 54, "tokens_out": 18, "tokens_total": 72, "latency_total": 0.006612954000047466, "latency_per_module": {"scorer": 0.006612954000047466}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Martin went to camp with his friends. Martin caught many fish so that everyone could eat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martin went to camp with his friends. Martin caught many fish so that everyone could eat.", "hypotheses": ["Martins friends went fishing without martin.", "He was the best fisherman of the group."]}
{"example_id": "75", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.44405487857142134, 0.5559451214285787], "posterior_probs": [0.44405487857142134, 0.5559451214285787], "prior_entropy": 0.6868743401477769, "posterior_entropy": 0.6868743401477769, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5559451214285787, "accuracy": 1, "tokens_in": 60, "tokens_out": 36, "tokens_total": 96, "latency_total": 0.006876130999444285, "latency_per_module": {"scorer": 0.006876130999444285}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 36, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.", "hypotheses": ["After the car wash Sam noticed his car seats were all soaking wet.", "He rolled up all his windows and began daydreaming of how well his car would look after the wash."]}
{"example_id": "76", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4737475754119093, 0.5262524245880907], "posterior_probs": [0.4737475754119093, 0.5262524245880907], "prior_entropy": 0.6917681669549507, "posterior_entropy": 0.6917681669549507, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5262524245880907, "accuracy": 1, "tokens_in": 76, "tokens_out": 15, "tokens_total": 91, "latency_total": 0.006764915000530891, "latency_per_module": {"scorer": 0.006764915000530891}, "tokens_per_module": {"scorer": {"tokens_in": 76, "tokens_out": 15, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!", "hypotheses": ["May sent out tweets looking for tickets.", "So she bought 2 tickets online."]}
{"example_id": "77", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.486044932696252, 0.5139550673037481], "posterior_probs": [0.486044932696252, 0.5139550673037481], "prior_entropy": 0.692757642168361, "posterior_entropy": 0.692757642168361, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5139550673037481, "accuracy": 1, "tokens_in": 60, "tokens_out": 23, "tokens_total": 83, "latency_total": 0.006750391000423406, "latency_per_module": {"scorer": 0.006750391000423406}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 23, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mindy decided to go over jake's house. She panicked and ran screaming out of his house.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mindy decided to go over jake's house. She panicked and ran screaming out of his house.", "hypotheses": ["When Mindy walked in the door, Jake was naked.", "Mindy scared Jake when she came into the house."]}
{"example_id": "78", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4790031487503871, 0.5209968512496128], "posterior_probs": [0.4790031487503871, 0.5209968512496128], "prior_entropy": 0.6922651856976607, "posterior_entropy": 0.6922651856976607, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5209968512496128, "accuracy": 1, "tokens_in": 56, "tokens_out": 14, "tokens_total": 70, "latency_total": 0.006503214999611373, "latency_per_module": {"scorer": 0.006503214999611373}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 14, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was having trouble finding my comfortable slippers. Finders keepers, she told me.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was having trouble finding my comfortable slippers. Finders keepers, she told me.", "hypotheses": ["The dog stole the slippers.", "My sister stole my slippers."]}
{"example_id": "79", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.44394140244086727, 0.5560585975591328], "posterior_probs": [0.44394140244086727, 0.5560585975591328], "prior_entropy": 0.6868488135486308, "posterior_entropy": 0.6868488135486308, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5560585975591328, "accuracy": 1, "tokens_in": 70, "tokens_out": 26, "tokens_total": 96, "latency_total": 0.00700577899988275, "latency_per_module": {"scorer": 0.00700577899988275}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 26, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.", "hypotheses": ["I used a marker to paint them bright pink.", "I wanted to have normal lips. I painted them red and people liked it."]}
{"example_id": "80", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4492307602125571, 0.5507692397874429], "posterior_probs": [0.4492307602125571, 0.5507692397874429], "prior_entropy": 0.6879832542902835, "posterior_entropy": 0.6879832542902835, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5507692397874429, "accuracy": 1, "tokens_in": 48, "tokens_out": 15, "tokens_total": 63, "latency_total": 0.007190516000264324, "latency_per_module": {"scorer": 0.007190516000264324}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 15, "tokens_total": 63}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A family went shopping together. The father bought the boy a new computer.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A family went shopping together. The father bought the boy a new computer.", "hypotheses": ["Their son was very well behaved.", "The boy needed a new cell phone."]}
{"example_id": "81", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47648300690444706, 0.5235169930955529], "posterior_probs": [0.47648300690444706, 0.5235169930955529], "prior_entropy": 0.6920406744505931, "posterior_entropy": 0.6920406744505931, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5235169930955529, "accuracy": 1, "tokens_in": 58, "tokens_out": 28, "tokens_total": 86, "latency_total": 0.0069649970000682515, "latency_per_module": {"scorer": 0.0069649970000682515}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 28, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was working on my laptop one day. After paying the bill, I no longer experienced issues.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was working on my laptop one day. After paying the bill, I no longer experienced issues.", "hypotheses": ["i got an e-mail saying my cable bill was current and service would be upgraded.", "The internet was very slow and then stopped completely."]}
{"example_id": "82", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4668909333315746, 0.5331090666684254], "posterior_probs": [0.4668909333315746, 0.5331090666684254], "prior_entropy": 0.6909531549137811, "posterior_entropy": 0.6909531549137811, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5331090666684254, "accuracy": 1, "tokens_in": 64, "tokens_out": 18, "tokens_total": 82, "latency_total": 0.006627485000535671, "latency_per_module": {"scorer": 0.006627485000535671}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 18, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!", "hypotheses": ["Isa went to the country site and found some flowers.", "Isa forgot about the bouquet."]}
{"example_id": "83", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4949569028203426, 0.5050430971796575], "posterior_probs": [0.4949569028203426, 0.5050430971796575], "prior_entropy": 0.6930963140371449, "posterior_entropy": 0.6930963140371449, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5050430971796575, "accuracy": 1, "tokens_in": 76, "tokens_out": 22, "tokens_total": 98, "latency_total": 0.008114143000057084, "latency_per_module": {"scorer": 0.008114143000057084}, "tokens_per_module": {"scorer": {"tokens_in": 76, "tokens_out": 22, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.", "hypotheses": ["People destroyed the dam the beaver was building.", "Everyone was all over the hotel, trying to see him."]}
{"example_id": "84", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.48274284883258106, 0.5172571511674189], "posterior_probs": [0.48274284883258106, 0.5172571511674189], "prior_entropy": 0.6925514437149196, "posterior_entropy": 0.6925514437149196, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5172571511674189, "accuracy": 1, "tokens_in": 52, "tokens_out": 20, "tokens_total": 72, "latency_total": 0.007401400999697216, "latency_per_module": {"scorer": 0.007401400999697216}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My family was on vacation in Italy. It was our favorite picture of the vacation.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My family was on vacation in Italy. It was our favorite picture of the vacation.", "hypotheses": ["We took a beautiful picture of Spain.", "We took a photo next to the coliseum in Rome."]}
{"example_id": "85", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5220601301779403, 0.47793986982205966], "posterior_probs": [0.5220601301779403, 0.47793986982205966], "prior_entropy": 0.6921735658547711, "posterior_entropy": 0.6921735658547711, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5220601301779403, "accuracy": 0, "tokens_in": 64, "tokens_out": 17, "tokens_total": 81, "latency_total": 0.007042818999252631, "latency_per_module": {"scorer": 0.007042818999252631}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 17, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.", "hypotheses": ["bill said he would be fine and left.", "Bill went to Lansing via air anyways."]}
{"example_id": "86", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4965328533869331, 0.5034671466130668], "posterior_probs": [0.4965328533869331, 0.5034671466130668], "prior_entropy": 0.6931231381539927, "posterior_entropy": 0.6931231381539927, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5034671466130668, "accuracy": 1, "tokens_in": 48, "tokens_out": 14, "tokens_total": 62, "latency_total": 0.006242815999939921, "latency_per_module": {"scorer": 0.006242815999939921}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 14, "tokens_total": 62}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!", "hypotheses": ["Liv's mother signed her up.", "She was not very talented."]}
{"example_id": "87", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.48536694808139563, 0.5146330519186044], "posterior_probs": [0.48536694808139563, 0.5146330519186044], "prior_entropy": 0.6927188669867086, "posterior_entropy": 0.6927188669867086, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5146330519186044, "accuracy": 1, "tokens_in": 70, "tokens_out": 28, "tokens_total": 98, "latency_total": 0.006806918000620499, "latency_per_module": {"scorer": 0.006806918000620499}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 28, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!", "hypotheses": ["One day a nurse said they should go to the coffee shop for a treat.", "The baseball player looked out the window at a coffee shop."]}
{"example_id": "88", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.500141024585799, 0.49985897541420105], "posterior_probs": [0.500141024585799, 0.49985897541420105], "prior_entropy": 0.6931471407820773, "posterior_entropy": 0.6931471407820773, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.500141024585799, "accuracy": 0, "tokens_in": 48, "tokens_out": 16, "tokens_total": 64, "latency_total": 0.005962965000435361, "latency_per_module": {"scorer": 0.005962965000435361}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 16, "tokens_total": 64}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jake was off roading. He had to get help to get out.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake was off roading. He had to get help to get out.", "hypotheses": ["Jake ended up getting free from the mud.", "Jake got stuff in the mud."]}
{"example_id": "89", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4483684781669959, 0.551631521833004], "posterior_probs": [0.4483684781669959, 0.551631521833004], "prior_entropy": 0.6878060363970406, "posterior_entropy": 0.6878060363970406, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.551631521833004, "accuracy": 1, "tokens_in": 50, "tokens_out": 19, "tokens_total": 69, "latency_total": 0.006171809999614197, "latency_per_module": {"scorer": 0.006171809999614197}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 19, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I got a new racing game yesterday. Finally after hours of playing I stopped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I got a new racing game yesterday. Finally after hours of playing I stopped.", "hypotheses": ["It was so fun, I was a clown.", "I sat down to test out the game."]}
{"example_id": "90", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.3680791136694201, 0.63192088633058], "posterior_probs": [0.3680791136694201, 0.63192088633058], "prior_entropy": 0.6579254323609798, "posterior_entropy": 0.6579254323609798, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.63192088633058, "accuracy": 1, "tokens_in": 62, "tokens_out": 20, "tokens_total": 82, "latency_total": 0.006784294000681257, "latency_per_module": {"scorer": 0.006784294000681257}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Freda is the boss of her office. Freda can't understand why people have a problem with her!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Freda is the boss of her office. Freda can't understand why people have a problem with her!", "hypotheses": ["She doesn't ask nicely to get things indefinite.", "She doesn't ask nicely to get things done."]}
{"example_id": "91", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5296516170952371, 0.4703483829047629], "posterior_probs": [0.5296516170952371, 0.4703483829047629], "prior_entropy": 0.6913877116125613, "posterior_entropy": 0.6913877116125613, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5296516170952371, "accuracy": 0, "tokens_in": 74, "tokens_out": 20, "tokens_total": 94, "latency_total": 0.006886149999445479, "latency_per_module": {"scorer": 0.006886149999445479}, "tokens_per_module": {"scorer": {"tokens_in": 74, "tokens_out": 20, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.", "hypotheses": ["One of them had a bad cough.", "Carly, noticed her daughter had gotten into Poison Ivy."]}
{"example_id": "92", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.48982945320701626, 0.5101705467929837], "posterior_probs": [0.48982945320701626, 0.5101705467929837], "prior_entropy": 0.6929402862449963, "posterior_entropy": 0.6929402862449963, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5101705467929837, "accuracy": 1, "tokens_in": 70, "tokens_out": 23, "tokens_total": 93, "latency_total": 0.006854344999737805, "latency_per_module": {"scorer": 0.006854344999737805}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 23, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.", "hypotheses": ["Alexis made sure the tree wasn't under 20 feet tall.", "Alexis was worried it would be too big."]}
{"example_id": "93", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6592424657664189, 0.3407575342335811], "posterior_probs": [0.6592424657664189, 0.3407575342335811], "prior_entropy": 0.6415366677654246, "posterior_entropy": 0.6415366677654246, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6592424657664189, "accuracy": 0, "tokens_in": 60, "tokens_out": 20, "tokens_total": 80, "latency_total": 0.006676959999822429, "latency_per_module": {"scorer": 0.006676959999822429}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.", "hypotheses": ["Francine decided to go to school to pursue her dreams.", "francine applied to business school."]}
{"example_id": "94", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5440713882680684, 0.45592861173193144], "posterior_probs": [0.5440713882680684, 0.45592861173193144], "prior_entropy": 0.6892575603602893, "posterior_entropy": 0.6892575603602893, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5440713882680684, "accuracy": 0, "tokens_in": 70, "tokens_out": 19, "tokens_total": 89, "latency_total": 0.00665439200020046, "latency_per_module": {"scorer": 0.00665439200020046}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 19, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.", "hypotheses": ["Jack caught a ball that bounced over the fence.", "Jack saw how wild the crowd was getting."]}
{"example_id": "95", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5983544817378198, 0.40164551826218015], "posterior_probs": [0.5983544817378198, 0.40164551826218015], "prior_entropy": 0.6736732287085543, "posterior_entropy": 0.6736732287085543, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5983544817378198, "accuracy": 0, "tokens_in": 60, "tokens_out": 15, "tokens_total": 75, "latency_total": 0.006488199000159511, "latency_per_module": {"scorer": 0.006488199000159511}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.", "hypotheses": ["Nathan never got into a fight.", "Nathan got detention in school."]}
{"example_id": "96", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6835728746419494, 0.3164271253580506], "posterior_probs": [0.6835728746419494, 0.3164271253580506], "prior_entropy": 0.624146934819759, "posterior_entropy": 0.624146934819759, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6835728746419494, "accuracy": 0, "tokens_in": 56, "tokens_out": 14, "tokens_total": 70, "latency_total": 0.006378404000315641, "latency_per_module": {"scorer": 0.006378404000315641}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 14, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.", "hypotheses": ["Mike found pasta hard to make.", "Mile loves italian food."]}
{"example_id": "97", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5251973619984914, 0.47480263800150857], "posterior_probs": [0.5251973619984914, 0.47480263800150857], "prior_entropy": 0.6918768284318874, "posterior_entropy": 0.6918768284318874, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5251973619984914, "accuracy": 0, "tokens_in": 62, "tokens_out": 18, "tokens_total": 80, "latency_total": 0.006317481999758456, "latency_per_module": {"scorer": 0.006317481999758456}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.", "hypotheses": ["Randy knew the area well.", "He didn't know that part of town very good."]}
{"example_id": "98", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4126552520352513, 0.5873447479647488], "posterior_probs": [0.4126552520352513, 0.5873447479647488], "prior_entropy": 0.6778104031708004, "posterior_entropy": 0.6778104031708004, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5873447479647488, "accuracy": 1, "tokens_in": 64, "tokens_out": 18, "tokens_total": 82, "latency_total": 0.0066677489994617645, "latency_per_module": {"scorer": 0.0066677489994617645}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 18, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!", "hypotheses": ["Cat sent a love note to the boy.", "She told him she did not like him."]}
{"example_id": "99", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5060554166199094, 0.49394458338009056], "posterior_probs": [0.5060554166199094, 0.49394458338009056], "prior_entropy": 0.693073842624229, "posterior_entropy": 0.693073842624229, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5060554166199094, "accuracy": 0, "tokens_in": 56, "tokens_out": 35, "tokens_total": 91, "latency_total": 0.006698264999613457, "latency_per_module": {"scorer": 0.006698264999613457}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 35, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lacy and Karen got in a fight. Karen apologized too so they could be friends again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lacy and Karen got in a fight. Karen apologized too so they could be friends again.", "hypotheses": ["Karen missed Lacy so much, she couldn't bear not talking to her friend.", "Lacy missed Karen so much, she couldn't bear not talking to her friend."]}
{"example_id": "100", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6252029901029774, 0.3747970098970227], "posterior_probs": [0.6252029901029774, 0.3747970098970227], "prior_entropy": 0.6614594576997013, "posterior_entropy": 0.6614594576997013, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6252029901029774, "accuracy": 0, "tokens_in": 64, "tokens_out": 25, "tokens_total": 89, "latency_total": 0.006661568999334122, "latency_per_module": {"scorer": 0.006661568999334122}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 25, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My friend had an announcement to make. So, I put on a smile and wished him the best of luck.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend had an announcement to make. So, I put on a smile and wished him the best of luck.", "hypotheses": ["My friend told us he had cancer and was expected to die in a week.", "my friend usually talks about some business deal."]}
{"example_id": "101", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5037615781635802, 0.49623842183641964], "posterior_probs": [0.5037615781635802, 0.49623842183641964], "prior_entropy": 0.6931188813504345, "posterior_entropy": 0.6931188813504345, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5037615781635802, "accuracy": 0, "tokens_in": 60, "tokens_out": 27, "tokens_total": 87, "latency_total": 0.006571019000148226, "latency_per_module": {"scorer": 0.006571019000148226}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 27, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: George was about to participate in his first professional fight. George proved his skills and won his first match.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "George was about to participate in his first professional fight. George proved his skills and won his first match.", "hypotheses": ["George trained hard for two days before the fight.", "George was the underdog, but he had been training months to compete in this event."]}
{"example_id": "102", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6479163582241244, 0.3520836417758756], "posterior_probs": [0.6479163582241244, 0.3520836417758756], "prior_entropy": 0.6487269617884174, "posterior_entropy": 0.6487269617884174, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6479163582241244, "accuracy": 0, "tokens_in": 56, "tokens_out": 18, "tokens_total": 74, "latency_total": 0.006467844999860972, "latency_per_module": {"scorer": 0.006467844999860972}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob and his kids love football. Bob and his kids share a hug to celebrate the win.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob and his kids love football. Bob and his kids share a hug to celebrate the win.", "hypotheses": ["The team that Bob and his kids like won.", "They played basketball out back all afternoon."]}
{"example_id": "103", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5873619102187421, 0.412638089781258], "posterior_probs": [0.5873619102187421, 0.412638089781258], "prior_entropy": 0.67780434429697, "posterior_entropy": 0.67780434429697, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5873619102187421, "accuracy": 0, "tokens_in": 58, "tokens_out": 25, "tokens_total": 83, "latency_total": 0.006493196000519674, "latency_per_module": {"scorer": 0.006493196000519674}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 25, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Alex was at target with his mom. He begged his mother to buy it until she gave in.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alex was at target with his mom. He begged his mother to buy it until she gave in.", "hypotheses": ["He didn't see the toy he really wanted in any of the aisles.", "Alex saw a game he really wanted."]}
{"example_id": "104", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6404063862747398, 0.3595936137252602], "posterior_probs": [0.6404063862747398, 0.3595936137252602], "prior_entropy": 0.6531840162431645, "posterior_entropy": 0.6531840162431645, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6404063862747398, "accuracy": 0, "tokens_in": 48, "tokens_out": 24, "tokens_total": 72, "latency_total": 0.006363411000165797, "latency_per_module": {"scorer": 0.006363411000165797}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 24, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nova dreamed of being a professional dancer. Nova's second choice accepted her.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nova dreamed of being a professional dancer. Nova's second choice accepted her.", "hypotheses": ["Nova applied to a few dance schools but was denied by her first choice.", "Nova applied to one dance school."]}
{"example_id": "105", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.45470671903037124, 0.5452932809696288], "posterior_probs": [0.45470671903037124, 0.5452932809696288], "prior_entropy": 0.6890385880217316, "posterior_entropy": 0.6890385880217316, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5452932809696288, "accuracy": 1, "tokens_in": 60, "tokens_out": 24, "tokens_total": 84, "latency_total": 0.0067821720003848895, "latency_per_module": {"scorer": 0.0067821720003848895}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 24, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.", "hypotheses": ["An unexpected event happened when the tide didn't come in that day.", "The sand castle was built right on the shore."]}
{"example_id": "106", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.43206894712477, 0.5679310528752299], "posterior_probs": [0.43206894712477, 0.5679310528752299], "prior_entropy": 0.6838893198791889, "posterior_entropy": 0.6838893198791889, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5679310528752299, "accuracy": 1, "tokens_in": 72, "tokens_out": 18, "tokens_total": 90, "latency_total": 0.0069317509996835724, "latency_per_module": {"scorer": 0.0069317509996835724}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 18, "tokens_total": 90}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.", "hypotheses": ["She wanted to be less involved and lose friends.", "Tami was tall for her age."]}
{"example_id": "107", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4372740685760933, 0.5627259314239066], "posterior_probs": [0.4372740685760933, 0.5627259314239066], "prior_entropy": 0.6852573237333645, "posterior_entropy": 0.6852573237333645, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5627259314239066, "accuracy": 1, "tokens_in": 42, "tokens_out": 19, "tokens_total": 61, "latency_total": 0.00598951199935982, "latency_per_module": {"scorer": 0.00598951199935982}, "tokens_per_module": {"scorer": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Donald is running for president. Hopefully he loses the election.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Donald is running for president. Hopefully he loses the election.", "hypotheses": ["Donald is a selfless, wonderful person.", "Donald is not the candidate I want for president."]}
{"example_id": "108", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5228436593353636, 0.4771563406646364], "posterior_probs": [0.5228436593353636, 0.4771563406646364], "prior_entropy": 0.6921031516315433, "posterior_entropy": 0.6921031516315433, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5228436593353636, "accuracy": 0, "tokens_in": 56, "tokens_out": 21, "tokens_total": 77, "latency_total": 0.0065093439998236136, "latency_per_module": {"scorer": 0.0065093439998236136}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 21, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was very out of shape. After weeks of jumping rope, I began to feel excellent.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was very out of shape. After weeks of jumping rope, I began to feel excellent.", "hypotheses": ["I had rope that I used for jumping at home.", "I committed to exercise every month by jumping rope."]}
{"example_id": "109", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47234653950590905, 0.527653460494091], "posterior_probs": [0.47234653950590905, 0.527653460494091], "prior_entropy": 0.6916169721313477, "posterior_entropy": 0.6916169721313477, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.527653460494091, "accuracy": 1, "tokens_in": 68, "tokens_out": 27, "tokens_total": 95, "latency_total": 0.006756254999345401, "latency_per_module": {"scorer": 0.006756254999345401}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 27, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.", "hypotheses": ["Ora's family needed to use more gas.", "Ora's mom kicked her out of the house so their gas bill would reduce."]}
{"example_id": "110", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5043134215278807, 0.4956865784721193], "posterior_probs": [0.5043134215278807, 0.4956865784721193], "prior_entropy": 0.6931099688858192, "posterior_entropy": 0.6931099688858192, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5043134215278807, "accuracy": 0, "tokens_in": 80, "tokens_out": 24, "tokens_total": 104, "latency_total": 0.007432475999848975, "latency_per_module": {"scorer": 0.007432475999848975}, "tokens_per_module": {"scorer": {"tokens_in": 80, "tokens_out": 24, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.", "hypotheses": ["The puppy was given to me by a stork.", "I got up and tried to find out why they kept barking."]}
{"example_id": "111", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5925861323348321, 0.4074138676651678], "posterior_probs": [0.5925861323348321, 0.4074138676651678], "prior_entropy": 0.6759034511018422, "posterior_entropy": 0.6759034511018422, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5925861323348321, "accuracy": 0, "tokens_in": 60, "tokens_out": 22, "tokens_total": 82, "latency_total": 0.007019532999947842, "latency_per_module": {"scorer": 0.007019532999947842}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 22, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.", "hypotheses": ["Dan ate a lot of food very slowly, hoping to win.", "Dan tried to eat 30 cold hot dogs."]}
{"example_id": "112", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5464544162805514, 0.4535455837194486], "posterior_probs": [0.5464544162805514, 0.4535455837194486], "prior_entropy": 0.688824924075583, "posterior_entropy": 0.688824924075583, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5464544162805514, "accuracy": 0, "tokens_in": 60, "tokens_out": 20, "tokens_total": 80, "latency_total": 0.007114217999514949, "latency_per_module": {"scorer": 0.007114217999514949}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.", "hypotheses": ["Melissa was being rude.", "Melissa's friend insisted they go out to eat somewhere Melissa hated."]}
{"example_id": "113", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4783050226856938, 0.5216949773143061], "posterior_probs": [0.4783050226856938, 0.5216949773143061], "prior_entropy": 0.6922055408777172, "posterior_entropy": 0.6922055408777172, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5216949773143061, "accuracy": 1, "tokens_in": 68, "tokens_out": 18, "tokens_total": 86, "latency_total": 0.007444953999765858, "latency_per_module": {"scorer": 0.007444953999765858}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 18, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.", "hypotheses": ["Nell studied biology to draw an animal.", "Nell was told to do something unexpected."]}
{"example_id": "114", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.527262563582004, 0.47273743641799604], "posterior_probs": [0.527262563582004, 0.47273743641799604], "prior_entropy": 0.69165994837897, "posterior_entropy": 0.69165994837897, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.527262563582004, "accuracy": 0, "tokens_in": 56, "tokens_out": 20, "tokens_total": 76, "latency_total": 0.007277705999513273, "latency_per_module": {"scorer": 0.007277705999513273}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 20, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.", "hypotheses": ["His dad asked Eli how to tie his shoes.", "eli asked his dad how to tie his shoes."]}
{"example_id": "115", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.48110675756754917, 0.5188932424324508], "posterior_probs": [0.48110675756754917, 0.5188932424324508], "prior_entropy": 0.6924331013528291, "posterior_entropy": 0.6924331013528291, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5188932424324508, "accuracy": 1, "tokens_in": 68, "tokens_out": 18, "tokens_total": 86, "latency_total": 0.007542511999417911, "latency_per_module": {"scorer": 0.007542511999417911}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 18, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.", "hypotheses": ["She was anxious to buy some today.", "Francine saw bananas for sale at the fair."]}
{"example_id": "116", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47096512638183935, 0.5290348736181606], "posterior_probs": [0.47096512638183935, 0.5290348736181606], "prior_entropy": 0.6914601839197501, "posterior_entropy": 0.6914601839197501, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5290348736181606, "accuracy": 1, "tokens_in": 56, "tokens_out": 22, "tokens_total": 78, "latency_total": 0.007083800000145857, "latency_per_module": {"scorer": 0.007083800000145857}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 22, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ben went to the beach on a sunny day. Ben crawled into his tent and napped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ben went to the beach on a sunny day. Ben crawled into his tent and napped.", "hypotheses": ["Ben spent hours sitting in the sun.", "Ben pitched a large tent on the sand to block out the sun."]}
{"example_id": "117", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6605205014879388, 0.33947949851206116], "posterior_probs": [0.6605205014879388, 0.33947949851206116], "prior_entropy": 0.6406896284443233, "posterior_entropy": 0.6406896284443233, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6605205014879388, "accuracy": 0, "tokens_in": 54, "tokens_out": 15, "tokens_total": 69, "latency_total": 0.007194609999714885, "latency_per_module": {"scorer": 0.007194609999714885}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 15, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.", "hypotheses": ["Tili ran for the prison gate one night.", "Doug formulated a plan."]}
{"example_id": "118", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47853895165243243, 0.5214610483475676], "posterior_probs": [0.47853895165243243, 0.5214610483475676], "prior_entropy": 0.6922257443158863, "posterior_entropy": 0.6922257443158863, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5214610483475676, "accuracy": 1, "tokens_in": 62, "tokens_out": 23, "tokens_total": 85, "latency_total": 0.007013515999460651, "latency_per_module": {"scorer": 0.007013515999460651}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 23, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.", "hypotheses": ["Sam lost his job and could not make the payment.", "Sam decided to work two jobs to pay off his debt."]}
{"example_id": "119", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.36083121223098397, 0.639168787769016], "posterior_probs": [0.36083121223098397, 0.639168787769016], "prior_entropy": 0.6538949456312322, "posterior_entropy": 0.6538949456312322, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.639168787769016, "accuracy": 1, "tokens_in": 62, "tokens_out": 19, "tokens_total": 81, "latency_total": 0.007141568999941228, "latency_per_module": {"scorer": 0.007141568999941228}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 19, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.", "hypotheses": ["The kitchen got so bad mold might grow.", "Katie needed to hurry and get it clean."]}
{"example_id": "120", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47581760802240475, 0.5241823919775953], "posterior_probs": [0.47581760802240475, 0.5241823919775953], "prior_entropy": 0.6919771479977935, "posterior_entropy": 0.6919771479977935, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5241823919775953, "accuracy": 1, "tokens_in": 66, "tokens_out": 20, "tokens_total": 86, "latency_total": 0.0070259459998851526, "latency_per_module": {"scorer": 0.0070259459998851526}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 20, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.", "hypotheses": ["The directions took Randy thru a great part of town.", "The house had boarded windows and looked bad."]}
{"example_id": "121", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.41262664839655694, 0.5873733516034432], "posterior_probs": [0.41262664839655694, 0.5873733516034432], "prior_entropy": 0.6778003044142535, "posterior_entropy": 0.6778003044142535, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5873733516034432, "accuracy": 1, "tokens_in": 72, "tokens_out": 21, "tokens_total": 93, "latency_total": 0.007495028999983333, "latency_per_module": {"scorer": 0.007495028999983333}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 21, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!", "hypotheses": ["I was worried about how to tow.", "I called my insurance company to see if I could get assistance."]}
{"example_id": "122", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5599508886754255, 0.4400491113245745], "posterior_probs": [0.5599508886754255, 0.4400491113245745], "prior_entropy": 0.6859416391441366, "posterior_entropy": 0.6859416391441366, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5599508886754255, "accuracy": 0, "tokens_in": 70, "tokens_out": 18, "tokens_total": 88, "latency_total": 0.0076517179995789775, "latency_per_module": {"scorer": 0.0076517179995789775}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 18, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.", "hypotheses": ["Tia had dinner with her parents.", "Tia thought something seemed good between her parents."]}
{"example_id": "123", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.554069221489842, 0.44593077851015805], "posterior_probs": [0.554069221489842, 0.44593077851015805], "prior_entropy": 0.6872887698399445, "posterior_entropy": 0.6872887698399445, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.554069221489842, "accuracy": 0, "tokens_in": 76, "tokens_out": 21, "tokens_total": 97, "latency_total": 0.007510639999964042, "latency_per_module": {"scorer": 0.007510639999964042}, "tokens_per_module": {"scorer": {"tokens_in": 76, "tokens_out": 21, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.", "hypotheses": ["Joe tripped down the stairs with his shoes untied.", "Joe tied them and fell down the stairs."]}
{"example_id": "124", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4648569698295527, 0.5351430301704473], "posterior_probs": [0.4648569698295527, 0.5351430301704473], "prior_entropy": 0.6906750776487804, "posterior_entropy": 0.6906750776487804, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5351430301704473, "accuracy": 1, "tokens_in": 66, "tokens_out": 20, "tokens_total": 86, "latency_total": 0.007710810999924433, "latency_per_module": {"scorer": 0.007710810999924433}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 20, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.", "hypotheses": ["Nita practiced playing rummy with a dog.", "Nita was never able to beat her Dad."]}
{"example_id": "125", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5531757724693236, 0.44682422753067647], "posterior_probs": [0.5531757724693236, 0.44682422753067647], "prior_entropy": 0.6874811455731205, "posterior_entropy": 0.6874811455731205, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5531757724693236, "accuracy": 0, "tokens_in": 56, "tokens_out": 17, "tokens_total": 73, "latency_total": 0.00742945299953135, "latency_per_module": {"scorer": 0.00742945299953135}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Fred had a job at the fair to fill the balloons. The balloon popped in his face!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Fred had a job at the fair to fill the balloons. The balloon popped in his face!", "hypotheses": ["Fred saw a child come over with a pin.", "Fred fill one balloon too small."]}
{"example_id": "126", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5468633476618596, 0.4531366523381404], "posterior_probs": [0.5468633476618596, 0.4531366523381404], "prior_entropy": 0.6887483802422558, "posterior_entropy": 0.6887483802422558, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5468633476618596, "accuracy": 0, "tokens_in": 68, "tokens_out": 21, "tokens_total": 89, "latency_total": 0.007688263000090956, "latency_per_module": {"scorer": 0.007688263000090956}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 21, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.", "hypotheses": ["Samantha found some friends who gave her a ride.", "She had a flat tire and changed it."]}
{"example_id": "127", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4549968142922656, 0.5450031857077344], "posterior_probs": [0.4549968142922656, 0.5450031857077344], "prior_entropy": 0.6890911202623089, "posterior_entropy": 0.6890911202623089, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5450031857077344, "accuracy": 1, "tokens_in": 66, "tokens_out": 13, "tokens_total": 79, "latency_total": 0.008875102000274637, "latency_per_module": {"scorer": 0.008875102000274637}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 13, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.", "hypotheses": ["The tree fell on my fort.", "It was the right size."]}
{"example_id": "128", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5035711515654095, 0.49642884843459045], "posterior_probs": [0.5035711515654095, 0.49642884843459045], "prior_entropy": 0.6931216740940784, "posterior_entropy": 0.6931216740940784, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5035711515654095, "accuracy": 0, "tokens_in": 58, "tokens_out": 22, "tokens_total": 80, "latency_total": 0.013991186000566813, "latency_per_module": {"scorer": 0.013991186000566813}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 22, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike had to go to the doctor. All the blood work came back clear and he was relieved.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike had to go to the doctor. All the blood work came back clear and he was relieved.", "hypotheses": ["Mike was having blood drawn because he needed a routine checkup.", "Mike complained of soreness in his kidneys."]}
{"example_id": "129", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5061941554108962, 0.49380584458910376], "posterior_probs": [0.5061941554108962, 0.49380584458910376], "prior_entropy": 0.6930704434725565, "posterior_entropy": 0.6930704434725565, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5061941554108962, "accuracy": 0, "tokens_in": 56, "tokens_out": 21, "tokens_total": 77, "latency_total": 0.012400518000504235, "latency_per_module": {"scorer": 0.012400518000504235}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 21, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.", "hypotheses": ["Rocky was very good at playing hockey.", "Rocky was a rocket scientist, and he hated rockets."]}
{"example_id": "130", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4433643422955545, 0.5566356577044455], "posterior_probs": [0.4433643422955545, 0.5566356577044455], "prior_entropy": 0.686718195974517, "posterior_entropy": 0.686718195974517, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5566356577044455, "accuracy": 1, "tokens_in": 40, "tokens_out": 17, "tokens_total": 57, "latency_total": 0.010962371999994502, "latency_per_module": {"scorer": 0.010962371999994502}, "tokens_per_module": {"scorer": {"tokens_in": 40, "tokens_out": 17, "tokens_total": 57}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Arnold was scared of girls. He nearly fainted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Arnold was scared of girls. He nearly fainted.", "hypotheses": ["Arnold saw a boy.", "A girl came up to hug him one day."]}
{"example_id": "131", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5233937045763193, 0.4766062954236807], "posterior_probs": [0.5233937045763193, 0.4766062954236807], "prior_entropy": 0.6920522500476836, "posterior_entropy": 0.6920522500476836, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5233937045763193, "accuracy": 0, "tokens_in": 66, "tokens_out": 19, "tokens_total": 85, "latency_total": 0.009544860999994853, "latency_per_module": {"scorer": 0.009544860999994853}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 19, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.", "hypotheses": ["Bobby found a cat in the garden.", "Bobby begged his mom for a feral cat."]}
{"example_id": "132", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.2999824166984342, 0.7000175833015658], "posterior_probs": [0.2999824166984342, 0.7000175833015658], "prior_entropy": 0.610849403022965, "posterior_entropy": 0.610849403022965, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7000175833015658, "accuracy": 1, "tokens_in": 72, "tokens_out": 24, "tokens_total": 96, "latency_total": 0.008537719999367255, "latency_per_module": {"scorer": 0.008537719999367255}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 24, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.", "hypotheses": ["Lucky looked for them in one store.", "Lucy looked for the sandals everywhere, but could never find them."]}
{"example_id": "133", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4819782992719845, 0.5180217007280155], "posterior_probs": [0.4819782992719845, 0.5180217007280155], "prior_entropy": 0.6924974764463473, "posterior_entropy": 0.6924974764463473, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5180217007280155, "accuracy": 1, "tokens_in": 70, "tokens_out": 15, "tokens_total": 85, "latency_total": 0.009019316999911098, "latency_per_module": {"scorer": 0.009019316999911098}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 15, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.", "hypotheses": ["We love to go fishing.", "We found one of them in the backyard."]}
{"example_id": "134", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.49344076594076103, 0.5065592340592391], "posterior_probs": [0.49344076594076103, 0.5065592340592391], "prior_entropy": 0.6930611309868531, "posterior_entropy": 0.6930611309868531, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5065592340592391, "accuracy": 1, "tokens_in": 54, "tokens_out": 14, "tokens_total": 68, "latency_total": 0.007912805999694683, "latency_per_module": {"scorer": 0.007912805999694683}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 14, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: We decided to move to a new town next year. It will be a fun adventure.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "We decided to move to a new town next year. It will be a fun adventure.", "hypotheses": ["We grew fond of this town.", "We got sick of this town."]}
{"example_id": "135", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4610257841579939, 0.5389742158420062], "posterior_probs": [0.4610257841579939, 0.5389742158420062], "prior_entropy": 0.6901061176167884, "posterior_entropy": 0.6901061176167884, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5389742158420062, "accuracy": 1, "tokens_in": 52, "tokens_out": 19, "tokens_total": 71, "latency_total": 0.007426031999784755, "latency_per_module": {"scorer": 0.007426031999784755}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.", "hypotheses": ["Ryan was mad at Kim for stealing his phone.", "Heather kept the phone away from Ryan."]}
{"example_id": "136", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5327502600876581, 0.46724973991234187], "posterior_probs": [0.5327502600876581, 0.46724973991234187], "prior_entropy": 0.6910004849454078, "posterior_entropy": 0.6910004849454078, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5327502600876581, "accuracy": 0, "tokens_in": 74, "tokens_out": 19, "tokens_total": 93, "latency_total": 0.007191681000222161, "latency_per_module": {"scorer": 0.007191681000222161}, "tokens_per_module": {"scorer": {"tokens_in": 74, "tokens_out": 19, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.", "hypotheses": ["Tracy spends his days scrubbing and cleaning.", "I asked him how it is so clean."]}
{"example_id": "137", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.3580635427488348, 0.6419364572511652], "posterior_probs": [0.3580635427488348, 0.6419364572511652], "prior_entropy": 0.6522958825764628, "posterior_entropy": 0.6522958825764628, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6419364572511652, "accuracy": 1, "tokens_in": 52, "tokens_out": 20, "tokens_total": 72, "latency_total": 0.006991936999838799, "latency_per_module": {"scorer": 0.006991936999838799}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kya was trying to be vegan. Before long, being vegan was effortless.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kya was trying to be vegan. Before long, being vegan was effortless.", "hypotheses": ["Kya learned many new steak recipes.", "At first it was hard, but she persevered."]}
{"example_id": "138", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5818840765131251, 0.4181159234868748], "posterior_probs": [0.5818840765131251, 0.4181159234868748], "prior_entropy": 0.6796765813872114, "posterior_entropy": 0.6796765813872114, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5818840765131251, "accuracy": 0, "tokens_in": 62, "tokens_out": 24, "tokens_total": 86, "latency_total": 0.00707595499989111, "latency_per_module": {"scorer": 0.00707595499989111}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 24, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.", "hypotheses": ["The system was very expensive to have installed.", "This month, my electric bill was double  what is used to be."]}
{"example_id": "139", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5041482688845201, 0.4958517311154799], "posterior_probs": [0.5041482688845201, 0.4958517311154799], "prior_entropy": 0.6931127638936314, "posterior_entropy": 0.6931127638936314, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5041482688845201, "accuracy": 0, "tokens_in": 62, "tokens_out": 18, "tokens_total": 80, "latency_total": 0.01265125700047065, "latency_per_module": {"scorer": 0.01265125700047065}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.", "hypotheses": ["We all had dinner at my big house.", "We all had dinner at my tiny house."]}
{"example_id": "140", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.52242454564882, 0.47757545435118004], "posterior_probs": [0.52242454564882, 0.47757545435118004], "prior_entropy": 0.6921411226333666, "posterior_entropy": 0.6921411226333666, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.52242454564882, "accuracy": 0, "tokens_in": 62, "tokens_out": 19, "tokens_total": 81, "latency_total": 0.008620630000223173, "latency_per_module": {"scorer": 0.008620630000223173}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 19, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.", "hypotheses": ["There wasn't any food on a plate.", "there was a piece of steak on a plate."]}
{"example_id": "141", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5214472455295223, 0.47855275447047774], "posterior_probs": [0.5214472455295223, 0.47855275447047774], "prior_entropy": 0.6922269295543793, "posterior_entropy": 0.6922269295543793, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5214472455295223, "accuracy": 0, "tokens_in": 54, "tokens_out": 13, "tokens_total": 67, "latency_total": 0.0073674289997143205, "latency_per_module": {"scorer": 0.0073674289997143205}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 13, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Helen hung up the stocking on the railing. And someone had put presents in her stocking!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Helen hung up the stocking on the railing. And someone had put presents in her stocking!", "hypotheses": ["Helen went to the store.", "Helen went to sleep."]}
{"example_id": "142", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4818382331647717, 0.5181617668352283], "posterior_probs": [0.4818382331647717, 0.5181617668352283], "prior_entropy": 0.6924873358643442, "posterior_entropy": 0.6924873358643442, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5181617668352283, "accuracy": 1, "tokens_in": 76, "tokens_out": 25, "tokens_total": 101, "latency_total": 0.00768677399992157, "latency_per_module": {"scorer": 0.00768677399992157}, "tokens_per_module": {"scorer": {"tokens_in": 76, "tokens_out": 25, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.", "hypotheses": ["Tim forgot the gift in his car until after her birthday.", "Tim realized it would barely make it in time for her birthday."]}
{"example_id": "143", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.47161095172450157, 0.5283890482754985], "posterior_probs": [0.47161095172450157, 0.5283890482754985], "prior_entropy": 0.6915344372670527, "posterior_entropy": 0.6915344372670527, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5283890482754985, "accuracy": 1, "tokens_in": 40, "tokens_out": 18, "tokens_total": 58, "latency_total": 0.006761333000213199, "latency_per_module": {"scorer": 0.006761333000213199}, "tokens_per_module": {"scorer": {"tokens_in": 40, "tokens_out": 18, "tokens_total": 58}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Greg was arrested for manslaughter. So Greg was convicted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Greg was arrested for manslaughter. So Greg was convicted.", "hypotheses": ["greg had not called the police right away.", "Greg's lawyer made a compelling defense case."]}
{"example_id": "144", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.44015820877619033, 0.5598417912238096], "posterior_probs": [0.44015820877619033, 0.5598417912238096], "prior_entropy": 0.6859679034141187, "posterior_entropy": 0.6859679034141187, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5598417912238096, "accuracy": 1, "tokens_in": 46, "tokens_out": 21, "tokens_total": 67, "latency_total": 0.006940199000382563, "latency_per_module": {"scorer": 0.006940199000382563}, "tokens_per_module": {"scorer": {"tokens_in": 46, "tokens_out": 21, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe could not remember his address. After that he always remembered it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe could not remember his address. After that he always remembered it.", "hypotheses": ["Joe could not have a pizza delivered.", "He made up a a rhyme that included his phone number."]}
{"example_id": "145", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5366545194477929, 0.46334548055220715], "posterior_probs": [0.5366545194477929, 0.46334548055220715], "prior_entropy": 0.6904576609280619, "posterior_entropy": 0.6904576609280619, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5366545194477929, "accuracy": 0, "tokens_in": 54, "tokens_out": 20, "tokens_total": 74, "latency_total": 0.007004235000749759, "latency_per_module": {"scorer": 0.007004235000749759}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nadia needed new ballet shoes. After her first dance show, she paid him back.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nadia needed new ballet shoes. After her first dance show, she paid him back.", "hypotheses": ["Nadia's friend gifted her the money for new shoes.", "A friend bought Nadia a pair."]}
{"example_id": "146", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5012941331574995, 0.49870586684250057], "posterior_probs": [0.5012941331574995, 0.49870586684250057], "prior_entropy": 0.6931438309929469, "posterior_entropy": 0.6931438309929469, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5012941331574995, "accuracy": 0, "tokens_in": 66, "tokens_out": 19, "tokens_total": 85, "latency_total": 0.006823986000199511, "latency_per_module": {"scorer": 0.006823986000199511}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 19, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.", "hypotheses": ["Matthew read the basketball rules and practiced the game.", "He practiced and tried out for a role."]}
{"example_id": "147", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5013264386531003, 0.4986735613468997], "posterior_probs": [0.5013264386531003, 0.4986735613468997], "prior_entropy": 0.693143661674817, "posterior_entropy": 0.693143661674817, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5013264386531003, "accuracy": 0, "tokens_in": 54, "tokens_out": 20, "tokens_total": 74, "latency_total": 0.007081120000293595, "latency_per_module": {"scorer": 0.007081120000293595}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.", "hypotheses": ["Mike entered a contest partnering with Joseph.", "Joseph could tell Mike was playing too over-confidently."]}
{"example_id": "148", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5719734763411052, 0.42802652365889493], "posterior_probs": [0.5719734763411052, 0.42802652365889493], "prior_entropy": 0.6827507390451305, "posterior_entropy": 0.6827507390451305, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5719734763411052, "accuracy": 0, "tokens_in": 52, "tokens_out": 28, "tokens_total": 80, "latency_total": 0.006932987000254798, "latency_per_module": {"scorer": 0.006932987000254798}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 28, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.", "hypotheses": ["All flights were grounded to Lisa couldn't leave for a couple of days.", "Lucy shared supplies in art class with Lisa, they bonded."]}
{"example_id": "149", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5042689001377829, 0.4957310998622171], "posterior_probs": [0.5042689001377829, 0.4957310998622171], "prior_entropy": 0.6931107330983648, "posterior_entropy": 0.6931107330983648, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5042689001377829, "accuracy": 0, "tokens_in": 60, "tokens_out": 20, "tokens_total": 80, "latency_total": 0.006654750999587122, "latency_per_module": {"scorer": 0.006654750999587122}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.", "hypotheses": ["Martha's boyfriend enrolled himself in cooking classes.", "Martha worked hard to learn some recipies."]}
{"example_id": "150", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5310802956494481, 0.4689197043505518], "posterior_probs": [0.5310802956494481, 0.4689197043505518], "prior_entropy": 0.6912139649068965, "posterior_entropy": 0.6912139649068965, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5310802956494481, "accuracy": 0, "tokens_in": 52, "tokens_out": 23, "tokens_total": 75, "latency_total": 0.008218013000259816, "latency_per_module": {"scorer": 0.008218013000259816}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 23, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.", "hypotheses": ["Her husband did not expect Lucy to be home yet.", "she couldn't wait to tell him she wanted a divorce."]}
{"example_id": "151", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5187758093381991, 0.4812241906618008], "posterior_probs": [0.5187758093381991, 0.4812241906618008], "prior_entropy": 0.692441952727644, "posterior_entropy": 0.692441952727644, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5187758093381991, "accuracy": 0, "tokens_in": 72, "tokens_out": 22, "tokens_total": 94, "latency_total": 0.008574653999858128, "latency_per_module": {"scorer": 0.008574653999858128}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 22, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.", "hypotheses": ["Kelsi and Thomas met at school.", "Kelsi saw Lucy was reading the same book she was."]}
{"example_id": "152", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5489156641466169, 0.45108433585338314], "posterior_probs": [0.5489156641466169, 0.45108433585338314], "prior_entropy": 0.6883540331646313, "posterior_entropy": 0.6883540331646313, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5489156641466169, "accuracy": 0, "tokens_in": 60, "tokens_out": 18, "tokens_total": 78, "latency_total": 0.010627594000652607, "latency_per_module": {"scorer": 0.010627594000652607}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 18, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I saved up money for a long time. I took the boat out on the lake and felt happy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I saved up money for a long time. I took the boat out on the lake and felt happy.", "hypotheses": ["i had enough money to spend on food and extra stuff.", "I bought my dream cat."]}
{"example_id": "153", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5717033204360721, 0.428296679563928], "posterior_probs": [0.5717033204360721, 0.428296679563928], "prior_entropy": 0.6828289102070211, "posterior_entropy": 0.6828289102070211, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5717033204360721, "accuracy": 0, "tokens_in": 72, "tokens_out": 18, "tokens_total": 90, "latency_total": 0.009762732000126562, "latency_per_module": {"scorer": 0.009762732000126562}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 18, "tokens_total": 90}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string", "hypotheses": ["Jake immediately threw his new toy in the trash.", "Jake decided Dan was the Green Goblin."]}
{"example_id": "154", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.46568290051530103, 0.534317099484699], "posterior_probs": [0.46568290051530103, 0.534317099484699], "prior_entropy": 0.6907900012428367, "posterior_entropy": 0.6907900012428367, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.534317099484699, "accuracy": 1, "tokens_in": 54, "tokens_out": 13, "tokens_total": 67, "latency_total": 0.008085082999969018, "latency_per_module": {"scorer": 0.008085082999969018}, "tokens_per_module": {"scorer": {"tokens_in": 54, "tokens_out": 13, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.", "hypotheses": ["Margo sat down to watch.", "Margo left the show."]}
{"example_id": "155", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4269841303580189, 0.573015869641981], "posterior_probs": [0.4269841303580189, 0.573015869641981], "prior_entropy": 0.6824463218505652, "posterior_entropy": 0.6824463218505652, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.573015869641981, "accuracy": 1, "tokens_in": 42, "tokens_out": 19, "tokens_total": 61, "latency_total": 0.0074807319997489685, "latency_per_module": {"scorer": 0.0074807319997489685}, "tokens_per_module": {"scorer": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Barry loves playing baseball. Barry also bought a hot dog.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Barry loves playing baseball. Barry also bought a hot dog.", "hypotheses": ["Barry's team won the game today.", "Barry went to the bar for a game."]}
{"example_id": "156", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.49943053747004224, 0.5005694625299577], "posterior_probs": [0.49943053747004224, 0.5005694625299577], "prior_entropy": 0.6931465319826591, "posterior_entropy": 0.6931465319826591, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5005694625299577, "accuracy": 1, "tokens_in": 62, "tokens_out": 20, "tokens_total": 82, "latency_total": 0.007836433999727888, "latency_per_module": {"scorer": 0.007836433999727888}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Erina's first day at her new job was today. Her new boss complimented her on her performance.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erina's first day at her new job was today. Her new boss complimented her on her performance.", "hypotheses": ["Erina gave it her all and did well.", "Erina took too many breaks the first day."]}
{"example_id": "157", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5264368049007686, 0.4735631950992314], "posterior_probs": [0.5264368049007686, 0.4735631950992314], "prior_entropy": 0.6917487192315404, "posterior_entropy": 0.6917487192315404, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5264368049007686, "accuracy": 0, "tokens_in": 70, "tokens_out": 15, "tokens_total": 85, "latency_total": 0.008178833999409107, "latency_per_module": {"scorer": 0.008178833999409107}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 15, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.", "hypotheses": ["Penny dropped her game on accident.", "She accidentally sold her game console."]}
{"example_id": "158", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5403909182322422, 0.45960908176775783], "posterior_probs": [0.5403909182322422, 0.45960908176775783], "prior_entropy": 0.6898807699753513, "posterior_entropy": 0.6898807699753513, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5403909182322422, "accuracy": 0, "tokens_in": 62, "tokens_out": 25, "tokens_total": 87, "latency_total": 0.008413609999479377, "latency_per_module": {"scorer": 0.008413609999479377}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 25, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.", "hypotheses": ["Amy robbed the frozen yogurt store and ate all of the yogurt.", "Amy was called back to work and her frozen yogurt melted."]}
{"example_id": "159", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5027835081552867, 0.4972164918447132], "posterior_probs": [0.5027835081552867, 0.4972164918447132], "prior_entropy": 0.693131684642603, "posterior_entropy": 0.693131684642603, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5027835081552867, "accuracy": 0, "tokens_in": 70, "tokens_out": 17, "tokens_total": 87, "latency_total": 0.007102471000507649, "latency_per_module": {"scorer": 0.007102471000507649}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 17, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.", "hypotheses": ["Ellen realized that she could.", "Ellen was told there was a dress code."]}
{"example_id": "160", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6198132078037958, 0.3801867921962042], "posterior_probs": [0.6198132078037958, 0.3801867921962042], "prior_entropy": 0.664155496307155, "posterior_entropy": 0.664155496307155, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6198132078037958, "accuracy": 0, "tokens_in": 48, "tokens_out": 18, "tokens_total": 66, "latency_total": 0.006743929000549542, "latency_per_module": {"scorer": 0.006743929000549542}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 18, "tokens_total": 66}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lily wanted a new Halloween costume. She ended up making a rabbit costume.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lily wanted a new Halloween costume. She ended up making a rabbit costume.", "hypotheses": ["Lily decided to make her costume a bear costume.", "All the costumes were gone though."]}
{"example_id": "161", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.464782121161704, 0.5352178788382961], "posterior_probs": [0.464782121161704, 0.5352178788382961], "prior_entropy": 0.6906645273748359, "posterior_entropy": 0.6906645273748359, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5352178788382961, "accuracy": 1, "tokens_in": 68, "tokens_out": 20, "tokens_total": 88, "latency_total": 0.007015991000116628, "latency_per_module": {"scorer": 0.007015991000116628}, "tokens_per_module": {"scorer": {"tokens_in": 68, "tokens_out": 20, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.", "hypotheses": ["She studied hard because she wanted to spell.", "Mia entered the spelling bee but didn't practice."]}
{"example_id": "162", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.49817956298321636, 0.5018204370167837], "posterior_probs": [0.49817956298321636, 0.5018204370167837], "prior_entropy": 0.6931405525614377, "posterior_entropy": 0.6931405525614377, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5018204370167837, "accuracy": 1, "tokens_in": 66, "tokens_out": 22, "tokens_total": 88, "latency_total": 0.007145250000576198, "latency_per_module": {"scorer": 0.007145250000576198}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 22, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.", "hypotheses": ["The bus was late and so was I to school.", "bus showed up early and I was late for class."]}
{"example_id": "163", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.37469656598213685, 0.6253034340178631], "posterior_probs": [0.37469656598213685, 0.6253034340178631], "prior_entropy": 0.6614080398426275, "posterior_entropy": 0.6614080398426275, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6253034340178631, "accuracy": 1, "tokens_in": 56, "tokens_out": 17, "tokens_total": 73, "latency_total": 0.006765012999494502, "latency_per_module": {"scorer": 0.006765012999494502}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.", "hypotheses": ["Jean booked her trip and went.", "Jean ended up having a bad time in Africa."]}
{"example_id": "164", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5138874089170941, 0.486112591082906], "posterior_probs": [0.5138874089170941, 0.486112591082906], "prior_entropy": 0.6927614106964033, "posterior_entropy": 0.6927614106964033, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5138874089170941, "accuracy": 0, "tokens_in": 48, "tokens_out": 24, "tokens_total": 72, "latency_total": 0.006810527999732585, "latency_per_module": {"scorer": 0.006810527999732585}, "tokens_per_module": {"scorer": {"tokens_in": 48, "tokens_out": 24, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary had never made rice before. She resolved to read directions next time!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary had never made rice before. She resolved to read directions next time!", "hypotheses": ["Mary ended up overcooking the pasta.", "The rice Mary put on the stove out over and burned on the stove."]}
{"example_id": "165", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.45114413817439564, 0.5488558618256044], "posterior_probs": [0.45114413817439564, 0.5488558618256044], "prior_entropy": 0.6883657645700085, "posterior_entropy": 0.6883657645700085, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5488558618256044, "accuracy": 1, "tokens_in": 58, "tokens_out": 19, "tokens_total": 77, "latency_total": 0.007118615999388567, "latency_per_module": {"scorer": 0.007118615999388567}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 19, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.", "hypotheses": ["Brad rushed to work.", "He was in such a slow mood, he didn't dress correctly."]}
{"example_id": "166", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5975131689009477, 0.40248683109905226], "posterior_probs": [0.5975131689009477, 0.40248683109905226], "prior_entropy": 0.6740071150648055, "posterior_entropy": 0.6740071150648055, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5975131689009477, "accuracy": 0, "tokens_in": 64, "tokens_out": 16, "tokens_total": 80, "latency_total": 0.007197805000032531, "latency_per_module": {"scorer": 0.007197805000032531}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 16, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.", "hypotheses": ["I left all my stuff out on the bed.", "I heard loud fire alarm."]}
{"example_id": "167", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5050753995895266, 0.49492460041047337], "posterior_probs": [0.5050753995895266, 0.49492460041047337], "prior_entropy": 0.693095660311174, "posterior_entropy": 0.693095660311174, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5050753995895266, "accuracy": 0, "tokens_in": 62, "tokens_out": 20, "tokens_total": 82, "latency_total": 0.0070873960003154934, "latency_per_module": {"scorer": 0.0070873960003154934}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: For a lark I started dragging my foot behind me at work. He told me to knock it off.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "For a lark I started dragging my foot behind me at work. He told me to knock it off.", "hypotheses": ["My shoes made a really loud sound in front of my boss.", "I fell and broke my leg."]}
{"example_id": "168", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6146910714067012, 0.3853089285932988], "posterior_probs": [0.6146910714067012, 0.3853089285932988], "prior_entropy": 0.666603394708587, "posterior_entropy": 0.666603394708587, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6146910714067012, "accuracy": 0, "tokens_in": 50, "tokens_out": 30, "tokens_total": 80, "latency_total": 0.007117760000255657, "latency_per_module": {"scorer": 0.007117760000255657}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 30, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tony liked art. Tony then went back to school and found a different major.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tony liked art. Tony then went back to school and found a different major.", "hypotheses": ["However, he was not very talented, and his hair did not sell very well.", "Tony applies for a lot of art gallery jobs and got rejected."]}
{"example_id": "169", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4112140779915971, 0.5887859220084029], "posterior_probs": [0.4112140779915971, 0.5887859220084029], "prior_entropy": 0.6772973833177964, "posterior_entropy": 0.6772973833177964, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5887859220084029, "accuracy": 1, "tokens_in": 50, "tokens_out": 23, "tokens_total": 73, "latency_total": 0.007051053999930446, "latency_per_module": {"scorer": 0.007051053999930446}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 23, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I found a $600 dollar envelope in the mail today. I am honest.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I found a $600 dollar envelope in the mail today. I am honest.", "hypotheses": ["I turned the $600 dollars to the authorities.", "I returned the empty envelope to the person it was addressed to."]}
{"example_id": "170", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6687183408185025, 0.3312816591814976], "posterior_probs": [0.6687183408185025, 0.3312816591814976], "prior_entropy": 0.6350825753029137, "posterior_entropy": 0.6350825753029137, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6687183408185025, "accuracy": 0, "tokens_in": 56, "tokens_out": 22, "tokens_total": 78, "latency_total": 0.007080503999532084, "latency_per_module": {"scorer": 0.007080503999532084}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 22, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.", "hypotheses": ["Dominick moved his house away from Becky's house.", "Dominick shifted their house near the Becky house so."]}
{"example_id": "171", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.3802296600706219, 0.619770339929378], "posterior_probs": [0.3802296600706219, 0.619770339929378], "prior_entropy": 0.6641764443158823, "posterior_entropy": 0.6641764443158823, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.619770339929378, "accuracy": 1, "tokens_in": 52, "tokens_out": 19, "tokens_total": 71, "latency_total": 0.00681328799964831, "latency_per_module": {"scorer": 0.00681328799964831}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was accidentally shot by his teammate in the army. He ends up being homeless.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was accidentally shot by his teammate in the army. He ends up being homeless.", "hypotheses": ["Tom developed emotional problems affecting his division.", "Tom was unable to find work being in a wheelchair."]}
{"example_id": "172", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4065824365907642, 0.5934175634092357], "posterior_probs": [0.4065824365907642, 0.5934175634092357], "prior_entropy": 0.6755905096228538, "posterior_entropy": 0.6755905096228538, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5934175634092357, "accuracy": 1, "tokens_in": 42, "tokens_out": 19, "tokens_total": 61, "latency_total": 0.007994567999958235, "latency_per_module": {"scorer": 0.007994567999958235}, "tokens_per_module": {"scorer": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tommy was having a bad day. Tommy had good friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tommy was having a bad day. Tommy had good friends.", "hypotheses": ["Tommy's friends cheered him up.", "Tommy's friends didn't pay attention to him."]}
{"example_id": "173", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5449985742051293, 0.45500142579487063], "posterior_probs": [0.5449985742051293, 0.45500142579487063], "prior_entropy": 0.689091952601288, "posterior_entropy": 0.689091952601288, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5449985742051293, "accuracy": 0, "tokens_in": 52, "tokens_out": 16, "tokens_total": 68, "latency_total": 0.008390001999941887, "latency_per_module": {"scorer": 0.008390001999941887}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jay wanted to vote. That is, until he got older and did it again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jay wanted to vote. That is, until he got older and did it again.", "hypotheses": ["Jay wasn't sure how to vote.", "Jay swore he'd never vote again."]}
{"example_id": "174", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.484791977761282, 0.515208022238718], "posterior_probs": [0.484791977761282, 0.515208022238718], "prior_entropy": 0.6926845413276974, "posterior_entropy": 0.6926845413276974, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.515208022238718, "accuracy": 1, "tokens_in": 60, "tokens_out": 14, "tokens_total": 74, "latency_total": 0.0073901910000131465, "latency_per_module": {"scorer": 0.0073901910000131465}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 14, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was really nervous before my first middle school dance. Now, she won't even talk to me.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was really nervous before my first middle school dance. Now, she won't even talk to me.", "hypotheses": ["My date threw up on me.", "I threw up on my date."]}
{"example_id": "175", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4577060736616809, 0.5422939263383191], "posterior_probs": [0.4577060736616809, 0.5422939263383191], "prior_entropy": 0.6895653495966495, "posterior_entropy": 0.6895653495966495, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5422939263383191, "accuracy": 1, "tokens_in": 62, "tokens_out": 32, "tokens_total": 94, "latency_total": 0.008159640000485524, "latency_per_module": {"scorer": 0.008159640000485524}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 32, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.", "hypotheses": ["Today I was ready for him. When he came into our room a jumped out and tickled him.", "He would jump on our ear to get our attention."]}
{"example_id": "176", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.39948043219530643, 0.6005195678046935], "posterior_probs": [0.39948043219530643, 0.6005195678046935], "prior_entropy": 0.6728004379125668, "posterior_entropy": 0.6728004379125668, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6005195678046935, "accuracy": 1, "tokens_in": 56, "tokens_out": 19, "tokens_total": 75, "latency_total": 0.008754843000133405, "latency_per_module": {"scorer": 0.008754843000133405}, "tokens_per_module": {"scorer": {"tokens_in": 56, "tokens_out": 19, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I loved to make my wife laugh. I started making her laugh again and she became happy!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I loved to make my wife laugh. I started making her laugh again and she became happy!", "hypotheses": ["The my wife became sad.", "My husband was sad so I thought I would cheer her up."]}
{"example_id": "177", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5202507998250527, 0.4797492001749473], "posterior_probs": [0.5202507998250527, 0.4797492001749473], "prior_entropy": 0.692326766386478, "posterior_entropy": 0.692326766386478, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5202507998250527, "accuracy": 0, "tokens_in": 52, "tokens_out": 13, "tokens_total": 65, "latency_total": 0.009127809000347042, "latency_per_module": {"scorer": 0.009127809000347042}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 13, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Rachel was cooking dinner. Then she pulled herself together and took care of the cut.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Rachel was cooking dinner. Then she pulled herself together and took care of the cut.", "hypotheses": ["Rachel burned her hand.", "She cut her finger when chopping vegetables."]}
{"example_id": "178", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5855762530292931, 0.414423746970707], "posterior_probs": [0.5855762530292931, 0.414423746970707], "prior_entropy": 0.6784282315920453, "posterior_entropy": 0.6784282315920453, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5855762530292931, "accuracy": 0, "tokens_in": 44, "tokens_out": 20, "tokens_total": 64, "latency_total": 0.008654493999529222, "latency_per_module": {"scorer": 0.008654493999529222}, "tokens_per_module": {"scorer": {"tokens_in": 44, "tokens_out": 20, "tokens_total": 64}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam loved striped clothes. She began to wear stripes every day!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam loved striped clothes. She began to wear stripes every day!", "hypotheses": ["Sam got lots of complaints about her clothing.", "She decided to go out and buy and entire outfit."]}
{"example_id": "179", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5009149302761192, 0.49908506972388084], "posterior_probs": [0.5009149302761192, 0.49908506972388084], "prior_entropy": 0.6931455063621907, "posterior_entropy": 0.6931455063621907, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5009149302761192, "accuracy": 0, "tokens_in": 62, "tokens_out": 22, "tokens_total": 84, "latency_total": 0.00843490799979918, "latency_per_module": {"scorer": 0.00843490799979918}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 22, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jimmy's phone ringed in class. As a result, she called the security guard to take him away.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jimmy's phone ringed in class. As a result, she called the security guard to take him away.", "hypotheses": ["He ended up getting in trouble.", "Jimmy talked on the phone and wouldn't stop, delighting the professor."]}
{"example_id": "180", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.457286767228584, 0.542713232771416], "posterior_probs": [0.457286767228584, 0.542713232771416], "prior_entropy": 0.6894938890322678, "posterior_entropy": 0.6894938890322678, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.542713232771416, "accuracy": 1, "tokens_in": 62, "tokens_out": 18, "tokens_total": 80, "latency_total": 0.00821739000002708, "latency_per_module": {"scorer": 0.00821739000002708}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.", "hypotheses": ["She severely undercooked the chicken and badly burned the potatoes.", "Kelly's dinner was tasty."]}
{"example_id": "181", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5152120120544086, 0.4847879879455914], "posterior_probs": [0.5152120120544086, 0.4847879879455914], "prior_entropy": 0.6926842985121199, "posterior_entropy": 0.6926842985121199, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5152120120544086, "accuracy": 0, "tokens_in": 50, "tokens_out": 16, "tokens_total": 66, "latency_total": 0.008390795000195794, "latency_per_module": {"scorer": 0.008390795000195794}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 16, "tokens_total": 66}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amber was scared about her future. She was no longer worried about her future.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber was scared about her future. She was no longer worried about her future.", "hypotheses": ["Amber talked to her academic advisor.", "Amber talked to her psychic advisor."]}
{"example_id": "182", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5951790614049827, 0.40482093859501733], "posterior_probs": [0.5951790614049827, 0.40482093859501733], "prior_entropy": 0.6749180336440842, "posterior_entropy": 0.6749180336440842, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5951790614049827, "accuracy": 0, "tokens_in": 62, "tokens_out": 13, "tokens_total": 75, "latency_total": 0.0068740369997613016, "latency_per_module": {"scorer": 0.0068740369997613016}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 13, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.", "hypotheses": ["Terry practiced for a long time.", "terry was so big."]}
{"example_id": "183", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5548732862741387, 0.4451267137258613], "posterior_probs": [0.5548732862741387, 0.4451267137258613], "prior_entropy": 0.6871128780618596, "posterior_entropy": 0.6871128780618596, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5548732862741387, "accuracy": 0, "tokens_in": 60, "tokens_out": 22, "tokens_total": 82, "latency_total": 0.008508486000209814, "latency_per_module": {"scorer": 0.008508486000209814}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 22, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.", "hypotheses": ["Sara was outside looking at the neighbor's dog.", "Sara was outside looking at the neighbor's underwear."]}
{"example_id": "184", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.3762754695839405, 0.6237245304160595], "posterior_probs": [0.3762754695839405, 0.6237245304160595], "prior_entropy": 0.6622113117156057, "posterior_entropy": 0.6622113117156057, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6237245304160595, "accuracy": 1, "tokens_in": 58, "tokens_out": 27, "tokens_total": 85, "latency_total": 0.0072974549993887194, "latency_per_module": {"scorer": 0.0072974549993887194}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 27, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Monica was at the library with her boyfriend. She kicked them out because they were loitering.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Monica was at the library with her boyfriend. She kicked them out because they were loitering.", "hypotheses": ["Children brought food and left their trash laying on the reading table.", "The librarian noticed Monica and her boyfriend were just hanging out reading."]}
{"example_id": "185", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4431772394760715, 0.5568227605239285], "posterior_probs": [0.4431772394760715, 0.5568227605239285], "prior_entropy": 0.6866755555945623, "posterior_entropy": 0.6866755555945623, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5568227605239285, "accuracy": 1, "tokens_in": 52, "tokens_out": 30, "tokens_total": 82, "latency_total": 0.006935572999282158, "latency_per_module": {"scorer": 0.006935572999282158}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 30, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.", "hypotheses": ["Jen applied her mother's makeup and looked like a clown.", "Jen's mother let her put on her own makeup but Jen's friends complimented her."]}
{"example_id": "186", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.42914630227747175, 0.5708536977225283], "posterior_probs": [0.42914630227747175, 0.5708536977225283], "prior_entropy": 0.6830728109033353, "posterior_entropy": 0.6830728109033353, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5708536977225283, "accuracy": 1, "tokens_in": 64, "tokens_out": 32, "tokens_total": 96, "latency_total": 0.007140015000004496, "latency_per_module": {"scorer": 0.007140015000004496}, "tokens_per_module": {"scorer": {"tokens_in": 64, "tokens_out": 32, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.", "hypotheses": ["He would run outside but nobody would be there.", "Ted got sick of being woken up, so he stayed up all night to moved out of his house."]}
{"example_id": "187", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4651342558008747, 0.5348657441991254], "posterior_probs": [0.4651342558008747, 0.5348657441991254], "prior_entropy": 0.6907139661688809, "posterior_entropy": 0.6907139661688809, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5348657441991254, "accuracy": 1, "tokens_in": 72, "tokens_out": 15, "tokens_total": 87, "latency_total": 0.007151716999942437, "latency_per_module": {"scorer": 0.007151716999942437}, "tokens_per_module": {"scorer": {"tokens_in": 72, "tokens_out": 15, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!", "hypotheses": ["They saw the monster themselves.", "Neil asked the locals where to find it."]}
{"example_id": "188", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.3994401099647912, 0.6005598900352088], "posterior_probs": [0.3994401099647912, 0.6005598900352088], "prior_entropy": 0.672783997955092, "posterior_entropy": 0.672783997955092, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6005598900352088, "accuracy": 1, "tokens_in": 62, "tokens_out": 18, "tokens_total": 80, "latency_total": 0.006925910000063595, "latency_per_module": {"scorer": 0.006925910000063595}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.", "hypotheses": ["It was a tornado outside.", "It was raining and I did not want to get wet."]}
{"example_id": "189", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5614921399756847, 0.4385078600243153], "posterior_probs": [0.5614921399756847, 0.4385078600243153], "prior_entropy": 0.6855654335802226, "posterior_entropy": 0.6855654335802226, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5614921399756847, "accuracy": 0, "tokens_in": 66, "tokens_out": 16, "tokens_total": 82, "latency_total": 0.006993675000558142, "latency_per_module": {"scorer": 0.006993675000558142}, "tokens_per_module": {"scorer": {"tokens_in": 66, "tokens_out": 16, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.", "hypotheses": ["Jim's wife knew he was tired and made coffee.", "jim made breakfast."]}
{"example_id": "190", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.6073004646067773, 0.3926995353932227], "posterior_probs": [0.6073004646067773, 0.3926995353932227], "prior_entropy": 0.6699403183745066, "posterior_entropy": 0.6699403183745066, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6073004646067773, "accuracy": 0, "tokens_in": 52, "tokens_out": 25, "tokens_total": 77, "latency_total": 0.0068363079999471665, "latency_per_module": {"scorer": 0.0068363079999471665}, "tokens_per_module": {"scorer": {"tokens_in": 52, "tokens_out": 25, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: The family prepared the food and packed it away. The family had a horrible day.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The family prepared the food and packed it away. The family had a horrible day.", "hypotheses": ["After the picnic, it started raining.", "Popped a tire and spent their picnic time waiting for a tow-truck."]}
{"example_id": "191", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5497691534768302, 0.4502308465231699], "posterior_probs": [0.5497691534768302, 0.4502308465231699], "prior_entropy": 0.6881850301904786, "posterior_entropy": 0.6881850301904786, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5497691534768302, "accuracy": 0, "tokens_in": 50, "tokens_out": 17, "tokens_total": 67, "latency_total": 0.00674079799955507, "latency_per_module": {"scorer": 0.00674079799955507}, "tokens_per_module": {"scorer": {"tokens_in": 50, "tokens_out": 17, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim never locked his bathroom window. Tim was glad he kept the window unlocked.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim never locked his bathroom window. Tim was glad he kept the window unlocked.", "hypotheses": ["Tim was able to break through the window.", "Tim lost his keys to the house."]}
{"example_id": "192", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.4674719487572201, 0.5325280512427799], "posterior_probs": [0.4674719487572201, 0.5325280512427799], "prior_entropy": 0.6910295370954334, "posterior_entropy": 0.6910295370954334, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5325280512427799, "accuracy": 1, "tokens_in": 62, "tokens_out": 34, "tokens_total": 96, "latency_total": 0.007076341999891156, "latency_per_module": {"scorer": 0.007076341999891156}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 34, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.", "hypotheses": ["The people that live on the old farm has a dog that fears nothing.", "There is an old dog on the farm who has lost its mind and barks all day."]}
{"example_id": "193", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5402261165773485, 0.4597738834226515], "posterior_probs": [0.5402261165773485, 0.4597738834226515], "prior_entropy": 0.6899073994065836, "posterior_entropy": 0.6899073994065836, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5402261165773485, "accuracy": 0, "tokens_in": 62, "tokens_out": 20, "tokens_total": 82, "latency_total": 0.0069219380002323305, "latency_per_module": {"scorer": 0.0069219380002323305}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.", "hypotheses": ["Oren got a part time job delivering pizza.", "he went inside and got a part time job."]}
{"example_id": "194", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5162868080307631, 0.48371319196923696], "posterior_probs": [0.5162868080307631, 0.48371319196923696], "prior_entropy": 0.6926165664692046, "posterior_entropy": 0.6926165664692046, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5162868080307631, "accuracy": 0, "tokens_in": 74, "tokens_out": 21, "tokens_total": 95, "latency_total": 0.007037342000330682, "latency_per_module": {"scorer": 0.007037342000330682}, "tokens_per_module": {"scorer": {"tokens_in": 74, "tokens_out": 21, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.", "hypotheses": ["Beth passed out from overeating.", "Beth went out and broke her phone with no ride home."]}
{"example_id": "195", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5288894510173028, 0.4711105489826972], "posterior_probs": [0.5288894510173028, 0.4711105489826972], "prior_entropy": 0.6914770498116267, "posterior_entropy": 0.6914770498116267, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5288894510173028, "accuracy": 0, "tokens_in": 62, "tokens_out": 38, "tokens_total": 100, "latency_total": 0.007656976000362192, "latency_per_module": {"scorer": 0.007656976000362192}, "tokens_per_module": {"scorer": {"tokens_in": 62, "tokens_out": 38, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.", "hypotheses": ["Everyone realized that the power was out for a long time.", "When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview."]}
{"example_id": "196", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.564453773628524, 0.435546226371476], "posterior_probs": [0.564453773628524, 0.435546226371476], "prior_entropy": 0.6848154375403865, "posterior_entropy": 0.6848154375403865, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.564453773628524, "accuracy": 0, "tokens_in": 58, "tokens_out": 23, "tokens_total": 81, "latency_total": 0.00772307900024316, "latency_per_module": {"scorer": 0.00772307900024316}, "tokens_per_module": {"scorer": {"tokens_in": 58, "tokens_out": 23, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!", "hypotheses": ["They were both very shy, but were attracted to eachother without even knowing!.", "She went under to say hello."]}
{"example_id": "197", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5572402122655649, 0.44275978773443514], "posterior_probs": [0.5572402122655649, 0.44275978773443514], "prior_entropy": 0.6865799077622886, "posterior_entropy": 0.6865799077622886, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5572402122655649, "accuracy": 0, "tokens_in": 70, "tokens_out": 19, "tokens_total": 89, "latency_total": 0.00789478299975599, "latency_per_module": {"scorer": 0.00789478299975599}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 19, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.", "hypotheses": ["the man failed to get a job, until recently.", "The man eventually gave up on looking."]}
{"example_id": "198", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.5240675038940131, 0.47593249610598687], "posterior_probs": [0.5240675038940131, 0.47593249610598687], "prior_entropy": 0.6919882432894742, "posterior_entropy": 0.6919882432894742, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5240675038940131, "accuracy": 0, "tokens_in": 60, "tokens_out": 33, "tokens_total": 93, "latency_total": 0.008103774999653979, "latency_per_module": {"scorer": 0.008103774999653979}, "tokens_per_module": {"scorer": {"tokens_in": 60, "tokens_out": 33, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.", "hypotheses": ["Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake.", "Chad didn't eat cause he had the flu."]}
{"example_id": "199", "dataset": "art", "method": "direct", "asked": false, "q": "", "a": "", "prior_probs": [0.38821194630071754, 0.6117880536992824], "posterior_probs": [0.38821194630071754, 0.6117880536992824], "prior_entropy": 0.667941545366203, "posterior_entropy": 0.667941545366203, "delta_entropy": 0.0, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6117880536992824, "accuracy": 1, "tokens_in": 42, "tokens_out": 23, "tokens_total": 65, "latency_total": 0.009081589000743406, "latency_per_module": {"scorer": 0.009081589000743406}, "tokens_per_module": {"scorer": {"tokens_in": 42, "tokens_out": 23, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amber loves to read. She went to the library instead.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber loves to read. She went to the library instead.", "hypotheses": ["Ambers friends wanted to see her at the library one night.", "Amber liked having a book in her hand."]}
{"example_id": "0", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ron ignores his bosses's orders and called him an idiot. happen?", "a": "no", "prior_probs": [0.4620790510502866, 0.5379209489497133], "posterior_probs": [0.578627754945582, 0.421372245054418], "prior_entropy": 0.690268420337182, "posterior_entropy": 0.680731060478617, "delta_entropy": 0.009537359858565075, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.578627754945582, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.016719444000955264, "latency_per_module": {"scorer": 0.008476703000269481, "prior": 0.008242741000685783}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.\nQuestion: Did Ron ignores his bosses's orders and called him an idiot. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.", "hypotheses": ["Ron ignores his bosses's orders and called him an idiot.", "Ron's boss called him an idiot."]}
{"example_id": "1", "dataset": "art", "method": "random_question", "asked": true, "q": "Did It stormed in New York. happen?", "a": "no", "prior_probs": [0.49074052349518316, 0.5092594765048167], "posterior_probs": [0.6287766711663705, 0.3712233288336295], "prior_entropy": 0.6929756949449963, "posterior_entropy": 0.6596035482432574, "delta_entropy": 0.033372146701738936, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6287766711663705, "accuracy": 0, "tokens_in": 108, "tokens_out": 24, "tokens_total": 132, "latency_total": 0.014823195999269956, "latency_per_module": {"scorer": 0.007596995999847422, "prior": 0.0072261999994225334}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 12, "tokens_total": 82}, "prior": {"tokens_in": 38, "tokens_out": 12, "tokens_total": 50}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sandy lived in New York. Sandy was prepared.\nQuestion: Did It stormed in New York. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sandy lived in New York. Sandy was prepared.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sandy lived in New York. Sandy was prepared.", "hypotheses": ["It stormed in New York.", "She partied all night."]}
{"example_id": "2", "dataset": "art", "method": "random_question", "asked": true, "q": "Did So Mary made pineapple splits for everyone. happen?", "a": "yes", "prior_probs": [0.5653405163187792, 0.4346594836812207], "posterior_probs": [0.42306656788177227, 0.5769334321182277], "prior_entropy": 0.6845839433504841, "posterior_entropy": 0.681262517726735, "delta_entropy": 0.0033214256237491524, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5769334321182277, "accuracy": 1, "tokens_in": 172, "tokens_out": 46, "tokens_total": 218, "latency_total": 0.015768854999805626, "latency_per_module": {"scorer": 0.00821834399994259, "prior": 0.007550510999863036}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}, "prior": {"tokens_in": 68, "tokens_out": 23, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!\nQuestion: Did So Mary made pineapple splits for everyone. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!", "hypotheses": ["Mary and her mom decided to make chocolate covered frozen bananas to avoid waste.", "So Mary made pineapple splits for everyone."]}
{"example_id": "3", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jim found he was missing an item. happen?", "a": "no", "prior_probs": [0.5590702485504583, 0.44092975144954183], "posterior_probs": [0.6138264604825773, 0.3861735395174226], "prior_entropy": 0.6861522671508851, "posterior_entropy": 0.6670056546319258, "delta_entropy": 0.019146612518959216, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6138264604825773, "accuracy": 0, "tokens_in": 140, "tokens_out": 32, "tokens_total": 172, "latency_total": 0.015260432999639306, "latency_per_module": {"scorer": 0.007719891999840911, "prior": 0.007540540999798395}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}, "prior": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jim was working on a project. Luckily, he found it on a nearby shelf.\nQuestion: Did Jim found he was missing an item. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jim was working on a project. Luckily, he found it on a nearby shelf.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was working on a project. Luckily, he found it on a nearby shelf.", "hypotheses": ["Jim found he was missing an item.", "Jim needed a certain animal for it."]}
{"example_id": "4", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He noticed the chair leg was falling off. happen?", "a": "no", "prior_probs": [0.5075695691361044, 0.49243043086389565], "posterior_probs": [0.6052906056334861, 0.394709394366514], "prior_entropy": 0.6930325794262594, "posterior_entropy": 0.6708081104365207, "delta_entropy": 0.022224468989738733, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6052906056334861, "accuracy": 0, "tokens_in": 158, "tokens_out": 40, "tokens_total": 198, "latency_total": 0.015280434999112913, "latency_per_module": {"scorer": 0.007829102999494353, "prior": 0.00745133199961856}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sean was sitting at his desk. After a minute, he was able to put the chair back together.\nQuestion: Did He noticed the chair leg was falling off. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sean was sitting at his desk. After a minute, he was able to put the chair back together.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sean was sitting at his desk. After a minute, he was able to put the chair back together.", "hypotheses": ["He noticed the chair leg was falling off.", "He leaned too far back and his chair tipped over."]}
{"example_id": "5", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Pablo thought that worms were a delicious source of protein. happen?", "a": "no", "prior_probs": [0.6068721153854889, 0.39312788461451104], "posterior_probs": [0.663092299505226, 0.3369077004947741], "prior_entropy": 0.6701266849595834, "posterior_entropy": 0.6389630355504997, "delta_entropy": 0.03116364940908367, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.663092299505226, "accuracy": 0, "tokens_in": 130, "tokens_out": 42, "tokens_total": 172, "latency_total": 0.014797812999859161, "latency_per_module": {"scorer": 0.007735066999885021, "prior": 0.00706274599997414}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 21, "tokens_total": 107}, "prior": {"tokens_in": 44, "tokens_out": 21, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Pablo likes to eat worms. Pablo does not enjoy eating worms.\nQuestion: Did Pablo thought that worms were a delicious source of protein. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Pablo likes to eat worms. Pablo does not enjoy eating worms.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pablo likes to eat worms. Pablo does not enjoy eating worms.", "hypotheses": ["Pablo thought that worms were a delicious source of protein.", "Pablo then learned what worms really are."]}
{"example_id": "6", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The scientist collected samples of the bacteria and tested them. happen?", "a": "no", "prior_probs": [0.5630861110677394, 0.4369138889322606], "posterior_probs": [0.6252826534032552, 0.3747173465967448], "prior_entropy": 0.6851662110791645, "posterior_entropy": 0.661418681099528, "delta_entropy": 0.023747529979636584, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6252826534032552, "accuracy": 0, "tokens_in": 146, "tokens_out": 38, "tokens_total": 184, "latency_total": 0.014888565999171988, "latency_per_module": {"scorer": 0.0076477329994304455, "prior": 0.007240832999741542}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 19, "tokens_total": 113}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.\nQuestion: Did The scientist collected samples of the bacteria and tested them. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.", "hypotheses": ["The scientist collected samples of the bacteria and tested them.", "He collected the bacteria and froze it."]}
{"example_id": "7", "dataset": "art", "method": "random_question", "asked": true, "q": "Did My commanding officer told me I wasn't doing bad at my job. happen?", "a": "no", "prior_probs": [0.6387544124977744, 0.36124558750222546], "posterior_probs": [0.7042901005113327, 0.2957098994886673], "prior_entropy": 0.6541314959287188, "posterior_entropy": 0.6071853672294203, "delta_entropy": 0.046946128699298484, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7042901005113327, "accuracy": 0, "tokens_in": 164, "tokens_out": 42, "tokens_total": 206, "latency_total": 0.014902384000379243, "latency_per_module": {"scorer": 0.007561045000329614, "prior": 0.007341339000049629}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 21, "tokens_total": 127}, "prior": {"tokens_in": 58, "tokens_out": 21, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I joined the Navy. That angered me so I hit him and was arrested by the military police.\nQuestion: Did My commanding officer told me I wasn't doing bad at my job. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I joined the Navy. That angered me so I hit him and was arrested by the military police.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I joined the Navy. That angered me so I hit him and was arrested by the military police.", "hypotheses": ["My commanding officer told me I wasn't doing bad at my job.", "My drill sergeant insulted my mother."]}
{"example_id": "8", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Dotty call some close friends to chat. happen?", "a": "yes", "prior_probs": [0.5659128800175401, 0.43408711998245986], "posterior_probs": [0.38434219064382263, 0.6156578093561773], "prior_entropy": 0.6844328221404599, "posterior_entropy": 0.6661498825742213, "delta_entropy": 0.01828293956623861, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6156578093561773, "accuracy": 1, "tokens_in": 130, "tokens_out": 34, "tokens_total": 164, "latency_total": 0.014367423999829043, "latency_per_module": {"scorer": 0.007560066999758419, "prior": 0.006807357000070624}, "tokens_per_module": {"scorer": {"tokens_in": 84, "tokens_out": 17, "tokens_total": 101}, "prior": {"tokens_in": 46, "tokens_out": 17, "tokens_total": 63}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dotty was being very grumpy. She felt much better afterwards.\nQuestion: Did Dotty call some close friends to chat. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Dotty was being very grumpy. She felt much better afterwards.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dotty was being very grumpy. She felt much better afterwards.", "hypotheses": ["Dotty ate something bad.", "Dotty call some close friends to chat."]}
{"example_id": "9", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ali did not want to take karate. happen?", "a": "no", "prior_probs": [0.5599978275854001, 0.4400021724145999], "posterior_probs": [0.6140151489071012, 0.3859848510928988], "prior_entropy": 0.6859303241447694, "posterior_entropy": 0.666918136520481, "delta_entropy": 0.019012187624288357, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6140151489071012, "accuracy": 0, "tokens_in": 170, "tokens_out": 34, "tokens_total": 204, "latency_total": 0.014763889000278141, "latency_per_module": {"scorer": 0.007541981000031228, "prior": 0.007221908000246913}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}, "prior": {"tokens_in": 66, "tokens_out": 17, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.\nQuestion: Did Ali did not want to take karate. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.", "hypotheses": ["Ali did not want to take karate.", "Ali did horribly in her last class."]}
{"example_id": "10", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Cory was teased by some of the kids in his classroom. happen?", "a": "no", "prior_probs": [0.4883579795936534, 0.5116420204063465], "posterior_probs": [0.6087945220562659, 0.391205477943734], "prior_entropy": 0.6928760827807745, "posterior_entropy": 0.6692842588175162, "delta_entropy": 0.023591823963258318, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6087945220562659, "accuracy": 0, "tokens_in": 172, "tokens_out": 50, "tokens_total": 222, "latency_total": 0.015575352001178544, "latency_per_module": {"scorer": 0.008008301000700158, "prior": 0.007567051000478386}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 25, "tokens_total": 133}, "prior": {"tokens_in": 64, "tokens_out": 25, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.\nQuestion: Did Cory was teased by some of the kids in his classroom. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.", "hypotheses": ["Cory was teased by some of the kids in his classroom.", "Cory ran away from home as fast as he could."]}
{"example_id": "11", "dataset": "art", "method": "random_question", "asked": true, "q": "Did People went to watch the band play. happen?", "a": "no", "prior_probs": [0.5310383771489922, 0.4689616228510078], "posterior_probs": [0.6190707065246792, 0.3809292934753208], "prior_entropy": 0.6912191794642322, "posterior_entropy": 0.6645172283722175, "delta_entropy": 0.026701951092014675, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6190707065246792, "accuracy": 0, "tokens_in": 160, "tokens_out": 36, "tokens_total": 196, "latency_total": 0.015682467000260658, "latency_per_module": {"scorer": 0.007755980999718304, "prior": 0.007926486000542354}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 18, "tokens_total": 116}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.\nQuestion: Did People went to watch the band play. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.", "hypotheses": ["People went to watch the band play.", "Dennis has been a member for ten seconds."]}
{"example_id": "12", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Deb went to a matinee movie instead. happen?", "a": "no", "prior_probs": [0.42302664764035913, 0.5769733523596409], "posterior_probs": [0.6243510748762258, 0.3756489251237743], "prior_entropy": 0.6812501313089362, "posterior_entropy": 0.6618938275892394, "delta_entropy": 0.019356303719696766, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6243510748762258, "accuracy": 0, "tokens_in": 144, "tokens_out": 34, "tokens_total": 178, "latency_total": 0.014942518999305321, "latency_per_module": {"scorer": 0.0076993889997538645, "prior": 0.007243129999551456}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 17, "tokens_total": 109}, "prior": {"tokens_in": 52, "tokens_out": 17, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Deb wanted to go shopping. She found everything she needed and had money left over.\nQuestion: Did Deb went to a matinee movie instead. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Deb wanted to go shopping. She found everything she needed and had money left over.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Deb wanted to go shopping. She found everything she needed and had money left over.", "hypotheses": ["Deb went to a matinee movie instead.", "Deb had a lot of coupons."]}
{"example_id": "13", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kory stole from the airport. happen?", "a": "no", "prior_probs": [0.617982844061609, 0.38201715593839114], "posterior_probs": [0.642384061401763, 0.357615938598237], "prior_entropy": 0.6650429923305812, "posterior_entropy": 0.6520341448372071, "delta_entropy": 0.013008847493374054, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.642384061401763, "accuracy": 0, "tokens_in": 142, "tokens_out": 36, "tokens_total": 178, "latency_total": 0.01461939200089546, "latency_per_module": {"scorer": 0.007526338000388932, "prior": 0.007093054000506527}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 18, "tokens_total": 106}, "prior": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My cousin Kory was working at the airport. He is now serving out his sentence.\nQuestion: Did Kory stole from the airport. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My cousin Kory was working at the airport. He is now serving out his sentence.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My cousin Kory was working at the airport. He is now serving out his sentence.", "hypotheses": ["Kory stole from the airport.", "He got caught anti-shoplifting from passengers."]}
{"example_id": "14", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He opened a lemonade stand. happen?", "a": "no", "prior_probs": [0.3696246176351904, 0.6303753823648096], "posterior_probs": [0.523637780949381, 0.476362219050619], "prior_entropy": 0.6587555935763323, "posterior_entropy": 0.6920292745481083, "delta_entropy": -0.033273680971775965, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.523637780949381, "accuracy": 0, "tokens_in": 194, "tokens_out": 38, "tokens_total": 232, "latency_total": 0.014387645999704546, "latency_per_module": {"scorer": 0.007063994000418461, "prior": 0.007323651999286085}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 19, "tokens_total": 133}, "prior": {"tokens_in": 80, "tokens_out": 19, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!\nQuestion: Did He opened a lemonade stand. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!", "hypotheses": ["He opened a lemonade stand.", "Daniel stayed home and didn't want to buy a plane."]}
{"example_id": "15", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Her neck pain stopped because of this. happen?", "a": "no", "prior_probs": [0.5233176960502864, 0.47668230394971345], "posterior_probs": [0.6492351193543503, 0.3507648806456497], "prior_entropy": 0.6920593561485264, "posterior_entropy": 0.6479188447660982, "delta_entropy": 0.04414051138242825, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6492351193543503, "accuracy": 0, "tokens_in": 156, "tokens_out": 32, "tokens_total": 188, "latency_total": 0.015066337999996904, "latency_per_module": {"scorer": 0.007405283000480267, "prior": 0.007661054999516637}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}, "prior": {"tokens_in": 60, "tokens_out": 16, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.\nQuestion: Did Her neck pain stopped because of this. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.", "hypotheses": ["Her neck pain stopped because of this.", "Jenna pulled a muscle lifting weights."]}
{"example_id": "16", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kat decided to take a nap instead of eating. happen?", "a": "yes", "prior_probs": [0.4115371464631088, 0.5884628535368912], "posterior_probs": [0.37231178719893504, 0.6276882128010649], "prior_entropy": 0.6774131328021895, "posterior_entropy": 0.6601745988312963, "delta_entropy": 0.017238533970893255, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6276882128010649, "accuracy": 1, "tokens_in": 152, "tokens_out": 34, "tokens_total": 186, "latency_total": 0.016437429000689008, "latency_per_module": {"scorer": 0.007790482000018528, "prior": 0.00864694700067048}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}, "prior": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.\nQuestion: Did Kat decided to take a nap instead of eating. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.", "hypotheses": ["Kat went to get a salad.", "Kat decided to take a nap instead of eating."]}
{"example_id": "17", "dataset": "art", "method": "random_question", "asked": true, "q": "Did His owner gave him a lower fat cat food. happen?", "a": "no", "prior_probs": [0.43587771834479677, 0.5641222816552033], "posterior_probs": [0.6431403144986843, 0.3568596855013158], "prior_entropy": 0.6849011558642406, "posterior_entropy": 0.6515899420076727, "delta_entropy": 0.03331121385656788, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6431403144986843, "accuracy": 0, "tokens_in": 140, "tokens_out": 38, "tokens_total": 178, "latency_total": 0.014658527999927173, "latency_per_module": {"scorer": 0.0072962579997692956, "prior": 0.007362270000157878}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 19, "tokens_total": 109}, "prior": {"tokens_in": 50, "tokens_out": 19, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Cosmo was a pudgy cat. Now he's fit and muscular!\nQuestion: Did His owner gave him a lower fat cat food. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Cosmo was a pudgy cat. Now he's fit and muscular!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cosmo was a pudgy cat. Now he's fit and muscular!", "hypotheses": ["His owner gave him a lower fat cat food.", "The vet put Cosmo on a treadmill."]}
{"example_id": "18", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tim could not find his socks. happen?", "a": "yes", "prior_probs": [0.4609254882025264, 0.5390745117974737], "posterior_probs": [0.42281367909483364, 0.5771863209051663], "prior_entropy": 0.6900904297666779, "posterior_entropy": 0.6811839412802924, "delta_entropy": 0.008906488486385533, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5771863209051663, "accuracy": 1, "tokens_in": 178, "tokens_out": 28, "tokens_total": 206, "latency_total": 0.014087962999838055, "latency_per_module": {"scorer": 0.0067975239999213954, "prior": 0.00729043899991666}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 14, "tokens_total": 120}, "prior": {"tokens_in": 72, "tokens_out": 14, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.\nQuestion: Did Tim could not find his socks. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.", "hypotheses": ["Tim became very sick one day.", "Tim could not find his socks."]}
{"example_id": "19", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Christian grabbed the gun and shot Adam in the eye. happen?", "a": "yes", "prior_probs": [0.4207589138170007, 0.5792410861829993], "posterior_probs": [0.3602046531786242, 0.6397953468213757], "prior_entropy": 0.6805357754094549, "posterior_entropy": 0.6535358540085932, "delta_entropy": 0.026999921400861715, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6397953468213757, "accuracy": 1, "tokens_in": 142, "tokens_out": 42, "tokens_total": 184, "latency_total": 0.014947579998988658, "latency_per_module": {"scorer": 0.007615351999447739, "prior": 0.007332227999540919}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}, "prior": {"tokens_in": 50, "tokens_out": 21, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: One day Adam bought two BB guns. Adam took the gun away from Christian.\nQuestion: Did Christian grabbed the gun and shot Adam in the eye. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: One day Adam bought two BB guns. Adam took the gun away from Christian.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "One day Adam bought two BB guns. Adam took the gun away from Christian.", "hypotheses": ["Adam's brother Christian was afraid of the guns.", "Christian grabbed the gun and shot Adam in the eye."]}
{"example_id": "20", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She set up a hunting blind in the woods. happen?", "a": "no", "prior_probs": [0.41679212274760574, 0.5832078772523943], "posterior_probs": [0.5958738373510231, 0.40412616264897694], "prior_entropy": 0.6792354461007641, "posterior_entropy": 0.6746492529530614, "delta_entropy": 0.0045861931477027, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5958738373510231, "accuracy": 0, "tokens_in": 136, "tokens_out": 44, "tokens_total": 180, "latency_total": 0.014183203999891703, "latency_per_module": {"scorer": 0.007190950000222074, "prior": 0.006992253999669629}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 22, "tokens_total": 110}, "prior": {"tokens_in": 48, "tokens_out": 22, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My friend is a hunter. The elk was nowhere to be found.\nQuestion: Did She set up a hunting blind in the woods. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My friend is a hunter. The elk was nowhere to be found.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend is a hunter. The elk was nowhere to be found.", "hypotheses": ["She set up a hunting blind in the woods.", "My friend who is a hunter found lots of elk."]}
{"example_id": "21", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I saw the string by the door. happen?", "a": "no", "prior_probs": [0.4435230981074238, 0.5564769018925761], "posterior_probs": [0.607592912066996, 0.392407087933004], "prior_entropy": 0.6867542648829669, "posterior_entropy": 0.6698126381309877, "delta_entropy": 0.01694162675197919, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.607592912066996, "accuracy": 0, "tokens_in": 120, "tokens_out": 32, "tokens_total": 152, "latency_total": 0.013740299999881245, "latency_per_module": {"scorer": 0.007045446000120137, "prior": 0.006694853999761108}, "tokens_per_module": {"scorer": {"tokens_in": 78, "tokens_out": 16, "tokens_total": 94}, "prior": {"tokens_in": 42, "tokens_out": 16, "tokens_total": 58}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I walked into my math class. I ended up failing.\nQuestion: Did I saw the string by the door. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I walked into my math class. I ended up failing.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I walked into my math class. I ended up failing.", "hypotheses": ["I saw the string by the door.", "I didn't study for the test."]}
{"example_id": "22", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Our founder Rachel only uses the PC. happen?", "a": "yes", "prior_probs": [0.5059038040919378, 0.4940961959080622], "posterior_probs": [0.3821787094205718, 0.6178212905794281], "prior_entropy": 0.693077469132524, "posterior_entropy": 0.6651206435026491, "delta_entropy": 0.027956825629874893, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6178212905794281, "accuracy": 1, "tokens_in": 172, "tokens_out": 34, "tokens_total": 206, "latency_total": 0.013631101000100898, "latency_per_module": {"scorer": 0.006496902999970189, "prior": 0.007134198000130709}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}, "prior": {"tokens_in": 68, "tokens_out": 17, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.\nQuestion: Did Our founder Rachel only uses the PC. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.", "hypotheses": ["we bought the owners grandmother a new pc.", "Our founder Rachel only uses the PC."]}
{"example_id": "23", "dataset": "art", "method": "random_question", "asked": true, "q": "Did It seemed that the cold weather stopped for two months. happen?", "a": "yes", "prior_probs": [0.432870459419034, 0.5671295405809661], "posterior_probs": [0.32125783913224776, 0.6787421608677522], "prior_entropy": 0.6841071564298835, "posterior_entropy": 0.6278139482787215, "delta_entropy": 0.05629320815116201, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6787421608677522, "accuracy": 1, "tokens_in": 154, "tokens_out": 32, "tokens_total": 186, "latency_total": 0.01388302899977134, "latency_per_module": {"scorer": 0.0071094989998528035, "prior": 0.006773529999918537}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 16, "tokens_total": 114}, "prior": {"tokens_in": 56, "tokens_out": 16, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary doesn't like cold weather. At least until she can afford to move to warmer state.\nQuestion: Did It seemed that the cold weather stopped for two months. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Mary doesn't like cold weather. At least until she can afford to move to warmer state.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary doesn't like cold weather. At least until she can afford to move to warmer state.", "hypotheses": ["Mary wears two jackets.", "It seemed that the cold weather stopped for two months."]}
{"example_id": "24", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers. happen?", "a": "yes", "prior_probs": [0.5128077854715616, 0.48719221452843847], "posterior_probs": [0.2547552668331887, 0.7452447331668112], "prior_entropy": 0.6928190659326015, "posterior_entropy": 0.5674992911138368, "delta_entropy": 0.12531977481876466, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7452447331668112, "accuracy": 1, "tokens_in": 210, "tokens_out": 90, "tokens_total": 300, "latency_total": 0.016319417999511643, "latency_per_module": {"scorer": 0.00899604599999293, "prior": 0.007323371999518713}, "tokens_per_module": {"scorer": {"tokens_in": 144, "tokens_out": 45, "tokens_total": 189}, "prior": {"tokens_in": 66, "tokens_out": 45, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.\nQuestion: Did The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.", "hypotheses": ["They started getting followed by a policeman, ran, and hid behind a building.", "The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers."]}
{"example_id": "25", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Bob got caught sneaking out. happen?", "a": "no", "prior_probs": [0.47422526177721863, 0.5257747382227814], "posterior_probs": [0.5519046509414592, 0.4480953490585407], "prior_entropy": 0.6918179172122214, "posterior_entropy": 0.6877492755067779, "delta_entropy": 0.004068641705443521, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5519046509414592, "accuracy": 0, "tokens_in": 148, "tokens_out": 26, "tokens_total": 174, "latency_total": 0.014054787000532087, "latency_per_module": {"scorer": 0.0071896310000738595, "prior": 0.006865156000458228}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 13, "tokens_total": 103}, "prior": {"tokens_in": 58, "tokens_out": 13, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob's parents grounded him. He came back home but his parents didn't even know he left.\nQuestion: Did Bob got caught sneaking out. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Bob's parents grounded him. He came back home but his parents didn't even know he left.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob's parents grounded him. He came back home but his parents didn't even know he left.", "hypotheses": ["Bob got caught sneaking out.", "Bob got away with sneaking out."]}
{"example_id": "26", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Amy won an award for how much work she accomplished and was given the same quota. happen?", "a": "no", "prior_probs": [0.48297515281453285, 0.5170248471854672], "posterior_probs": [0.6638150680427155, 0.3361849319572845], "prior_entropy": 0.6925673776487646, "posterior_entropy": 0.6384724756259421, "delta_entropy": 0.05409490202282252, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6638150680427155, "accuracy": 0, "tokens_in": 178, "tokens_out": 54, "tokens_total": 232, "latency_total": 0.014874798000164446, "latency_per_module": {"scorer": 0.007862657000259787, "prior": 0.007012140999904659}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 27, "tokens_total": 143}, "prior": {"tokens_in": 62, "tokens_out": 27, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.\nQuestion: Did Amy won an award for how much work she accomplished and was given the same quota. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.", "hypotheses": ["Amy won an award for how much work she accomplished and was given the same quota.", "Amy's boss said she needed to do more."]}
{"example_id": "27", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jason learned to knit. happen?", "a": "yes", "prior_probs": [0.6126871071764336, 0.3873128928235664], "posterior_probs": [0.5002598762278195, 0.4997401237721805], "prior_entropy": 0.6675309227274598, "posterior_entropy": 0.6931470454866318, "delta_entropy": -0.02561612275917191, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5002598762278195, "accuracy": 0, "tokens_in": 158, "tokens_out": 40, "tokens_total": 198, "latency_total": 0.014065693999327777, "latency_per_module": {"scorer": 0.006951121999918541, "prior": 0.007114571999409236}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 20, "tokens_total": 114}, "prior": {"tokens_in": 64, "tokens_out": 20, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.\nQuestion: Did Jason learned to knit. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.", "hypotheses": ["He didn't let his inspiration go to waste, he trained and trained.", "Jason learned to knit."]}
{"example_id": "28", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Erin, practiced drawing at home with no luck. happen?", "a": "no", "prior_probs": [0.4758078557354764, 0.5241921442645237], "posterior_probs": [0.5713159913147557, 0.4286840086852443], "prior_entropy": 0.6919762037360642, "posterior_entropy": 0.6829404661028381, "delta_entropy": 0.00903573763322607, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5713159913147557, "accuracy": 0, "tokens_in": 136, "tokens_out": 50, "tokens_total": 186, "latency_total": 0.014741607998985273, "latency_per_module": {"scorer": 0.00777725899934012, "prior": 0.0069643489996451535}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 25, "tokens_total": 113}, "prior": {"tokens_in": 48, "tokens_out": 25, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Erin tried to learn how to draw. So she joined a drawing class.\nQuestion: Did Erin, practiced drawing at home with no luck. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Erin tried to learn how to draw. So she joined a drawing class.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erin tried to learn how to draw. So she joined a drawing class.", "hypotheses": ["Erin, practiced drawing at home with no luck.", "Erin, practiced drawing at home and became recognized for her talent."]}
{"example_id": "29", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jon crashed the police car into a telephone poll. happen?", "a": "no", "prior_probs": [0.47143509427231317, 0.5285649057276869], "posterior_probs": [0.6258173277287377, 0.37418267227126234], "prior_entropy": 0.6915143840109692, "posterior_entropy": 0.6611443006749416, "delta_entropy": 0.030370083336027642, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6258173277287377, "accuracy": 0, "tokens_in": 140, "tokens_out": 30, "tokens_total": 170, "latency_total": 0.014271525999902224, "latency_per_module": {"scorer": 0.0072714950001682155, "prior": 0.007000030999734008}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 15, "tokens_total": 105}, "prior": {"tokens_in": 50, "tokens_out": 15, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jon decided to steal a police car. Jon went to prison for three years.\nQuestion: Did Jon crashed the police car into a telephone poll. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jon decided to steal a police car. Jon went to prison for three years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jon decided to steal a police car. Jon went to prison for three years.", "hypotheses": ["Jon crashed the police car into a telephone poll.", "Jon wasn't caught."]}
{"example_id": "30", "dataset": "art", "method": "random_question", "asked": true, "q": "Did After getting a good grade, I learned an easy lesson. happen?", "a": "yes", "prior_probs": [0.47574630971646853, 0.5242536902835314], "posterior_probs": [0.34275030904397885, 0.6572496909560211], "prior_entropy": 0.6919702357676847, "posterior_entropy": 0.642842909583263, "delta_entropy": 0.04912732618442173, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6572496909560211, "accuracy": 1, "tokens_in": 152, "tokens_out": 36, "tokens_total": 188, "latency_total": 0.013993151000249782, "latency_per_module": {"scorer": 0.00716064100015501, "prior": 0.006832510000094771}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 18, "tokens_total": 116}, "prior": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I used to procrastinate about studying. Now, I never procrastinate studying.\nQuestion: Did After getting a good grade, I learned an easy lesson. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I used to procrastinate about studying. Now, I never procrastinate studying.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used to procrastinate about studying. Now, I never procrastinate studying.", "hypotheses": ["I failed a big test.", "After getting a good grade, I learned an easy lesson."]}
{"example_id": "31", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jacob decided to buy himself a car. happen?", "a": "no", "prior_probs": [0.499290943621067, 0.500709056378933], "posterior_probs": [0.539850527857254, 0.46014947214274604], "prior_entropy": 0.6931461750357113, "posterior_entropy": 0.6899676802449393, "delta_entropy": 0.003178494790772035, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.539850527857254, "accuracy": 0, "tokens_in": 156, "tokens_out": 30, "tokens_total": 186, "latency_total": 0.014291330000560265, "latency_per_module": {"scorer": 0.0073482490006426815, "prior": 0.0069430809999175835}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 15, "tokens_total": 111}, "prior": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.\nQuestion: Did Jacob decided to buy himself a car. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.", "hypotheses": ["Jacob decided to buy himself a car.", "Jacob couldn't afford a car."]}
{"example_id": "32", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Roy made the other team uncomfortable. happen?", "a": "yes", "prior_probs": [0.5412382362692145, 0.4587617637307855], "posterior_probs": [0.42328655456496544, 0.5767134454350346], "prior_entropy": 0.6897421297482087, "posterior_entropy": 0.6813306578829044, "delta_entropy": 0.008411471865304332, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5767134454350346, "accuracy": 1, "tokens_in": 154, "tokens_out": 34, "tokens_total": 188, "latency_total": 0.014167092999741726, "latency_per_module": {"scorer": 0.007203133000075468, "prior": 0.006963959999666258}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 17, "tokens_total": 111}, "prior": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.\nQuestion: Did Roy made the other team uncomfortable. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.", "hypotheses": ["He yelled at the players for every home run.", "Roy made the other team uncomfortable."]}
{"example_id": "33", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The school nurse sent Stan home from school. happen?", "a": "yes", "prior_probs": [0.5939922725940271, 0.40600772740597296], "posterior_probs": [0.46918951039200385, 0.5308104896079961], "prior_entropy": 0.6753725208367414, "posterior_entropy": 0.6912474046636119, "delta_entropy": -0.015874883826870523, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5308104896079961, "accuracy": 1, "tokens_in": 170, "tokens_out": 46, "tokens_total": 216, "latency_total": 0.01393675599956623, "latency_per_module": {"scorer": 0.006880848000037076, "prior": 0.007055907999529154}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}, "prior": {"tokens_in": 66, "tokens_out": 23, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.\nQuestion: Did The school nurse sent Stan home from school. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.", "hypotheses": ["Stan was out of school for a week with the stomach ache.", "The school nurse sent Stan home from school."]}
{"example_id": "34", "dataset": "art", "method": "random_question", "asked": true, "q": "Did They decided to try the advice given in a book about guitar playing. happen?", "a": "yes", "prior_probs": [0.6792737606353848, 0.32072623936461514], "posterior_probs": [0.3720221583060543, 0.6279778416939458], "prior_entropy": 0.6274156648989593, "posterior_entropy": 0.6600231427129066, "delta_entropy": -0.03260747781394735, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6279778416939458, "accuracy": 1, "tokens_in": 140, "tokens_out": 52, "tokens_total": 192, "latency_total": 0.01442735500040726, "latency_per_module": {"scorer": 0.007293743000445829, "prior": 0.007133611999961431}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 26, "tokens_total": 120}, "prior": {"tokens_in": 46, "tokens_out": 26, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lisa and Tim had been married for a long time. It worked.\nQuestion: Did They decided to try the advice given in a book about guitar playing. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Lisa and Tim had been married for a long time. It worked.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa and Tim had been married for a long time. It worked.", "hypotheses": ["Lisa and Tim went to a fertility clinic to get pregnant.", "They decided to try the advice given in a book about guitar playing."]}
{"example_id": "35", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?", "a": "no", "prior_probs": [0.6086536919959997, 0.3913463080040002], "posterior_probs": [0.7024292513921555, 0.2975707486078445], "prior_entropy": 0.6693464989756049, "posterior_entropy": 0.6087919301335103, "delta_entropy": 0.06055456884209465, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7024292513921555, "accuracy": 0, "tokens_in": 182, "tokens_out": 54, "tokens_total": 236, "latency_total": 0.014750816999367089, "latency_per_module": {"scorer": 0.007683597999857739, "prior": 0.00706721899950935}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 27, "tokens_total": 145}, "prior": {"tokens_in": 64, "tokens_out": 27, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.\nQuestion: Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.", "hypotheses": ["Adam made himself a sandwich using bread, turkey, and a slice of American cheese.", "Adam made himself a pb&j sandwich."]}
{"example_id": "36", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tom heard a game was on and left. happen?", "a": "yes", "prior_probs": [0.5647430126639348, 0.4352569873360652], "posterior_probs": [0.35342275001483686, 0.6465772499851631], "prior_entropy": 0.6847402799808633, "posterior_entropy": 0.6495397461918826, "delta_entropy": 0.03520053378898069, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6465772499851631, "accuracy": 1, "tokens_in": 126, "tokens_out": 36, "tokens_total": 162, "latency_total": 0.013487350000104925, "latency_per_module": {"scorer": 0.007374462999905518, "prior": 0.006112887000199407}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 18, "tokens_total": 100}, "prior": {"tokens_in": 44, "tokens_out": 18, "tokens_total": 62}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was painting his fence. Tom left his fence half painted.\nQuestion: Did Tom heard a game was on and left. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tom was painting his fence. Tom left his fence half painted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was painting his fence. Tom left his fence half painted.", "hypotheses": ["Tom got tired of painting after he finished.", "Tom heard a game was on and left."]}
{"example_id": "37", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She would be with her friends out there. happen?", "a": "no", "prior_probs": [0.6175754766923246, 0.3824245233076754], "posterior_probs": [0.6861092164085281, 0.31389078359147204], "prior_entropy": 0.6652385826527232, "posterior_entropy": 0.62217845014932, "delta_entropy": 0.043060132503403126, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6861092164085281, "accuracy": 0, "tokens_in": 150, "tokens_out": 36, "tokens_total": 186, "latency_total": 0.016263829999843438, "latency_per_module": {"scorer": 0.00898541100013972, "prior": 0.007278418999703717}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.\nQuestion: Did She would be with her friends out there. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.", "hypotheses": ["She would be with her friends out there.", "Amy wanted to live by the beadch."]}
{"example_id": "38", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Roger tried but he wasn't as good as his idol. happen?", "a": "yes", "prior_probs": [0.40261986142381756, 0.5973801385761824], "posterior_probs": [0.31948490138913493, 0.6805150986108651], "prior_entropy": 0.6740596404089629, "posterior_entropy": 0.6264805809228802, "delta_entropy": 0.047579059486082764, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6805150986108651, "accuracy": 1, "tokens_in": 184, "tokens_out": 48, "tokens_total": 232, "latency_total": 0.016433682000752015, "latency_per_module": {"scorer": 0.008183429000382603, "prior": 0.008250253000369412}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 24, "tokens_total": 138}, "prior": {"tokens_in": 70, "tokens_out": 24, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.\nQuestion: Did Roger tried but he wasn't as good as his idol. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.", "hypotheses": ["Roger overslept and lounged most the day.", "Roger tried but he wasn't as good as his idol."]}
{"example_id": "39", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Barry did not tell anyone that Julie farted. happen?", "a": "no", "prior_probs": [0.5156470688694731, 0.484352931130527], "posterior_probs": [0.6214058532731264, 0.37859414672687375], "prior_entropy": 0.6926574390754046, "posterior_entropy": 0.6633716971305815, "delta_entropy": 0.02928574194482303, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6214058532731264, "accuracy": 0, "tokens_in": 156, "tokens_out": 44, "tokens_total": 200, "latency_total": 0.014591349999136582, "latency_per_module": {"scorer": 0.007307413999114942, "prior": 0.00728393600002164}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}, "prior": {"tokens_in": 58, "tokens_out": 22, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.\nQuestion: Did Barry did not tell anyone that Julie farted. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.", "hypotheses": ["Barry did not tell anyone that Julie farted.", "Barry laughed at Julie's unzipped pants."]}
{"example_id": "40", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Bob stopped in the middle of his hike to tie his shoes. happen?", "a": "yes", "prior_probs": [0.44667890952075856, 0.5533210904792414], "posterior_probs": [0.3929538976400134, 0.6070461023599866], "prior_entropy": 0.6874500759315252, "posterior_entropy": 0.670051079307378, "delta_entropy": 0.017398996624147234, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6070461023599866, "accuracy": 1, "tokens_in": 154, "tokens_out": 56, "tokens_total": 210, "latency_total": 0.013674230999640713, "latency_per_module": {"scorer": 0.006744612000147754, "prior": 0.006929618999492959}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 28, "tokens_total": 128}, "prior": {"tokens_in": 54, "tokens_out": 28, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.\nQuestion: Did Bob stopped in the middle of his hike to tie his shoes. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.", "hypotheses": ["Bob stopped in the middle of the hike because he had no bug spray.", "Bob stopped in the middle of his hike to tie his shoes."]}
{"example_id": "41", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Lucy started ordering the pizza. happen?", "a": "yes", "prior_probs": [0.5715470914982475, 0.42845290850175255], "posterior_probs": [0.5054331071466536, 0.49456689285334643], "prior_entropy": 0.6828739799192332, "posterior_entropy": 0.6930881420895553, "delta_entropy": -0.010214162170322116, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5054331071466536, "accuracy": 0, "tokens_in": 172, "tokens_out": 36, "tokens_total": 208, "latency_total": 0.014126547000159917, "latency_per_module": {"scorer": 0.007084985999426863, "prior": 0.007041561000733054}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 18, "tokens_total": 120}, "prior": {"tokens_in": 70, "tokens_out": 18, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.\nQuestion: Did Lucy started ordering the pizza. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.", "hypotheses": ["Lucy decided to make the pizzas at home.", "Lucy started ordering the pizza."]}
{"example_id": "42", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jim found his new hat in a storm. happen?", "a": "no", "prior_probs": [0.49518695439491683, 0.5048130456050831], "posterior_probs": [0.5759092465883465, 0.42409075341165353], "prior_entropy": 0.6931008490264108, "posterior_entropy": 0.6815780690437176, "delta_entropy": 0.011522779982693132, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5759092465883465, "accuracy": 0, "tokens_in": 154, "tokens_out": 36, "tokens_total": 190, "latency_total": 0.014449184000113746, "latency_per_module": {"scorer": 0.007304949999706878, "prior": 0.007144234000406868}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}, "prior": {"tokens_in": 58, "tokens_out": 18, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was a very windy day. Jim wished he hadn't gone out in his new hat.\nQuestion: Did Jim found his new hat in a storm. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: It was a very windy day. Jim wished he hadn't gone out in his new hat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was a very windy day. Jim wished he hadn't gone out in his new hat.", "hypotheses": ["Jim found his new hat in a storm.", "Jim's hat blew away in the wind."]}
{"example_id": "43", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The water was perfect for all levels of fishing. happen?", "a": "no", "prior_probs": [0.5889699375363152, 0.4110300624636848], "posterior_probs": [0.664761432879087, 0.335238567120913], "prior_entropy": 0.6772312612503355, "posterior_entropy": 0.6378266161575247, "delta_entropy": 0.03940464509281083, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.664761432879087, "accuracy": 0, "tokens_in": 168, "tokens_out": 34, "tokens_total": 202, "latency_total": 0.014157879999402212, "latency_per_module": {"scorer": 0.007179786000051536, "prior": 0.006978093999350676}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}, "prior": {"tokens_in": 64, "tokens_out": 17, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.\nQuestion: Did The water was perfect for all levels of fishing. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.", "hypotheses": ["The water was perfect for all levels of fishing.", "The water was spitting up poles."]}
{"example_id": "44", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?", "a": "no", "prior_probs": [0.5439580116016066, 0.4560419883983934], "posterior_probs": [0.6560598541048825, 0.34394014589511757], "prior_entropy": 0.6892775731215821, "posterior_entropy": 0.6436144263886251, "delta_entropy": 0.045663146732957016, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6560598541048825, "accuracy": 0, "tokens_in": 198, "tokens_out": 60, "tokens_total": 258, "latency_total": 0.01576289700005873, "latency_per_module": {"scorer": 0.008227782000176376, "prior": 0.007535114999882353}, "tokens_per_module": {"scorer": {"tokens_in": 128, "tokens_out": 30, "tokens_total": 158}, "prior": {"tokens_in": 70, "tokens_out": 30, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.\nQuestion: Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.", "hypotheses": ["I went to visit her and stepped out onto the balcony of her apartment with a great view.", "I looked down from her balcony to see the clouds."]}
{"example_id": "45", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Allister was still a novice at the bow. happen?", "a": "no", "prior_probs": [0.5420201553935746, 0.45797984460642543], "posterior_probs": [0.5608728440586146, 0.43912715594138535], "prior_entropy": 0.689611624953167, "posterior_entropy": 0.6857177571864664, "delta_entropy": 0.0038938677667005317, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5608728440586146, "accuracy": 0, "tokens_in": 168, "tokens_out": 38, "tokens_total": 206, "latency_total": 0.015291785999579588, "latency_per_module": {"scorer": 0.008111658999951032, "prior": 0.0071801269996285555}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}, "prior": {"tokens_in": 64, "tokens_out": 19, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.\nQuestion: Did Allister was still a novice at the bow. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.", "hypotheses": ["Allister was still a novice at the bow.", "Allister was a pro at the bow."]}
{"example_id": "46", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Billy got home from work early. happen?", "a": "yes", "prior_probs": [0.36029998877311387, 0.6397000112268861], "posterior_probs": [0.35586779600101964, 0.6441322039989804], "prior_entropy": 0.6535906023014595, "posterior_entropy": 0.6510035547666551, "delta_entropy": 0.002587047534804432, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6441322039989804, "accuracy": 1, "tokens_in": 158, "tokens_out": 36, "tokens_total": 194, "latency_total": 0.01528345899987471, "latency_per_module": {"scorer": 0.007552070000201638, "prior": 0.007731388999673072}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.\nQuestion: Did Billy got home from work early. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.", "hypotheses": ["Billy played games and forgot about cleaning until 5PM.", "Billy got home from work early."]}
{"example_id": "47", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She ended up falling into the river. happen?", "a": "no", "prior_probs": [0.5429731534759819, 0.4570268465240181], "posterior_probs": [0.5858130486295954, 0.4141869513704046], "prior_entropy": 0.6894492362019962, "posterior_entropy": 0.6783462540618717, "delta_entropy": 0.011102982140124507, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5858130486295954, "accuracy": 0, "tokens_in": 172, "tokens_out": 38, "tokens_total": 210, "latency_total": 0.014129648999187339, "latency_per_module": {"scorer": 0.006939828999747988, "prior": 0.007189819999439351}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}, "prior": {"tokens_in": 68, "tokens_out": 19, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.\nQuestion: Did She ended up falling into the river. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.", "hypotheses": ["She ended up falling into the river.", "Maya slipped on some rocks and broke her back."]}
{"example_id": "48", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She had to put in some broth. happen?", "a": "no", "prior_probs": [0.4976706673336924, 0.5023293326663075], "posterior_probs": [0.5352169298952173, 0.4647830701047827], "prior_entropy": 0.6931363289373521, "posterior_entropy": 0.6906646612738052, "delta_entropy": 0.002471667663546895, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5352169298952173, "accuracy": 0, "tokens_in": 140, "tokens_out": 32, "tokens_total": 172, "latency_total": 0.015619218999745499, "latency_per_module": {"scorer": 0.007850428999518044, "prior": 0.007768790000227455}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}, "prior": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Susan was making a soup. She did her best to cut away the bad parts.\nQuestion: Did She had to put in some broth. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Susan was making a soup. She did her best to cut away the bad parts.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Susan was making a soup. She did her best to cut away the bad parts.", "hypotheses": ["She had to put in some broth.", "She had to put in some chicken."]}
{"example_id": "49", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Lulu's daughter was going to go to school for the first time. happen?", "a": "no", "prior_probs": [0.5997206215358223, 0.40027937846417777], "posterior_probs": [0.629891948814377, 0.3701080511856229], "prior_entropy": 0.6731247826300334, "posterior_entropy": 0.6590131622967765, "delta_entropy": 0.014111620333256858, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.629891948814377, "accuracy": 0, "tokens_in": 170, "tokens_out": 64, "tokens_total": 234, "latency_total": 0.016995024000607373, "latency_per_module": {"scorer": 0.008869074000358523, "prior": 0.00812595000024885}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 32, "tokens_total": 142}, "prior": {"tokens_in": 60, "tokens_out": 32, "tokens_total": 92}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.\nQuestion: Did Lulu's daughter was going to go to school for the first time. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.", "hypotheses": ["Lulu's daughter was going to go to school for the first time.", "Lulu's mom was thinking of sending her to a new house despite her objections."]}
{"example_id": "50", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I programmed with Java for robot game because it was easy. happen?", "a": "yes", "prior_probs": [0.4704830929730231, 0.529516907026977], "posterior_probs": [0.4368249688091804, 0.5631750311908196], "prior_entropy": 0.6914036714465945, "posterior_entropy": 0.6851436362907115, "delta_entropy": 0.006260035155883026, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5631750311908196, "accuracy": 1, "tokens_in": 152, "tokens_out": 40, "tokens_total": 192, "latency_total": 0.019350486000803357, "latency_per_module": {"scorer": 0.011312223999993876, "prior": 0.008038262000809482}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I wanted to create a video game. Indeed, Java was terrible for programming video games.\nQuestion: Did I programmed with Java for robot game because it was easy. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I wanted to create a video game. Indeed, Java was terrible for programming video games.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I wanted to create a video game. Indeed, Java was terrible for programming video games.", "hypotheses": ["I programmed with Java for robot game.", "I programmed with Java for robot game because it was easy."]}
{"example_id": "51", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I went with her to celebrate. happen?", "a": "yes", "prior_probs": [0.5536741528365954, 0.4463258471634046], "posterior_probs": [0.4518363652156137, 0.5481636347843863], "prior_entropy": 0.6873742336400734, "posterior_entropy": 0.6885005074649375, "delta_entropy": -0.001126273824864077, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5481636347843863, "accuracy": 1, "tokens_in": 174, "tokens_out": 32, "tokens_total": 206, "latency_total": 0.024109906000376213, "latency_per_module": {"scorer": 0.011947404000238748, "prior": 0.012162502000137465}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 16, "tokens_total": 120}, "prior": {"tokens_in": 70, "tokens_out": 16, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.\nQuestion: Did I went with her to celebrate. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.", "hypotheses": ["My best friend visited me on a vacation.", "I went with her to celebrate."]}
{"example_id": "52", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tom bought costly ones but they broke right away. happen?", "a": "yes", "prior_probs": [0.6080432574413293, 0.39195674255867086], "posterior_probs": [0.4014084410123193, 0.5985915589876808], "prior_entropy": 0.6696153193660608, "posterior_entropy": 0.6735786095837957, "delta_entropy": -0.003963290217734894, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5985915589876808, "accuracy": 1, "tokens_in": 160, "tokens_out": 34, "tokens_total": 194, "latency_total": 0.023661139000068943, "latency_per_module": {"scorer": 0.011516738999489462, "prior": 0.01214440000057948}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 17, "tokens_total": 117}, "prior": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.\nQuestion: Did Tom bought costly ones but they broke right away. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.", "hypotheses": ["Tom was an elitist.", "Tom bought costly ones but they broke right away."]}
{"example_id": "53", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kaya could not find a single thing at the store,. happen?", "a": "yes", "prior_probs": [0.531998472862934, 0.46800152713706594], "posterior_probs": [0.4192030045646056, 0.5807969954353945], "prior_entropy": 0.691097975897329, "posterior_entropy": 0.6800334472615046, "delta_entropy": 0.011064528635824411, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5807969954353945, "accuracy": 1, "tokens_in": 188, "tokens_out": 50, "tokens_total": 238, "latency_total": 0.02629908499875455, "latency_per_module": {"scorer": 0.01347694799915189, "prior": 0.012822136999602662}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 25, "tokens_total": 141}, "prior": {"tokens_in": 72, "tokens_out": 25, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.\nQuestion: Did Kaya could not find a single thing at the store,. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.", "hypotheses": ["At the store, Kaya saw a very beautiful vase.", "Kaya could not find a single thing at the store,."]}
{"example_id": "54", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Sam put a towel under the leaky fridge. happen?", "a": "no", "prior_probs": [0.5717724395920868, 0.4282275604079132], "posterior_probs": [0.6390454428289989, 0.360954557171001], "prior_entropy": 0.6828089385588626, "posterior_entropy": 0.6539654361392004, "delta_entropy": 0.02884350241966216, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6390454428289989, "accuracy": 0, "tokens_in": 172, "tokens_out": 36, "tokens_total": 208, "latency_total": 0.023031591000290064, "latency_per_module": {"scorer": 0.01008684499993251, "prior": 0.012944746000357554}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 66, "tokens_out": 18, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.\nQuestion: Did Sam put a towel under the leaky fridge. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.", "hypotheses": ["Sam put a towel under the leaky fridge.", "Sam dishwasher broke and was leaking."]}
{"example_id": "55", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She went to a concert to get rid of it. happen?", "a": "yes", "prior_probs": [0.46141858598069874, 0.5385814140193013], "posterior_probs": [0.34972427267813305, 0.650275727321867], "prior_entropy": 0.6901671682063506, "posterior_entropy": 0.6472757859001699, "delta_entropy": 0.042891382306180725, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.650275727321867, "accuracy": 1, "tokens_in": 146, "tokens_out": 52, "tokens_total": 198, "latency_total": 0.021871449000173016, "latency_per_module": {"scorer": 0.010784694999529165, "prior": 0.01108675400064385}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 26, "tokens_total": 120}, "prior": {"tokens_in": 52, "tokens_out": 26, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Anna had a bad headache. Thankfully, when she awoke, the headache was gone.\nQuestion: Did She went to a concert to get rid of it. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Anna had a bad headache. Thankfully, when she awoke, the headache was gone.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Anna had a bad headache. Thankfully, when she awoke, the headache was gone.", "hypotheses": ["Anna took a few asprins and laid down and took a nap.", "She went to a concert to get rid of it."]}
{"example_id": "56", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I actually have a bike. happen?", "a": "yes", "prior_probs": [0.5567255632063237, 0.44327443679367623], "posterior_probs": [0.4961829330994172, 0.5038170669005828], "prior_entropy": 0.6866977243851222, "posterior_entropy": 0.6931180402754451, "delta_entropy": -0.006420315890322881, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5038170669005828, "accuracy": 1, "tokens_in": 132, "tokens_out": 40, "tokens_total": 172, "latency_total": 0.02005562700014707, "latency_per_module": {"scorer": 0.012008056999547989, "prior": 0.00804757000059908}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 20, "tokens_total": 102}, "prior": {"tokens_in": 50, "tokens_out": 20, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Last night I had a dream about biking. I woke up, bitterly disappointed.\nQuestion: Did I actually have a bike. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Last night I had a dream about biking. I woke up, bitterly disappointed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Last night I had a dream about biking. I woke up, bitterly disappointed.", "hypotheses": ["my dream was very real and I was on a fun bike tour.", "I actually have a bike."]}
{"example_id": "57", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She forgot what time it was and was home late. happen?", "a": "no", "prior_probs": [0.4345182363492293, 0.5654817636507706], "posterior_probs": [0.5734261958865692, 0.4265738041134308], "prior_entropy": 0.6845467737315923, "posterior_entropy": 0.6823252734878612, "delta_entropy": 0.0022215002437311338, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5734261958865692, "accuracy": 0, "tokens_in": 154, "tokens_out": 46, "tokens_total": 200, "latency_total": 0.020632793000004312, "latency_per_module": {"scorer": 0.01001687400002993, "prior": 0.010615918999974383}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 23, "tokens_total": 121}, "prior": {"tokens_in": 56, "tokens_out": 23, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was starting to get late outside. Her parents grounded her for a week for being late.\nQuestion: Did She forgot what time it was and was home late. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: It was starting to get late outside. Her parents grounded her for a week for being late.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was starting to get late outside. Her parents grounded her for a week for being late.", "hypotheses": ["She forgot what time it was and was home late.", "She was supposed to be home an hour after she arrived."]}
{"example_id": "58", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kim applied for jobs to make money. happen?", "a": "yes", "prior_probs": [0.574938188784644, 0.425061811215356], "posterior_probs": [0.47290484657773374, 0.5270951534222663], "prior_entropy": 0.6818732852598134, "posterior_entropy": 0.6916781664047806, "delta_entropy": -0.009804881144967248, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5270951534222663, "accuracy": 1, "tokens_in": 156, "tokens_out": 34, "tokens_total": 190, "latency_total": 0.02251396800056682, "latency_per_module": {"scorer": 0.011713969000084035, "prior": 0.010799999000482785}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}, "prior": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.\nQuestion: Did Kim applied for jobs to make money. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.", "hypotheses": ["Kim needed more money than she could get.", "Kim applied for jobs to make money."]}
{"example_id": "59", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Joe had plenty of time to cook something. happen?", "a": "no", "prior_probs": [0.49453006156178386, 0.5054699384382161], "posterior_probs": [0.5218400180201996, 0.47815998197980025], "prior_entropy": 0.6930873389112259, "posterior_entropy": 0.6921929041972648, "delta_entropy": 0.000894434713961112, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5218400180201996, "accuracy": 0, "tokens_in": 150, "tokens_out": 36, "tokens_total": 186, "latency_total": 0.020424668000487145, "latency_per_module": {"scorer": 0.0103224880003836, "prior": 0.010102180000103544}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!\nQuestion: Did Joe had plenty of time to cook something. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!", "hypotheses": ["Joe had plenty of time to cook something.", "Joe didn't have time to cook something."]}
{"example_id": "60", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He went to the near by super market. happen?", "a": "no", "prior_probs": [0.4444638647440158, 0.5555361352559842], "posterior_probs": [0.6486373170766571, 0.35136268292334283], "prior_entropy": 0.6869659093462148, "posterior_entropy": 0.6482861145182428, "delta_entropy": 0.038679794827972014, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6486373170766571, "accuracy": 0, "tokens_in": 142, "tokens_out": 40, "tokens_total": 182, "latency_total": 0.01986586500061094, "latency_per_module": {"scorer": 0.00923639299981005, "prior": 0.010629472000800888}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.\nQuestion: Did He went to the near by super market. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.", "hypotheses": ["He went to the near by super market.", "Tim looked for a long time in the messy fridge."]}
{"example_id": "61", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The rap tape was mean and rude. happen?", "a": "no", "prior_probs": [0.6524055812220552, 0.3475944187779449], "posterior_probs": [0.7030050701302726, 0.2969949298697275], "prior_entropy": 0.6459447581254353, "posterior_entropy": 0.6082965702015342, "delta_entropy": 0.03764818792390112, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7030050701302726, "accuracy": 0, "tokens_in": 156, "tokens_out": 32, "tokens_total": 188, "latency_total": 0.017537699000968132, "latency_per_module": {"scorer": 0.00822471100036637, "prior": 0.009312988000601763}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}, "prior": {"tokens_in": 60, "tokens_out": 16, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.\nQuestion: Did The rap tape was mean and rude. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.", "hypotheses": ["The rap tape was mean and rude.", "Gina  take their new tape."]}
{"example_id": "62", "dataset": "art", "method": "random_question", "asked": true, "q": "Did We ordered a dessert everyone would like. happen?", "a": "no", "prior_probs": [0.5428062460011205, 0.45719375399887957], "posterior_probs": [0.5853486189041158, 0.4146513810958843], "prior_entropy": 0.6894779411916492, "posterior_entropy": 0.6785068196410964, "delta_entropy": 0.010971121550552798, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5853486189041158, "accuracy": 0, "tokens_in": 144, "tokens_out": 32, "tokens_total": 176, "latency_total": 0.018477698999959102, "latency_per_module": {"scorer": 0.009477876999881119, "prior": 0.008999822000077984}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 16, "tokens_total": 106}, "prior": {"tokens_in": 54, "tokens_out": 16, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.\nQuestion: Did We ordered a dessert everyone would like. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.", "hypotheses": ["We ordered a dessert everyone would like.", "We ordered appetizers everyone would like."]}
{"example_id": "63", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ora decided she wanted to maintainer her weight. happen?", "a": "yes", "prior_probs": [0.5766461630064038, 0.4233538369935963], "posterior_probs": [0.40121883532206487, 0.5987811646779352], "prior_entropy": 0.6813514588057896, "posterior_entropy": 0.6735027683312809, "delta_entropy": 0.007848690474508757, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5987811646779352, "accuracy": 1, "tokens_in": 150, "tokens_out": 42, "tokens_total": 192, "latency_total": 0.01609378499961167, "latency_per_module": {"scorer": 0.007455041999492096, "prior": 0.008638743000119575}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 21, "tokens_total": 117}, "prior": {"tokens_in": 54, "tokens_out": 21, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ora had always been overweight. With their help, Ora lost over twenty pounds!\nQuestion: Did Ora decided she wanted to maintainer her weight. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Ora had always been overweight. With their help, Ora lost over twenty pounds!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora had always been overweight. With their help, Ora lost over twenty pounds!", "hypotheses": ["Ora decided to eat healthy for a month.", "Ora decided she wanted to maintainer her weight."]}
{"example_id": "64", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The food that Priya ordered was microwaved and precooked. happen?", "a": "yes", "prior_probs": [0.4374002057690711, 0.562599794230929], "posterior_probs": [0.30954852399197325, 0.6904514760080268], "prior_entropy": 0.6852891073100924, "posterior_entropy": 0.618738953166184, "delta_entropy": 0.06655015414390841, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6904514760080268, "accuracy": 1, "tokens_in": 150, "tokens_out": 38, "tokens_total": 188, "latency_total": 0.013796651999655296, "latency_per_module": {"scorer": 0.006951077999474364, "prior": 0.006845574000180932}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 19, "tokens_total": 117}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Priya decided to try a new restaurant. Priya thought her food was delicious.\nQuestion: Did The food that Priya ordered was microwaved and precooked. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Priya decided to try a new restaurant. Priya thought her food was delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Priya decided to try a new restaurant. Priya thought her food was delicious.", "hypotheses": ["She ordered two shrimp dishes.", "The food that Priya ordered was microwaved and precooked."]}
{"example_id": "65", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jamie and Candice couldn't decide what to do. happen?", "a": "yes", "prior_probs": [0.43257818279028354, 0.5674218172097164], "posterior_probs": [0.40822417282041834, 0.5917758271795815], "prior_entropy": 0.6840280241266522, "posterior_entropy": 0.6762056859186427, "delta_entropy": 0.007822338208009483, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5917758271795815, "accuracy": 1, "tokens_in": 150, "tokens_out": 46, "tokens_total": 196, "latency_total": 0.013910697000028449, "latency_per_module": {"scorer": 0.007140544000321825, "prior": 0.0067701529997066245}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 23, "tokens_total": 119}, "prior": {"tokens_in": 54, "tokens_out": 23, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jamie and Candice were going on a date. Finally, they settled on ice cream!\nQuestion: Did Jamie and Candice couldn't decide what to do. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Jamie and Candice were going on a date. Finally, they settled on ice cream!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jamie and Candice were going on a date. Finally, they settled on ice cream!", "hypotheses": ["Jamie and Candice did not know what movie to see.", "Jamie and Candice couldn't decide what to do."]}
{"example_id": "66", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Scott found a job in New York. happen?", "a": "no", "prior_probs": [0.6046598668084326, 0.39534013319156736], "posterior_probs": [0.6375940117979644, 0.3624059882020357], "prior_entropy": 0.6710769560047052, "posterior_entropy": 0.6547899638549723, "delta_entropy": 0.016286992149732837, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6375940117979644, "accuracy": 0, "tokens_in": 184, "tokens_out": 54, "tokens_total": 238, "latency_total": 0.01462486699983856, "latency_per_module": {"scorer": 0.0073553069996705744, "prior": 0.007269560000167985}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 27, "tokens_total": 137}, "prior": {"tokens_in": 74, "tokens_out": 27, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.\nQuestion: Did Scott found a job in New York. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.", "hypotheses": ["Scott found a job in New York.", "The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living."]}
{"example_id": "67", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I bought jeans thought they were a bit expensive. happen?", "a": "yes", "prior_probs": [0.42338107670920266, 0.5766189232907972], "posterior_probs": [0.33595767133238874, 0.6640423286676113], "prior_entropy": 0.6813598749171441, "posterior_entropy": 0.6383177449042357, "delta_entropy": 0.043042130012908464, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6640423286676113, "accuracy": 1, "tokens_in": 160, "tokens_out": 30, "tokens_total": 190, "latency_total": 0.01395517800028756, "latency_per_module": {"scorer": 0.0068491490001179045, "prior": 0.007106029000169656}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 15, "tokens_total": 115}, "prior": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I went to the store one day to buy clothes. I went home and the jeans fit much better.\nQuestion: Did I bought jeans thought they were a bit expensive. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I went to the store one day to buy clothes. I went home and the jeans fit much better.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I went to the store one day to buy clothes. I went home and the jeans fit much better.", "hypotheses": ["I got larger sizes.", "I bought jeans thought they were a bit expensive."]}
{"example_id": "68", "dataset": "art", "method": "random_question", "asked": true, "q": "Did They ran into a cute friend of Sam's on the way to dinner. happen?", "a": "no", "prior_probs": [0.5093751289457198, 0.49062487105428026], "posterior_probs": [0.6646352315268922, 0.3353647684731078], "prior_entropy": 0.6929713841707495, "posterior_entropy": 0.6379129760809052, "delta_entropy": 0.0550584080898443, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6646352315268922, "accuracy": 0, "tokens_in": 186, "tokens_out": 54, "tokens_total": 240, "latency_total": 0.015043420000438346, "latency_per_module": {"scorer": 0.007767537999825436, "prior": 0.00727588200061291}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 27, "tokens_total": 145}, "prior": {"tokens_in": 68, "tokens_out": 27, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.\nQuestion: Did They ran into a cute friend of Sam's on the way to dinner. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.", "hypotheses": ["They ran into a cute friend of Sam's on the way to dinner.", "The date went bad, they went home on good terms."]}
{"example_id": "69", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Pam tried her best to be as honest as possible. happen?", "a": "no", "prior_probs": [0.5997088313230708, 0.4002911686769291], "posterior_probs": [0.6315549156136061, 0.368445084386394], "prior_entropy": 0.6731295491372684, "posterior_entropy": 0.6581229393318856, "delta_entropy": 0.01500660980538282, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6315549156136061, "accuracy": 0, "tokens_in": 182, "tokens_out": 42, "tokens_total": 224, "latency_total": 0.014360029000272334, "latency_per_module": {"scorer": 0.007334977999562398, "prior": 0.0070250510007099365}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 21, "tokens_total": 133}, "prior": {"tokens_in": 70, "tokens_out": 21, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.\nQuestion: Did Pam tried her best to be as honest as possible. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.", "hypotheses": ["Pam tried her best to be as honest as possible.", "Pam wasted time with doing her surveys."]}
{"example_id": "70", "dataset": "art", "method": "random_question", "asked": true, "q": "Did In Lydia's dream, she was poor and lonely. happen?", "a": "no", "prior_probs": [0.4946844792302691, 0.505315520769731], "posterior_probs": [0.5413426026341418, 0.4586573973658582], "prior_entropy": 0.6930906699713483, "posterior_entropy": 0.6897248530792355, "delta_entropy": 0.003365816892112794, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5413426026341418, "accuracy": 0, "tokens_in": 138, "tokens_out": 44, "tokens_total": 182, "latency_total": 0.014322119001008105, "latency_per_module": {"scorer": 0.007327810000788304, "prior": 0.0069943090002198005}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 22, "tokens_total": 112}, "prior": {"tokens_in": 48, "tokens_out": 22, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lydia had a strange dream last night. Lydia wished the dream were real.\nQuestion: Did In Lydia's dream, she was poor and lonely. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lydia had a strange dream last night. Lydia wished the dream were real.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lydia had a strange dream last night. Lydia wished the dream were real.", "hypotheses": ["In Lydia's dream, she was poor and lonely.", "In Lydia's dream, she was rich and famous."]}
{"example_id": "71", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Her friend asked Dana to teach her how to ride. happen?", "a": "yes", "prior_probs": [0.486309255929223, 0.5136907440707771], "posterior_probs": [0.3776516131269616, 0.6223483868730385], "prior_entropy": 0.6927722607542264, "posterior_entropy": 0.6629027645544058, "delta_entropy": 0.0298694961998206, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6223483868730385, "accuracy": 1, "tokens_in": 142, "tokens_out": 42, "tokens_total": 184, "latency_total": 0.014645166999798676, "latency_per_module": {"scorer": 0.007368382000095153, "prior": 0.007276784999703523}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}, "prior": {"tokens_in": 50, "tokens_out": 21, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dana always wanted to ride a bike. They were riding around together within minutes.\nQuestion: Did Her friend asked Dana to teach her how to ride. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Dana always wanted to ride a bike. They were riding around together within minutes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dana always wanted to ride a bike. They were riding around together within minutes.", "hypotheses": ["Dana asked a neighbor to ride with her.", "Her friend asked Dana to teach her how to ride."]}
{"example_id": "72", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kelly made it onto the team. happen?", "a": "yes", "prior_probs": [0.5442938108983753, 0.4557061891016247], "posterior_probs": [0.47094451336404575, 0.5290554866359544], "prior_entropy": 0.6892181487243876, "posterior_entropy": 0.6914577863852409, "delta_entropy": -0.002239637660853333, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5290554866359544, "accuracy": 1, "tokens_in": 134, "tokens_out": 36, "tokens_total": 170, "latency_total": 0.01380775399866252, "latency_per_module": {"scorer": 0.007062854999276169, "prior": 0.006744898999386351}, "tokens_per_module": {"scorer": {"tokens_in": 84, "tokens_out": 18, "tokens_total": 102}, "prior": {"tokens_in": 50, "tokens_out": 18, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.\nQuestion: Did Kelly made it onto the team. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.", "hypotheses": ["Kelly tried out for the soccer team but was cut.", "Kelly made it onto the team."]}
{"example_id": "73", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tom didn't check the grill's safety. happen?", "a": "yes", "prior_probs": [0.4592465255863792, 0.5407534744136208], "posterior_probs": [0.4000717522970673, 0.5999282477029327], "prior_entropy": 0.6898218015182185, "posterior_entropy": 0.6730407493345406, "delta_entropy": 0.01678105218367787, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5999282477029327, "accuracy": 1, "tokens_in": 150, "tokens_out": 36, "tokens_total": 186, "latency_total": 0.014309387999674072, "latency_per_module": {"scorer": 0.007574412999929336, "prior": 0.006734974999744736}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.\nQuestion: Did Tom didn't check the grill's safety. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.", "hypotheses": ["Tom didn't check the grills gas.", "Tom didn't check the grill's safety."]}
{"example_id": "74", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Martins friends went fishing without martin. happen?", "a": "no", "prior_probs": [0.33492383062893677, 0.6650761693710633], "posterior_probs": [0.5865516441304067, 0.4134483558695933], "prior_entropy": 0.6376109297354657, "posterior_entropy": 0.6780890710074758, "delta_entropy": -0.040478141272010104, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5865516441304067, "accuracy": 0, "tokens_in": 146, "tokens_out": 36, "tokens_total": 182, "latency_total": 0.014682412999718508, "latency_per_module": {"scorer": 0.007596753999678185, "prior": 0.007085659000040323}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}, "prior": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Martin went to camp with his friends. Martin caught many fish so that everyone could eat.\nQuestion: Did Martins friends went fishing without martin. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Martin went to camp with his friends. Martin caught many fish so that everyone could eat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martin went to camp with his friends. Martin caught many fish so that everyone could eat.", "hypotheses": ["Martins friends went fishing without martin.", "He was the best fisherman of the group."]}
{"example_id": "75", "dataset": "art", "method": "random_question", "asked": true, "q": "Did After the car wash Sam noticed his car seats were all soaking wet. happen?", "a": "no", "prior_probs": [0.44405487857142134, 0.5559451214285787], "posterior_probs": [0.6112216507022545, 0.3887783492977455], "prior_entropy": 0.6868743401477769, "posterior_entropy": 0.6681984897033622, "delta_entropy": 0.018675850444414777, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6112216507022545, "accuracy": 0, "tokens_in": 168, "tokens_out": 72, "tokens_total": 240, "latency_total": 0.015327267999964533, "latency_per_module": {"scorer": 0.008003187999747752, "prior": 0.007324080000216782}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 36, "tokens_total": 144}, "prior": {"tokens_in": 60, "tokens_out": 36, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.\nQuestion: Did After the car wash Sam noticed his car seats were all soaking wet. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.", "hypotheses": ["After the car wash Sam noticed his car seats were all soaking wet.", "He rolled up all his windows and began daydreaming of how well his car would look after the wash."]}
{"example_id": "76", "dataset": "art", "method": "random_question", "asked": true, "q": "Did So she bought 2 tickets online. happen?", "a": "yes", "prior_probs": [0.4737475754119093, 0.5262524245880907], "posterior_probs": [0.4016095354736598, 0.5983904645263401], "prior_entropy": 0.6917681669549507, "posterior_entropy": 0.6736588827911949, "delta_entropy": 0.01810928416375579, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5983904645263401, "accuracy": 1, "tokens_in": 186, "tokens_out": 30, "tokens_total": 216, "latency_total": 0.015814885000509094, "latency_per_module": {"scorer": 0.007995833000677521, "prior": 0.007819051999831572}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 15, "tokens_total": 125}, "prior": {"tokens_in": 76, "tokens_out": 15, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!\nQuestion: Did So she bought 2 tickets online. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!", "hypotheses": ["May sent out tweets looking for tickets.", "So she bought 2 tickets online."]}
{"example_id": "77", "dataset": "art", "method": "random_question", "asked": true, "q": "Did When Mindy walked in the door, Jake was naked. happen?", "a": "no", "prior_probs": [0.486044932696252, 0.5139550673037481], "posterior_probs": [0.5926495053176007, 0.4073504946823992], "prior_entropy": 0.692757642168361, "posterior_entropy": 0.6758796990385001, "delta_entropy": 0.016877943129860906, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5926495053176007, "accuracy": 0, "tokens_in": 164, "tokens_out": 46, "tokens_total": 210, "latency_total": 0.016176082999663777, "latency_per_module": {"scorer": 0.007761406000099669, "prior": 0.008414676999564108}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}, "prior": {"tokens_in": 60, "tokens_out": 23, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mindy decided to go over jake's house. She panicked and ran screaming out of his house.\nQuestion: Did When Mindy walked in the door, Jake was naked. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mindy decided to go over jake's house. She panicked and ran screaming out of his house.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mindy decided to go over jake's house. She panicked and ran screaming out of his house.", "hypotheses": ["When Mindy walked in the door, Jake was naked.", "Mindy scared Jake when she came into the house."]}
{"example_id": "78", "dataset": "art", "method": "random_question", "asked": true, "q": "Did My sister stole my slippers. happen?", "a": "yes", "prior_probs": [0.4790031487503871, 0.5209968512496128], "posterior_probs": [0.4218260211495296, 0.5781739788504705], "prior_entropy": 0.6922651856976607, "posterior_entropy": 0.6808745500392104, "delta_entropy": 0.011390635658450332, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5781739788504705, "accuracy": 1, "tokens_in": 146, "tokens_out": 28, "tokens_total": 174, "latency_total": 0.015342112999860547, "latency_per_module": {"scorer": 0.007698902999436541, "prior": 0.0076432100004240056}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 14, "tokens_total": 104}, "prior": {"tokens_in": 56, "tokens_out": 14, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was having trouble finding my comfortable slippers. Finders keepers, she told me.\nQuestion: Did My sister stole my slippers. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I was having trouble finding my comfortable slippers. Finders keepers, she told me.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was having trouble finding my comfortable slippers. Finders keepers, she told me.", "hypotheses": ["The dog stole the slippers.", "My sister stole my slippers."]}
{"example_id": "79", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I wanted to have normal lips. I painted them red and people liked it. happen?", "a": "yes", "prior_probs": [0.44394140244086727, 0.5560585975591328], "posterior_probs": [0.3419649756170469, 0.6580350243829531], "prior_entropy": 0.6868488135486308, "posterior_entropy": 0.6423302396141802, "delta_entropy": 0.04451857393445058, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6580350243829531, "accuracy": 1, "tokens_in": 192, "tokens_out": 52, "tokens_total": 244, "latency_total": 0.01606853700013744, "latency_per_module": {"scorer": 0.008376984000278753, "prior": 0.007691552999858686}, "tokens_per_module": {"scorer": {"tokens_in": 122, "tokens_out": 26, "tokens_total": 148}, "prior": {"tokens_in": 70, "tokens_out": 26, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.\nQuestion: Did I wanted to have normal lips. I painted them red and people liked it. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.", "hypotheses": ["I used a marker to paint them bright pink.", "I wanted to have normal lips. I painted them red and people liked it."]}
{"example_id": "80", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The boy needed a new cell phone. happen?", "a": "yes", "prior_probs": [0.4492307602125571, 0.5507692397874429], "posterior_probs": [0.3883124042080154, 0.6116875957919847], "prior_entropy": 0.6879832542902835, "posterior_entropy": 0.6679872158394434, "delta_entropy": 0.01999603845084008, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6116875957919847, "accuracy": 1, "tokens_in": 132, "tokens_out": 30, "tokens_total": 162, "latency_total": 0.015012210999884701, "latency_per_module": {"scorer": 0.007902911999735807, "prior": 0.007109299000148894}, "tokens_per_module": {"scorer": {"tokens_in": 84, "tokens_out": 15, "tokens_total": 99}, "prior": {"tokens_in": 48, "tokens_out": 15, "tokens_total": 63}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A family went shopping together. The father bought the boy a new computer.\nQuestion: Did The boy needed a new cell phone. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: A family went shopping together. The father bought the boy a new computer.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A family went shopping together. The father bought the boy a new computer.", "hypotheses": ["Their son was very well behaved.", "The boy needed a new cell phone."]}
{"example_id": "81", "dataset": "art", "method": "random_question", "asked": true, "q": "Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?", "a": "no", "prior_probs": [0.47648300690444706, 0.5235169930955529], "posterior_probs": [0.6916962383653161, 0.3083037616346839], "prior_entropy": 0.6920406744505931, "posterior_entropy": 0.6177367389586781, "delta_entropy": 0.07430393549191505, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6916962383653161, "accuracy": 0, "tokens_in": 172, "tokens_out": 56, "tokens_total": 228, "latency_total": 0.01612368400037667, "latency_per_module": {"scorer": 0.008230405000176688, "prior": 0.007893279000199982}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 28, "tokens_total": 142}, "prior": {"tokens_in": 58, "tokens_out": 28, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was working on my laptop one day. After paying the bill, I no longer experienced issues.\nQuestion: Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was working on my laptop one day. After paying the bill, I no longer experienced issues.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was working on my laptop one day. After paying the bill, I no longer experienced issues.", "hypotheses": ["i got an e-mail saying my cable bill was current and service would be upgraded.", "The internet was very slow and then stopped completely."]}
{"example_id": "82", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Isa forgot about the bouquet. happen?", "a": "yes", "prior_probs": [0.4668909333315746, 0.5331090666684254], "posterior_probs": [0.43985971734003826, 0.5601402826599617], "prior_entropy": 0.6909531549137811, "posterior_entropy": 0.6858959294611592, "delta_entropy": 0.005057225452621883, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5601402826599617, "accuracy": 1, "tokens_in": 162, "tokens_out": 36, "tokens_total": 198, "latency_total": 0.01638626900057716, "latency_per_module": {"scorer": 0.008399506999921869, "prior": 0.00798676200065529}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 18, "tokens_total": 116}, "prior": {"tokens_in": 64, "tokens_out": 18, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!\nQuestion: Did Isa forgot about the bouquet. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!", "hypotheses": ["Isa went to the country site and found some flowers.", "Isa forgot about the bouquet."]}
{"example_id": "83", "dataset": "art", "method": "random_question", "asked": true, "q": "Did People destroyed the dam the beaver was building. happen?", "a": "no", "prior_probs": [0.4949569028203426, 0.5050430971796575], "posterior_probs": [0.6120771427496029, 0.38792285725039716], "prior_entropy": 0.6930963140371449, "posterior_entropy": 0.6678098817482663, "delta_entropy": 0.025286432288878613, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6120771427496029, "accuracy": 0, "tokens_in": 192, "tokens_out": 44, "tokens_total": 236, "latency_total": 0.02268604199980473, "latency_per_module": {"scorer": 0.010964971000248624, "prior": 0.011721070999556105}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 22, "tokens_total": 138}, "prior": {"tokens_in": 76, "tokens_out": 22, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.\nQuestion: Did People destroyed the dam the beaver was building. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.", "hypotheses": ["People destroyed the dam the beaver was building.", "Everyone was all over the hotel, trying to see him."]}
{"example_id": "84", "dataset": "art", "method": "random_question", "asked": true, "q": "Did We took a beautiful picture of Spain. happen?", "a": "no", "prior_probs": [0.48274284883258106, 0.5172571511674189], "posterior_probs": [0.5751762452410287, 0.42482375475897133], "prior_entropy": 0.6925514437149196, "posterior_entropy": 0.6818012696066166, "delta_entropy": 0.010750174108303034, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5751762452410287, "accuracy": 0, "tokens_in": 140, "tokens_out": 40, "tokens_total": 180, "latency_total": 0.020024430000376015, "latency_per_module": {"scorer": 0.009743665999849327, "prior": 0.010280764000526688}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 20, "tokens_total": 108}, "prior": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My family was on vacation in Italy. It was our favorite picture of the vacation.\nQuestion: Did We took a beautiful picture of Spain. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My family was on vacation in Italy. It was our favorite picture of the vacation.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My family was on vacation in Italy. It was our favorite picture of the vacation.", "hypotheses": ["We took a beautiful picture of Spain.", "We took a photo next to the coliseum in Rome."]}
{"example_id": "85", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Bill went to Lansing via air anyways. happen?", "a": "yes", "prior_probs": [0.5220601301779403, 0.47793986982205966], "posterior_probs": [0.3823462571981869, 0.6176537428018131], "prior_entropy": 0.6921735658547711, "posterior_entropy": 0.6652010590872883, "delta_entropy": 0.02697250676748275, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6176537428018131, "accuracy": 1, "tokens_in": 164, "tokens_out": 34, "tokens_total": 198, "latency_total": 0.01761716999953933, "latency_per_module": {"scorer": 0.008702256999640667, "prior": 0.008914912999898661}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 17, "tokens_total": 117}, "prior": {"tokens_in": 64, "tokens_out": 17, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.\nQuestion: Did Bill went to Lansing via air anyways. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.", "hypotheses": ["bill said he would be fine and left.", "Bill went to Lansing via air anyways."]}
{"example_id": "86", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She was not very talented. happen?", "a": "yes", "prior_probs": [0.4965328533869331, 0.5034671466130668], "posterior_probs": [0.40596506435865604, 0.5940349356413439], "prior_entropy": 0.6931231381539927, "posterior_entropy": 0.6753562840245265, "delta_entropy": 0.017766854129466214, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5940349356413439, "accuracy": 1, "tokens_in": 128, "tokens_out": 28, "tokens_total": 156, "latency_total": 0.017356139999719744, "latency_per_module": {"scorer": 0.008434716999545344, "prior": 0.0089214230001744}, "tokens_per_module": {"scorer": {"tokens_in": 80, "tokens_out": 14, "tokens_total": 94}, "prior": {"tokens_in": 48, "tokens_out": 14, "tokens_total": 62}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!\nQuestion: Did She was not very talented. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!", "hypotheses": ["Liv's mother signed her up.", "She was not very talented."]}
{"example_id": "87", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The baseball player looked out the window at a coffee shop. happen?", "a": "yes", "prior_probs": [0.48536694808139563, 0.5146330519186044], "posterior_probs": [0.39933991083615533, 0.6006600891638447], "prior_entropy": 0.6927188669867086, "posterior_entropy": 0.6727431159710969, "delta_entropy": 0.019975751015611665, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6006600891638447, "accuracy": 1, "tokens_in": 184, "tokens_out": 56, "tokens_total": 240, "latency_total": 0.019538204000127735, "latency_per_module": {"scorer": 0.010082382999826223, "prior": 0.009455821000301512}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 28, "tokens_total": 142}, "prior": {"tokens_in": 70, "tokens_out": 28, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!\nQuestion: Did The baseball player looked out the window at a coffee shop. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!", "hypotheses": ["One day a nurse said they should go to the coffee shop for a treat.", "The baseball player looked out the window at a coffee shop."]}
{"example_id": "88", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jake got stuff in the mud. happen?", "a": "yes", "prior_probs": [0.500141024585799, 0.49985897541420105], "posterior_probs": [0.43596389807070185, 0.5640361019292981], "prior_entropy": 0.6931471407820773, "posterior_entropy": 0.6849233673147995, "delta_entropy": 0.008223773467277762, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5640361019292981, "accuracy": 1, "tokens_in": 130, "tokens_out": 32, "tokens_total": 162, "latency_total": 0.018352743999457743, "latency_per_module": {"scorer": 0.009888397999930021, "prior": 0.008464345999527723}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 16, "tokens_total": 98}, "prior": {"tokens_in": 48, "tokens_out": 16, "tokens_total": 64}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jake was off roading. He had to get help to get out.\nQuestion: Did Jake got stuff in the mud. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Jake was off roading. He had to get help to get out.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake was off roading. He had to get help to get out.", "hypotheses": ["Jake ended up getting free from the mud.", "Jake got stuff in the mud."]}
{"example_id": "89", "dataset": "art", "method": "random_question", "asked": true, "q": "Did It was so fun, I was a clown. happen?", "a": "no", "prior_probs": [0.4483684781669959, 0.551631521833004], "posterior_probs": [0.6041421385104238, 0.39585786148957625], "prior_entropy": 0.6878060363970406, "posterior_entropy": 0.6712963883434058, "delta_entropy": 0.016509648053634862, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6041421385104238, "accuracy": 0, "tokens_in": 140, "tokens_out": 38, "tokens_total": 178, "latency_total": 0.016744192000260227, "latency_per_module": {"scorer": 0.0090296340003988, "prior": 0.007714557999861427}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 19, "tokens_total": 109}, "prior": {"tokens_in": 50, "tokens_out": 19, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I got a new racing game yesterday. Finally after hours of playing I stopped.\nQuestion: Did It was so fun, I was a clown. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I got a new racing game yesterday. Finally after hours of playing I stopped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I got a new racing game yesterday. Finally after hours of playing I stopped.", "hypotheses": ["It was so fun, I was a clown.", "I sat down to test out the game."]}
{"example_id": "90", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She doesn't ask nicely to get things indefinite. happen?", "a": "no", "prior_probs": [0.3680791136694201, 0.63192088633058], "posterior_probs": [0.5439159007972553, 0.45608409920274473], "prior_entropy": 0.6579254323609798, "posterior_entropy": 0.6892849931420373, "delta_entropy": -0.031359560781057505, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5439159007972553, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.016864204000739846, "latency_per_module": {"scorer": 0.008196146000045701, "prior": 0.008668058000694145}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Freda is the boss of her office. Freda can't understand why people have a problem with her!\nQuestion: Did She doesn't ask nicely to get things indefinite. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Freda is the boss of her office. Freda can't understand why people have a problem with her!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Freda is the boss of her office. Freda can't understand why people have a problem with her!", "hypotheses": ["She doesn't ask nicely to get things indefinite.", "She doesn't ask nicely to get things done."]}
{"example_id": "91", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Carly, noticed her daughter had gotten into Poison Ivy. happen?", "a": "yes", "prior_probs": [0.5296516170952371, 0.4703483829047629], "posterior_probs": [0.40055619273471005, 0.5994438072652899], "prior_entropy": 0.6913877116125613, "posterior_entropy": 0.6732365393740614, "delta_entropy": 0.018151172238499935, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5994438072652899, "accuracy": 1, "tokens_in": 190, "tokens_out": 40, "tokens_total": 230, "latency_total": 0.018049793000500358, "latency_per_module": {"scorer": 0.009220454000569589, "prior": 0.00882933899993077}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 20, "tokens_total": 136}, "prior": {"tokens_in": 74, "tokens_out": 20, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.\nQuestion: Did Carly, noticed her daughter had gotten into Poison Ivy. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.", "hypotheses": ["One of them had a bad cough.", "Carly, noticed her daughter had gotten into Poison Ivy."]}
{"example_id": "92", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Alexis made sure the tree wasn't under 20 feet tall. happen?", "a": "no", "prior_probs": [0.48982945320701626, 0.5101705467929837], "posterior_probs": [0.5861605045654602, 0.41383949543453974], "prior_entropy": 0.6929402862449963, "posterior_entropy": 0.6782255481044289, "delta_entropy": 0.01471473814056734, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5861605045654602, "accuracy": 0, "tokens_in": 184, "tokens_out": 46, "tokens_total": 230, "latency_total": 0.01691801199922338, "latency_per_module": {"scorer": 0.008599287999459193, "prior": 0.008318723999764188}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 23, "tokens_total": 137}, "prior": {"tokens_in": 70, "tokens_out": 23, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.\nQuestion: Did Alexis made sure the tree wasn't under 20 feet tall. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.", "hypotheses": ["Alexis made sure the tree wasn't under 20 feet tall.", "Alexis was worried it would be too big."]}
{"example_id": "93", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Francine decided to go to school to pursue her dreams. happen?", "a": "no", "prior_probs": [0.6592424657664189, 0.3407575342335811], "posterior_probs": [0.6563665991731485, 0.34363340082685134], "prior_entropy": 0.6415366677654246, "posterior_entropy": 0.6434161266894474, "delta_entropy": -0.0018794589240227477, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6563665991731485, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.014968806999604567, "latency_per_module": {"scorer": 0.007329215999561711, "prior": 0.007639591000042856}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.\nQuestion: Did Francine decided to go to school to pursue her dreams. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.", "hypotheses": ["Francine decided to go to school to pursue her dreams.", "francine applied to business school."]}
{"example_id": "94", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jack caught a ball that bounced over the fence. happen?", "a": "no", "prior_probs": [0.5440713882680684, 0.45592861173193144], "posterior_probs": [0.6289758792893719, 0.37102412071062807], "prior_entropy": 0.6892575603602893, "posterior_entropy": 0.6594984860689397, "delta_entropy": 0.029759074291349608, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6289758792893719, "accuracy": 0, "tokens_in": 180, "tokens_out": 38, "tokens_total": 218, "latency_total": 0.015094598001269333, "latency_per_module": {"scorer": 0.0074887770006171195, "prior": 0.007605821000652213}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 19, "tokens_total": 129}, "prior": {"tokens_in": 70, "tokens_out": 19, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.\nQuestion: Did Jack caught a ball that bounced over the fence. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.", "hypotheses": ["Jack caught a ball that bounced over the fence.", "Jack saw how wild the crowd was getting."]}
{"example_id": "95", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Nathan never got into a fight. happen?", "a": "no", "prior_probs": [0.5983544817378198, 0.40164551826218015], "posterior_probs": [0.6266960741546074, 0.3733039258453927], "prior_entropy": 0.6736732287085543, "posterior_entropy": 0.660690699410478, "delta_entropy": 0.012982529298076306, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6266960741546074, "accuracy": 0, "tokens_in": 154, "tokens_out": 30, "tokens_total": 184, "latency_total": 0.014741865999894799, "latency_per_module": {"scorer": 0.007482242999685695, "prior": 0.007259623000209103}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 15, "tokens_total": 109}, "prior": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.\nQuestion: Did Nathan never got into a fight. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.", "hypotheses": ["Nathan never got into a fight.", "Nathan got detention in school."]}
{"example_id": "96", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Mike found pasta hard to make. happen?", "a": "no", "prior_probs": [0.6835728746419494, 0.3164271253580506], "posterior_probs": [0.7071798178427685, 0.29282018215723155], "prior_entropy": 0.624146934819759, "posterior_entropy": 0.6046575516987129, "delta_entropy": 0.01948938312104609, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7071798178427685, "accuracy": 0, "tokens_in": 146, "tokens_out": 28, "tokens_total": 174, "latency_total": 0.01472671299961803, "latency_per_module": {"scorer": 0.007384386999547132, "prior": 0.007342326000070898}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 14, "tokens_total": 104}, "prior": {"tokens_in": 56, "tokens_out": 14, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.\nQuestion: Did Mike found pasta hard to make. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.", "hypotheses": ["Mike found pasta hard to make.", "Mile loves italian food."]}
{"example_id": "97", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He didn't know that part of town very good. happen?", "a": "yes", "prior_probs": [0.5251973619984914, 0.47480263800150857], "posterior_probs": [0.3719844503562799, 0.62801554964372], "prior_entropy": 0.6918768284318874, "posterior_entropy": 0.6600033976174073, "delta_entropy": 0.03187343081448002, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.62801554964372, "accuracy": 1, "tokens_in": 166, "tokens_out": 36, "tokens_total": 202, "latency_total": 0.01621157200042944, "latency_per_module": {"scorer": 0.008458215000246128, "prior": 0.007753357000183314}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.\nQuestion: Did He didn't know that part of town very good. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.", "hypotheses": ["Randy knew the area well.", "He didn't know that part of town very good."]}
{"example_id": "98", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Cat sent a love note to the boy. happen?", "a": "no", "prior_probs": [0.4126552520352513, 0.5873447479647488], "posterior_probs": [0.5593052674525093, 0.4406947325474907], "prior_entropy": 0.6778104031708004, "posterior_entropy": 0.6860963640804533, "delta_entropy": -0.008285960909652834, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5593052674525093, "accuracy": 0, "tokens_in": 166, "tokens_out": 36, "tokens_total": 202, "latency_total": 0.014954385999772057, "latency_per_module": {"scorer": 0.007097718000295572, "prior": 0.007856667999476485}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 18, "tokens_total": 120}, "prior": {"tokens_in": 64, "tokens_out": 18, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!\nQuestion: Did Cat sent a love note to the boy. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!", "hypotheses": ["Cat sent a love note to the boy.", "She told him she did not like him."]}
{"example_id": "99", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Lacy missed Karen so much, she couldn't bear not talking to her friend. happen?", "a": "yes", "prior_probs": [0.5060554166199094, 0.49394458338009056], "posterior_probs": [0.48299223953448067, 0.5170077604655193], "prior_entropy": 0.693073842624229, "posterior_entropy": 0.6925685411093562, "delta_entropy": 0.0005053015148728246, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5170077604655193, "accuracy": 1, "tokens_in": 166, "tokens_out": 70, "tokens_total": 236, "latency_total": 0.015143536000323365, "latency_per_module": {"scorer": 0.007646215000022494, "prior": 0.007497321000300872}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 35, "tokens_total": 145}, "prior": {"tokens_in": 56, "tokens_out": 35, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lacy and Karen got in a fight. Karen apologized too so they could be friends again.\nQuestion: Did Lacy missed Karen so much, she couldn't bear not talking to her friend. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Lacy and Karen got in a fight. Karen apologized too so they could be friends again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lacy and Karen got in a fight. Karen apologized too so they could be friends again.", "hypotheses": ["Karen missed Lacy so much, she couldn't bear not talking to her friend.", "Lacy missed Karen so much, she couldn't bear not talking to her friend."]}
{"example_id": "100", "dataset": "art", "method": "random_question", "asked": true, "q": "Did my friend usually talks about some business deal. happen?", "a": "yes", "prior_probs": [0.6252029901029774, 0.3747970098970227], "posterior_probs": [0.4423911725792829, 0.5576088274207172], "prior_entropy": 0.6614594576997013, "posterior_entropy": 0.6864948623157481, "delta_entropy": -0.025035404616046808, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5576088274207172, "accuracy": 1, "tokens_in": 166, "tokens_out": 50, "tokens_total": 216, "latency_total": 0.014706203000059759, "latency_per_module": {"scorer": 0.007476028000382939, "prior": 0.007230174999676819}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 25, "tokens_total": 127}, "prior": {"tokens_in": 64, "tokens_out": 25, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My friend had an announcement to make. So, I put on a smile and wished him the best of luck.\nQuestion: Did my friend usually talks about some business deal. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: My friend had an announcement to make. So, I put on a smile and wished him the best of luck.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend had an announcement to make. So, I put on a smile and wished him the best of luck.", "hypotheses": ["My friend told us he had cancer and was expected to die in a week.", "my friend usually talks about some business deal."]}
{"example_id": "101", "dataset": "art", "method": "random_question", "asked": true, "q": "Did George was the underdog, but he had been training months to compete in this event. happen?", "a": "yes", "prior_probs": [0.5037615781635802, 0.49623842183641964], "posterior_probs": [0.33047910829379884, 0.6695208917062011], "prior_entropy": 0.6931188813504345, "posterior_entropy": 0.6345174140752035, "delta_entropy": 0.05860146727523108, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6695208917062011, "accuracy": 1, "tokens_in": 174, "tokens_out": 54, "tokens_total": 228, "latency_total": 0.015597679999700631, "latency_per_module": {"scorer": 0.008123159000206215, "prior": 0.007474520999494416}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 27, "tokens_total": 141}, "prior": {"tokens_in": 60, "tokens_out": 27, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: George was about to participate in his first professional fight. George proved his skills and won his first match.\nQuestion: Did George was the underdog, but he had been training months to compete in this event. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: George was about to participate in his first professional fight. George proved his skills and won his first match.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "George was about to participate in his first professional fight. George proved his skills and won his first match.", "hypotheses": ["George trained hard for two days before the fight.", "George was the underdog, but he had been training months to compete in this event."]}
{"example_id": "102", "dataset": "art", "method": "random_question", "asked": true, "q": "Did They played basketball out back all afternoon. happen?", "a": "yes", "prior_probs": [0.6479163582241244, 0.3520836417758756], "posterior_probs": [0.4148306671802267, 0.5851693328197733], "prior_entropy": 0.6487269617884174, "posterior_entropy": 0.6785685657971783, "delta_entropy": -0.029841604008760902, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5851693328197733, "accuracy": 1, "tokens_in": 148, "tokens_out": 36, "tokens_total": 184, "latency_total": 0.014617579999139707, "latency_per_module": {"scorer": 0.00756228599948372, "prior": 0.007055293999655987}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob and his kids love football. Bob and his kids share a hug to celebrate the win.\nQuestion: Did They played basketball out back all afternoon. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Bob and his kids love football. Bob and his kids share a hug to celebrate the win.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob and his kids love football. Bob and his kids share a hug to celebrate the win.", "hypotheses": ["The team that Bob and his kids like won.", "They played basketball out back all afternoon."]}
{"example_id": "103", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He didn't see the toy he really wanted in any of the aisles. happen?", "a": "no", "prior_probs": [0.5873619102187421, 0.412638089781258], "posterior_probs": [0.688925036893191, 0.311074963106809], "prior_entropy": 0.67780434429697, "posterior_entropy": 0.6199580637323454, "delta_entropy": 0.0578462805646246, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.688925036893191, "accuracy": 0, "tokens_in": 170, "tokens_out": 50, "tokens_total": 220, "latency_total": 0.01470543999948859, "latency_per_module": {"scorer": 0.0076306349992592, "prior": 0.007074805000229389}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 25, "tokens_total": 137}, "prior": {"tokens_in": 58, "tokens_out": 25, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Alex was at target with his mom. He begged his mother to buy it until she gave in.\nQuestion: Did He didn't see the toy he really wanted in any of the aisles. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Alex was at target with his mom. He begged his mother to buy it until she gave in.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alex was at target with his mom. He begged his mother to buy it until she gave in.", "hypotheses": ["He didn't see the toy he really wanted in any of the aisles.", "Alex saw a game he really wanted."]}
{"example_id": "104", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Nova applied to a few dance schools but was denied by her first choice. happen?", "a": "no", "prior_probs": [0.6404063862747398, 0.3595936137252602], "posterior_probs": [0.6551086188003145, 0.34489138119968565], "prior_entropy": 0.6531840162431645, "posterior_entropy": 0.6442267151415673, "delta_entropy": 0.008957301101597204, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6551086188003145, "accuracy": 0, "tokens_in": 146, "tokens_out": 48, "tokens_total": 194, "latency_total": 0.016066180000052555, "latency_per_module": {"scorer": 0.007687792999604426, "prior": 0.008378387000448129}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 24, "tokens_total": 122}, "prior": {"tokens_in": 48, "tokens_out": 24, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nova dreamed of being a professional dancer. Nova's second choice accepted her.\nQuestion: Did Nova applied to a few dance schools but was denied by her first choice. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nova dreamed of being a professional dancer. Nova's second choice accepted her.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nova dreamed of being a professional dancer. Nova's second choice accepted her.", "hypotheses": ["Nova applied to a few dance schools but was denied by her first choice.", "Nova applied to one dance school."]}
{"example_id": "105", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The sand castle was built right on the shore. happen?", "a": "yes", "prior_probs": [0.45470671903037124, 0.5452932809696288], "posterior_probs": [0.40369890937083347, 0.5963010906291666], "prior_entropy": 0.6890385880217316, "posterior_entropy": 0.6744829706455153, "delta_entropy": 0.014555617376216246, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5963010906291666, "accuracy": 1, "tokens_in": 160, "tokens_out": 48, "tokens_total": 208, "latency_total": 0.017612790999919525, "latency_per_module": {"scorer": 0.00872980300027848, "prior": 0.008882987999641045}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 24, "tokens_total": 124}, "prior": {"tokens_in": 60, "tokens_out": 24, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.\nQuestion: Did The sand castle was built right on the shore. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.", "hypotheses": ["An unexpected event happened when the tide didn't come in that day.", "The sand castle was built right on the shore."]}
{"example_id": "106", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tami was tall for her age. happen?", "a": "yes", "prior_probs": [0.43206894712477, 0.5679310528752299], "posterior_probs": [0.3553356120242836, 0.6446643879757163], "prior_entropy": 0.6838893198791889, "posterior_entropy": 0.6506871683184323, "delta_entropy": 0.033202151560756565, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6446643879757163, "accuracy": 1, "tokens_in": 180, "tokens_out": 36, "tokens_total": 216, "latency_total": 0.01791714899991348, "latency_per_module": {"scorer": 0.009049665000020468, "prior": 0.008867483999893011}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 18, "tokens_total": 126}, "prior": {"tokens_in": 72, "tokens_out": 18, "tokens_total": 90}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.\nQuestion: Did Tami was tall for her age. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.", "hypotheses": ["She wanted to be less involved and lose friends.", "Tami was tall for her age."]}
{"example_id": "107", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Donald is a selfless, wonderful person. happen?", "a": "no", "prior_probs": [0.4372740685760933, 0.5627259314239066], "posterior_probs": [0.5742594267861512, 0.42574057321384884], "prior_entropy": 0.6852573237333645, "posterior_entropy": 0.6820773478694779, "delta_entropy": 0.0031799758638865194, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5742594267861512, "accuracy": 0, "tokens_in": 122, "tokens_out": 38, "tokens_total": 160, "latency_total": 0.016391518999625987, "latency_per_module": {"scorer": 0.008358324999790057, "prior": 0.00803319399983593}, "tokens_per_module": {"scorer": {"tokens_in": 80, "tokens_out": 19, "tokens_total": 99}, "prior": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Donald is running for president. Hopefully he loses the election.\nQuestion: Did Donald is a selfless, wonderful person. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Donald is running for president. Hopefully he loses the election.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Donald is running for president. Hopefully he loses the election.", "hypotheses": ["Donald is a selfless, wonderful person.", "Donald is not the candidate I want for president."]}
{"example_id": "108", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I committed to exercise every month by jumping rope. happen?", "a": "yes", "prior_probs": [0.5228436593353636, 0.4771563406646364], "posterior_probs": [0.36940810192510887, 0.630591898074891], "prior_entropy": 0.6921031516315433, "posterior_entropy": 0.6586399109207632, "delta_entropy": 0.03346324071078011, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.630591898074891, "accuracy": 1, "tokens_in": 152, "tokens_out": 42, "tokens_total": 194, "latency_total": 0.016223194998929102, "latency_per_module": {"scorer": 0.008140078999531397, "prior": 0.008083115999397705}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 21, "tokens_total": 117}, "prior": {"tokens_in": 56, "tokens_out": 21, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was very out of shape. After weeks of jumping rope, I began to feel excellent.\nQuestion: Did I committed to exercise every month by jumping rope. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I was very out of shape. After weeks of jumping rope, I began to feel excellent.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was very out of shape. After weeks of jumping rope, I began to feel excellent.", "hypotheses": ["I had rope that I used for jumping at home.", "I committed to exercise every month by jumping rope."]}
{"example_id": "109", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ora's mom kicked her out of the house so their gas bill would reduce. happen?", "a": "yes", "prior_probs": [0.47234653950590905, 0.527653460494091], "posterior_probs": [0.3551095925255607, 0.6448904074744394], "prior_entropy": 0.6916169721313477, "posterior_entropy": 0.6505524244193581, "delta_entropy": 0.04106454771198953, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6448904074744394, "accuracy": 1, "tokens_in": 190, "tokens_out": 54, "tokens_total": 244, "latency_total": 0.0165858569998818, "latency_per_module": {"scorer": 0.008979221999652509, "prior": 0.007606635000229289}, "tokens_per_module": {"scorer": {"tokens_in": 122, "tokens_out": 27, "tokens_total": 149}, "prior": {"tokens_in": 68, "tokens_out": 27, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.\nQuestion: Did Ora's mom kicked her out of the house so their gas bill would reduce. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.", "hypotheses": ["Ora's family needed to use more gas.", "Ora's mom kicked her out of the house so their gas bill would reduce."]}
{"example_id": "110", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The puppy was given to me by a stork. happen?", "a": "no", "prior_probs": [0.5043134215278807, 0.4956865784721193], "posterior_probs": [0.598330244325901, 0.40166975567409896], "prior_entropy": 0.6931099688858192, "posterior_entropy": 0.6736828888448205, "delta_entropy": 0.01942708004099869, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.598330244325901, "accuracy": 0, "tokens_in": 202, "tokens_out": 48, "tokens_total": 250, "latency_total": 0.01793201400050748, "latency_per_module": {"scorer": 0.009549911000249267, "prior": 0.008382103000258212}, "tokens_per_module": {"scorer": {"tokens_in": 122, "tokens_out": 24, "tokens_total": 146}, "prior": {"tokens_in": 80, "tokens_out": 24, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.\nQuestion: Did The puppy was given to me by a stork. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.", "hypotheses": ["The puppy was given to me by a stork.", "I got up and tried to find out why they kept barking."]}
{"example_id": "111", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Dan tried to eat 30 cold hot dogs. happen?", "a": "yes", "prior_probs": [0.5925861323348321, 0.4074138676651678], "posterior_probs": [0.43152534748000687, 0.5684746525199932], "prior_entropy": 0.6759034511018422, "posterior_entropy": 0.6837400894878912, "delta_entropy": -0.007836638386048977, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5684746525199932, "accuracy": 1, "tokens_in": 158, "tokens_out": 44, "tokens_total": 202, "latency_total": 0.016075804000138305, "latency_per_module": {"scorer": 0.008279112000309397, "prior": 0.007796691999828909}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}, "prior": {"tokens_in": 60, "tokens_out": 22, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.\nQuestion: Did Dan tried to eat 30 cold hot dogs. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.", "hypotheses": ["Dan ate a lot of food very slowly, hoping to win.", "Dan tried to eat 30 cold hot dogs."]}
{"example_id": "112", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Melissa was being rude. happen?", "a": "no", "prior_probs": [0.5464544162805514, 0.4535455837194486], "posterior_probs": [0.5784649224119455, 0.42153507758805453], "prior_entropy": 0.688824924075583, "posterior_entropy": 0.6807826472604681, "delta_entropy": 0.008042276815114935, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5784649224119455, "accuracy": 0, "tokens_in": 150, "tokens_out": 40, "tokens_total": 190, "latency_total": 0.019271471000138263, "latency_per_module": {"scorer": 0.010308089000318432, "prior": 0.00896338199981983}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.\nQuestion: Did Melissa was being rude. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.", "hypotheses": ["Melissa was being rude.", "Melissa's friend insisted they go out to eat somewhere Melissa hated."]}
{"example_id": "113", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Nell was told to do something unexpected. happen?", "a": "yes", "prior_probs": [0.4783050226856938, 0.5216949773143061], "posterior_probs": [0.3966425942713651, 0.6033574057286349], "prior_entropy": 0.6922055408777172, "posterior_entropy": 0.6716268503199789, "delta_entropy": 0.02057869055773831, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6033574057286349, "accuracy": 1, "tokens_in": 174, "tokens_out": 36, "tokens_total": 210, "latency_total": 0.01383305399940582, "latency_per_module": {"scorer": 0.006789164000110759, "prior": 0.007043889999295061}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 68, "tokens_out": 18, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.\nQuestion: Did Nell was told to do something unexpected. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.", "hypotheses": ["Nell studied biology to draw an animal.", "Nell was told to do something unexpected."]}
{"example_id": "114", "dataset": "art", "method": "random_question", "asked": true, "q": "Did His dad asked Eli how to tie his shoes. happen?", "a": "no", "prior_probs": [0.527262563582004, 0.47273743641799604], "posterior_probs": [0.5579518583675479, 0.4420481416324521], "prior_entropy": 0.69165994837897, "posterior_entropy": 0.6864152247577697, "delta_entropy": 0.00524472362120032, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5579518583675479, "accuracy": 0, "tokens_in": 152, "tokens_out": 40, "tokens_total": 192, "latency_total": 0.013877540000066801, "latency_per_module": {"scorer": 0.00706150299993169, "prior": 0.006816037000135111}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 20, "tokens_total": 116}, "prior": {"tokens_in": 56, "tokens_out": 20, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.\nQuestion: Did His dad asked Eli how to tie his shoes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.", "hypotheses": ["His dad asked Eli how to tie his shoes.", "eli asked his dad how to tie his shoes."]}
{"example_id": "115", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She was anxious to buy some today. happen?", "a": "no", "prior_probs": [0.48110675756754917, 0.5188932424324508], "posterior_probs": [0.6185057717387646, 0.38149422826123536], "prior_entropy": 0.6924331013528291, "posterior_entropy": 0.6647908873843817, "delta_entropy": 0.02764221396844735, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6185057717387646, "accuracy": 0, "tokens_in": 172, "tokens_out": 36, "tokens_total": 208, "latency_total": 0.013885742000638857, "latency_per_module": {"scorer": 0.006792524000047706, "prior": 0.0070932180005911505}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}, "prior": {"tokens_in": 68, "tokens_out": 18, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.\nQuestion: Did She was anxious to buy some today. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.", "hypotheses": ["She was anxious to buy some today.", "Francine saw bananas for sale at the fair."]}
{"example_id": "116", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ben pitched a large tent on the sand to block out the sun. happen?", "a": "yes", "prior_probs": [0.47096512638183935, 0.5290348736181606], "posterior_probs": [0.3482738071706149, 0.6517261928293852], "prior_entropy": 0.6914601839197501, "posterior_entropy": 0.6463715041180824, "delta_entropy": 0.04508867980166764, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6517261928293852, "accuracy": 1, "tokens_in": 160, "tokens_out": 44, "tokens_total": 204, "latency_total": 0.014182475999405142, "latency_per_module": {"scorer": 0.007137513999623479, "prior": 0.007044961999781663}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 22, "tokens_total": 126}, "prior": {"tokens_in": 56, "tokens_out": 22, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ben went to the beach on a sunny day. Ben crawled into his tent and napped.\nQuestion: Did Ben pitched a large tent on the sand to block out the sun. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Ben went to the beach on a sunny day. Ben crawled into his tent and napped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ben went to the beach on a sunny day. Ben crawled into his tent and napped.", "hypotheses": ["Ben spent hours sitting in the sun.", "Ben pitched a large tent on the sand to block out the sun."]}
{"example_id": "117", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tili ran for the prison gate one night. happen?", "a": "no", "prior_probs": [0.6605205014879388, 0.33947949851206116], "posterior_probs": [0.7161064630653952, 0.2838935369346048], "prior_entropy": 0.6406896284443233, "posterior_entropy": 0.5965931208986868, "delta_entropy": 0.04409650754563654, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7161064630653952, "accuracy": 0, "tokens_in": 148, "tokens_out": 30, "tokens_total": 178, "latency_total": 0.01415635699959239, "latency_per_module": {"scorer": 0.007259394999891811, "prior": 0.006896961999700579}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 15, "tokens_total": 109}, "prior": {"tokens_in": 54, "tokens_out": 15, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.\nQuestion: Did Tili ran for the prison gate one night. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.", "hypotheses": ["Tili ran for the prison gate one night.", "Doug formulated a plan."]}
{"example_id": "118", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Sam lost his job and could not make the payment. happen?", "a": "no", "prior_probs": [0.47853895165243243, 0.5214610483475676], "posterior_probs": [0.571836827637379, 0.428163172362621], "prior_entropy": 0.6922257443158863, "posterior_entropy": 0.6827903163881975, "delta_entropy": 0.009435427927688811, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.571836827637379, "accuracy": 0, "tokens_in": 166, "tokens_out": 46, "tokens_total": 212, "latency_total": 0.013772493999567814, "latency_per_module": {"scorer": 0.006722350999552873, "prior": 0.007050143000014941}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}, "prior": {"tokens_in": 62, "tokens_out": 23, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.\nQuestion: Did Sam lost his job and could not make the payment. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.", "hypotheses": ["Sam lost his job and could not make the payment.", "Sam decided to work two jobs to pay off his debt."]}
{"example_id": "119", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Katie needed to hurry and get it clean. happen?", "a": "yes", "prior_probs": [0.36083121223098397, 0.639168787769016], "posterior_probs": [0.31434060061653146, 0.6856593993834684], "prior_entropy": 0.6538949456312322, "posterior_entropy": 0.622529733706699, "delta_entropy": 0.03136521192453323, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6856593993834684, "accuracy": 1, "tokens_in": 162, "tokens_out": 38, "tokens_total": 200, "latency_total": 0.01415468199957104, "latency_per_module": {"scorer": 0.007142304999433691, "prior": 0.007012377000137349}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 19, "tokens_total": 119}, "prior": {"tokens_in": 62, "tokens_out": 19, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.\nQuestion: Did Katie needed to hurry and get it clean. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.", "hypotheses": ["The kitchen got so bad mold might grow.", "Katie needed to hurry and get it clean."]}
{"example_id": "120", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The directions took Randy thru a great part of town. happen?", "a": "no", "prior_probs": [0.47581760802240475, 0.5241823919775953], "posterior_probs": [0.6577187940367402, 0.3422812059632599], "prior_entropy": 0.6919771479977935, "posterior_entropy": 0.6425370059674315, "delta_entropy": 0.049440142030362066, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6577187940367402, "accuracy": 0, "tokens_in": 174, "tokens_out": 40, "tokens_total": 214, "latency_total": 0.01366197399875091, "latency_per_module": {"scorer": 0.006489349999355909, "prior": 0.007172623999395}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 20, "tokens_total": 128}, "prior": {"tokens_in": 66, "tokens_out": 20, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.\nQuestion: Did The directions took Randy thru a great part of town. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.", "hypotheses": ["The directions took Randy thru a great part of town.", "The house had boarded windows and looked bad."]}
{"example_id": "121", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I was worried about how to tow. happen?", "a": "no", "prior_probs": [0.41262664839655694, 0.5873733516034432], "posterior_probs": [0.5133367128800578, 0.4866632871199422], "prior_entropy": 0.6778003044142535, "posterior_entropy": 0.6927914025423878, "delta_entropy": -0.014991098128134261, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5133367128800578, "accuracy": 0, "tokens_in": 180, "tokens_out": 42, "tokens_total": 222, "latency_total": 0.013876011999855109, "latency_per_module": {"scorer": 0.006869830000141519, "prior": 0.00700618199971359}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 21, "tokens_total": 129}, "prior": {"tokens_in": 72, "tokens_out": 21, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!\nQuestion: Did I was worried about how to tow. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!", "hypotheses": ["I was worried about how to tow.", "I called my insurance company to see if I could get assistance."]}
{"example_id": "122", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tia thought something seemed good between her parents. happen?", "a": "yes", "prior_probs": [0.5599508886754255, 0.4400491113245745], "posterior_probs": [0.4389791252571459, 0.5610208747428541], "prior_entropy": 0.6859416391441366, "posterior_entropy": 0.6856814888216267, "delta_entropy": 0.0002601503225099222, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5610208747428541, "accuracy": 1, "tokens_in": 180, "tokens_out": 36, "tokens_total": 216, "latency_total": 0.013402124000094773, "latency_per_module": {"scorer": 0.006402119999620481, "prior": 0.007000004000474291}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 18, "tokens_total": 128}, "prior": {"tokens_in": 70, "tokens_out": 18, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.\nQuestion: Did Tia thought something seemed good between her parents. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.", "hypotheses": ["Tia had dinner with her parents.", "Tia thought something seemed good between her parents."]}
{"example_id": "123", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Joe tied them and fell down the stairs. happen?", "a": "yes", "prior_probs": [0.554069221489842, 0.44593077851015805], "posterior_probs": [0.45520569944273587, 0.5447943005572641], "prior_entropy": 0.6872887698399445, "posterior_entropy": 0.6891287363108339, "delta_entropy": -0.0018399664708894647, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5447943005572641, "accuracy": 1, "tokens_in": 190, "tokens_out": 42, "tokens_total": 232, "latency_total": 0.01644289799969556, "latency_per_module": {"scorer": 0.007889880000220728, "prior": 0.008553017999474832}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 21, "tokens_total": 135}, "prior": {"tokens_in": 76, "tokens_out": 21, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.\nQuestion: Did Joe tied them and fell down the stairs. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.", "hypotheses": ["Joe tripped down the stairs with his shoes untied.", "Joe tied them and fell down the stairs."]}
{"example_id": "124", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Nita practiced playing rummy with a dog. happen?", "a": "no", "prior_probs": [0.4648569698295527, 0.5351430301704473], "posterior_probs": [0.5494695799517437, 0.45053042004825633], "prior_entropy": 0.6906750776487804, "posterior_entropy": 0.688244685137449, "delta_entropy": 0.0024303925113314806, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5494695799517437, "accuracy": 0, "tokens_in": 172, "tokens_out": 40, "tokens_total": 212, "latency_total": 0.016132909999214462, "latency_per_module": {"scorer": 0.0076773479995608795, "prior": 0.008455561999653582}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 20, "tokens_total": 126}, "prior": {"tokens_in": 66, "tokens_out": 20, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.\nQuestion: Did Nita practiced playing rummy with a dog. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.", "hypotheses": ["Nita practiced playing rummy with a dog.", "Nita was never able to beat her Dad."]}
{"example_id": "125", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Fred saw a child come over with a pin. happen?", "a": "no", "prior_probs": [0.5531757724693236, 0.44682422753067647], "posterior_probs": [0.6776444487220556, 0.32235555127794435], "prior_entropy": 0.6874811455731205, "posterior_entropy": 0.6286322725880162, "delta_entropy": 0.05884887298510422, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6776444487220556, "accuracy": 0, "tokens_in": 152, "tokens_out": 34, "tokens_total": 186, "latency_total": 0.016529386000001978, "latency_per_module": {"scorer": 0.008223097000154667, "prior": 0.008306288999847311}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}, "prior": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Fred had a job at the fair to fill the balloons. The balloon popped in his face!\nQuestion: Did Fred saw a child come over with a pin. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Fred had a job at the fair to fill the balloons. The balloon popped in his face!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Fred had a job at the fair to fill the balloons. The balloon popped in his face!", "hypotheses": ["Fred saw a child come over with a pin.", "Fred fill one balloon too small."]}
{"example_id": "126", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She had a flat tire and changed it. happen?", "a": "yes", "prior_probs": [0.5468633476618596, 0.4531366523381404], "posterior_probs": [0.40411640245129926, 0.5958835975487007], "prior_entropy": 0.6887483802422558, "posterior_entropy": 0.6746454628524573, "delta_entropy": 0.01410291738979852, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5958835975487007, "accuracy": 1, "tokens_in": 174, "tokens_out": 42, "tokens_total": 216, "latency_total": 0.016286458999275055, "latency_per_module": {"scorer": 0.008062730999881751, "prior": 0.008223727999393304}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 21, "tokens_total": 127}, "prior": {"tokens_in": 68, "tokens_out": 21, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.\nQuestion: Did She had a flat tire and changed it. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.", "hypotheses": ["Samantha found some friends who gave her a ride.", "She had a flat tire and changed it."]}
{"example_id": "127", "dataset": "art", "method": "random_question", "asked": true, "q": "Did It was the right size. happen?", "a": "yes", "prior_probs": [0.4549968142922656, 0.5450031857077344], "posterior_probs": [0.40016588926648955, 0.5998341107335105], "prior_entropy": 0.6890911202623089, "posterior_entropy": 0.6730788719875, "delta_entropy": 0.01601224827480885, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5998341107335105, "accuracy": 1, "tokens_in": 164, "tokens_out": 26, "tokens_total": 190, "latency_total": 0.01659030300015729, "latency_per_module": {"scorer": 0.00884780900014448, "prior": 0.007742494000012812}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 13, "tokens_total": 111}, "prior": {"tokens_in": 66, "tokens_out": 13, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.\nQuestion: Did It was the right size. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.", "hypotheses": ["The tree fell on my fort.", "It was the right size."]}
{"example_id": "128", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Mike was having blood drawn because he needed a routine checkup. happen?", "a": "no", "prior_probs": [0.5035711515654095, 0.49642884843459045], "posterior_probs": [0.6479735181300184, 0.3520264818699817], "prior_entropy": 0.6931216740940784, "posterior_entropy": 0.648692093209366, "delta_entropy": 0.04442958088471238, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6479735181300184, "accuracy": 0, "tokens_in": 162, "tokens_out": 44, "tokens_total": 206, "latency_total": 0.01741198599938798, "latency_per_module": {"scorer": 0.008609951999460463, "prior": 0.008802033999927517}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 22, "tokens_total": 126}, "prior": {"tokens_in": 58, "tokens_out": 22, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike had to go to the doctor. All the blood work came back clear and he was relieved.\nQuestion: Did Mike was having blood drawn because he needed a routine checkup. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mike had to go to the doctor. All the blood work came back clear and he was relieved.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike had to go to the doctor. All the blood work came back clear and he was relieved.", "hypotheses": ["Mike was having blood drawn because he needed a routine checkup.", "Mike complained of soreness in his kidneys."]}
{"example_id": "129", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Rocky was very good at playing hockey. happen?", "a": "no", "prior_probs": [0.5061941554108962, 0.49380584458910376], "posterior_probs": [0.563181834938075, 0.436818165061925], "prior_entropy": 0.6930704434725565, "posterior_entropy": 0.685141907650995, "delta_entropy": 0.0079285358215615, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.563181834938075, "accuracy": 0, "tokens_in": 148, "tokens_out": 42, "tokens_total": 190, "latency_total": 0.0168763729998318, "latency_per_module": {"scorer": 0.007818781999958446, "prior": 0.009057590999873355}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}, "prior": {"tokens_in": 56, "tokens_out": 21, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.\nQuestion: Did Rocky was very good at playing hockey. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.", "hypotheses": ["Rocky was very good at playing hockey.", "Rocky was a rocket scientist, and he hated rockets."]}
{"example_id": "130", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Arnold saw a boy. happen?", "a": "no", "prior_probs": [0.4433643422955545, 0.5566356577044455], "posterior_probs": [0.5311127139057278, 0.4688872860942722], "prior_entropy": 0.686718195974517, "posterior_entropy": 0.6912099273179113, "delta_entropy": -0.004491731343394312, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5311127139057278, "accuracy": 0, "tokens_in": 110, "tokens_out": 34, "tokens_total": 144, "latency_total": 0.013688129999536613, "latency_per_module": {"scorer": 0.007267493999279395, "prior": 0.006420636000257218}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 17, "tokens_total": 87}, "prior": {"tokens_in": 40, "tokens_out": 17, "tokens_total": 57}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Arnold was scared of girls. He nearly fainted.\nQuestion: Did Arnold saw a boy. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Arnold was scared of girls. He nearly fainted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Arnold was scared of girls. He nearly fainted.", "hypotheses": ["Arnold saw a boy.", "A girl came up to hug him one day."]}
{"example_id": "131", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Bobby found a cat in the garden. happen?", "a": "no", "prior_probs": [0.5233937045763193, 0.4766062954236807], "posterior_probs": [0.5847974362098407, 0.41520256379015924], "prior_entropy": 0.6920522500476836, "posterior_entropy": 0.678696224851346, "delta_entropy": 0.013356025196337606, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5847974362098407, "accuracy": 0, "tokens_in": 168, "tokens_out": 38, "tokens_total": 206, "latency_total": 0.016335150999111647, "latency_per_module": {"scorer": 0.006871515999591793, "prior": 0.009463634999519854}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}, "prior": {"tokens_in": 66, "tokens_out": 19, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.\nQuestion: Did Bobby found a cat in the garden. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.", "hypotheses": ["Bobby found a cat in the garden.", "Bobby begged his mom for a feral cat."]}
{"example_id": "132", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Lucky looked for them in one store. happen?", "a": "no", "prior_probs": [0.2999824166984342, 0.7000175833015658], "posterior_probs": [0.5238665030943123, 0.4761334969056878], "prior_entropy": 0.610849403022965, "posterior_entropy": 0.6920075276159368, "delta_entropy": -0.08115812459297178, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5238665030943123, "accuracy": 0, "tokens_in": 180, "tokens_out": 48, "tokens_total": 228, "latency_total": 0.014025628999661421, "latency_per_module": {"scorer": 0.0071360969996021595, "prior": 0.006889532000059262}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 24, "tokens_total": 132}, "prior": {"tokens_in": 72, "tokens_out": 24, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.\nQuestion: Did Lucky looked for them in one store. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.", "hypotheses": ["Lucky looked for them in one store.", "Lucy looked for the sandals everywhere, but could never find them."]}
{"example_id": "133", "dataset": "art", "method": "random_question", "asked": true, "q": "Did We found one of them in the backyard. happen?", "a": "yes", "prior_probs": [0.4819782992719845, 0.5180217007280155], "posterior_probs": [0.39587947183927336, 0.6041205281607266], "prior_entropy": 0.6924974764463473, "posterior_entropy": 0.6713055232350339, "delta_entropy": 0.021191953211313397, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6041205281607266, "accuracy": 1, "tokens_in": 178, "tokens_out": 30, "tokens_total": 208, "latency_total": 0.01354989899937209, "latency_per_module": {"scorer": 0.006681182999273005, "prior": 0.006868716000099084}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 15, "tokens_total": 123}, "prior": {"tokens_in": 70, "tokens_out": 15, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.\nQuestion: Did We found one of them in the backyard. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.", "hypotheses": ["We love to go fishing.", "We found one of them in the backyard."]}
{"example_id": "134", "dataset": "art", "method": "random_question", "asked": true, "q": "Did We grew fond of this town. happen?", "a": "no", "prior_probs": [0.49344076594076103, 0.5065592340592391], "posterior_probs": [0.5772291439048569, 0.42277085609514303], "prior_entropy": 0.6930611309868531, "posterior_entropy": 0.681170609570151, "delta_entropy": 0.011890521416702104, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5772291439048569, "accuracy": 0, "tokens_in": 142, "tokens_out": 28, "tokens_total": 170, "latency_total": 0.026571482000690594, "latency_per_module": {"scorer": 0.013257373000669759, "prior": 0.013314109000020835}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 14, "tokens_total": 102}, "prior": {"tokens_in": 54, "tokens_out": 14, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: We decided to move to a new town next year. It will be a fun adventure.\nQuestion: Did We grew fond of this town. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: We decided to move to a new town next year. It will be a fun adventure.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "We decided to move to a new town next year. It will be a fun adventure.", "hypotheses": ["We grew fond of this town.", "We got sick of this town."]}
{"example_id": "135", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ryan was mad at Kim for stealing his phone. happen?", "a": "no", "prior_probs": [0.4610257841579939, 0.5389742158420062], "posterior_probs": [0.602280342496119, 0.39771965750388105], "prior_entropy": 0.6901061176167884, "posterior_entropy": 0.672076227528758, "delta_entropy": 0.018029890088030465, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.602280342496119, "accuracy": 0, "tokens_in": 144, "tokens_out": 38, "tokens_total": 182, "latency_total": 0.028330359999927168, "latency_per_module": {"scorer": 0.015042175999951723, "prior": 0.013288183999975445}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 19, "tokens_total": 111}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.\nQuestion: Did Ryan was mad at Kim for stealing his phone. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.", "hypotheses": ["Ryan was mad at Kim for stealing his phone.", "Heather kept the phone away from Ryan."]}
{"example_id": "136", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tracy spends his days scrubbing and cleaning. happen?", "a": "no", "prior_probs": [0.5327502600876581, 0.46724973991234187], "posterior_probs": [0.6044295915605569, 0.3955704084394431], "prior_entropy": 0.6910004849454078, "posterior_entropy": 0.6711746935665772, "delta_entropy": 0.01982579137883056, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6044295915605569, "accuracy": 0, "tokens_in": 186, "tokens_out": 38, "tokens_total": 224, "latency_total": 0.02669584099930944, "latency_per_module": {"scorer": 0.012619922999874689, "prior": 0.01407591799943475}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 19, "tokens_total": 131}, "prior": {"tokens_in": 74, "tokens_out": 19, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.\nQuestion: Did Tracy spends his days scrubbing and cleaning. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.", "hypotheses": ["Tracy spends his days scrubbing and cleaning.", "I asked him how it is so clean."]}
{"example_id": "137", "dataset": "art", "method": "random_question", "asked": true, "q": "Did At first it was hard, but she persevered. happen?", "a": "yes", "prior_probs": [0.3580635427488348, 0.6419364572511652], "posterior_probs": [0.28964902593398345, 0.7103509740660164], "prior_entropy": 0.6522958825764628, "posterior_entropy": 0.6018371268070214, "delta_entropy": 0.050458755769441366, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7103509740660164, "accuracy": 1, "tokens_in": 148, "tokens_out": 40, "tokens_total": 188, "latency_total": 0.02098111000032077, "latency_per_module": {"scorer": 0.011444077000305697, "prior": 0.009537033000015072}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 20, "tokens_total": 116}, "prior": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kya was trying to be vegan. Before long, being vegan was effortless.\nQuestion: Did At first it was hard, but she persevered. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Kya was trying to be vegan. Before long, being vegan was effortless.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kya was trying to be vegan. Before long, being vegan was effortless.", "hypotheses": ["Kya learned many new steak recipes.", "At first it was hard, but she persevered."]}
{"example_id": "138", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The system was very expensive to have installed. happen?", "a": "no", "prior_probs": [0.5818840765131251, 0.4181159234868748], "posterior_probs": [0.6456029362016825, 0.3543970637983174], "prior_entropy": 0.6796765813872114, "posterior_entropy": 0.6501261825380267, "delta_entropy": 0.029550398849184756, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6456029362016825, "accuracy": 0, "tokens_in": 162, "tokens_out": 48, "tokens_total": 210, "latency_total": 0.019234597999457037, "latency_per_module": {"scorer": 0.008834442999614112, "prior": 0.010400154999842925}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 24, "tokens_total": 124}, "prior": {"tokens_in": 62, "tokens_out": 24, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.\nQuestion: Did The system was very expensive to have installed. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.", "hypotheses": ["The system was very expensive to have installed.", "This month, my electric bill was double  what is used to be."]}
{"example_id": "139", "dataset": "art", "method": "random_question", "asked": true, "q": "Did We all had dinner at my tiny house. happen?", "a": "yes", "prior_probs": [0.5041482688845201, 0.4958517311154799], "posterior_probs": [0.4594548880022557, 0.5405451119977444], "prior_entropy": 0.6931127638936314, "posterior_entropy": 0.6898557555968071, "delta_entropy": 0.003257008296824293, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5405451119977444, "accuracy": 1, "tokens_in": 162, "tokens_out": 36, "tokens_total": 198, "latency_total": 0.01483098699918628, "latency_per_module": {"scorer": 0.007365976999608392, "prior": 0.007465009999577887}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 18, "tokens_total": 118}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.\nQuestion: Did We all had dinner at my tiny house. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.", "hypotheses": ["We all had dinner at my big house.", "We all had dinner at my tiny house."]}
{"example_id": "140", "dataset": "art", "method": "random_question", "asked": true, "q": "Did there was a piece of steak on a plate. happen?", "a": "yes", "prior_probs": [0.52242454564882, 0.47757545435118004], "posterior_probs": [0.43497869131203637, 0.5650213086879636], "prior_entropy": 0.6921411226333666, "posterior_entropy": 0.684667644645748, "delta_entropy": 0.007473477987618549, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5650213086879636, "accuracy": 1, "tokens_in": 164, "tokens_out": 38, "tokens_total": 202, "latency_total": 0.014881373000207532, "latency_per_module": {"scorer": 0.007277015999534342, "prior": 0.00760435700067319}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}, "prior": {"tokens_in": 62, "tokens_out": 19, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.\nQuestion: Did there was a piece of steak on a plate. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.", "hypotheses": ["There wasn't any food on a plate.", "there was a piece of steak on a plate."]}
{"example_id": "141", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Helen went to the store. happen?", "a": "no", "prior_probs": [0.5214472455295223, 0.47855275447047774], "posterior_probs": [0.570148339473299, 0.42985166052670093], "prior_entropy": 0.6922269295543793, "posterior_entropy": 0.6832730590304166, "delta_entropy": 0.008953870523962681, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.570148339473299, "accuracy": 0, "tokens_in": 140, "tokens_out": 26, "tokens_total": 166, "latency_total": 0.014790663999519893, "latency_per_module": {"scorer": 0.007721864999439276, "prior": 0.007068799000080617}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 13, "tokens_total": 99}, "prior": {"tokens_in": 54, "tokens_out": 13, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Helen hung up the stocking on the railing. And someone had put presents in her stocking!\nQuestion: Did Helen went to the store. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Helen hung up the stocking on the railing. And someone had put presents in her stocking!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Helen hung up the stocking on the railing. And someone had put presents in her stocking!", "hypotheses": ["Helen went to the store.", "Helen went to sleep."]}
{"example_id": "142", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tim forgot the gift in his car until after her birthday. happen?", "a": "no", "prior_probs": [0.4818382331647717, 0.5181617668352283], "posterior_probs": [0.5948622137362112, 0.4051377862637888], "prior_entropy": 0.6924873358643442, "posterior_entropy": 0.6750399439514677, "delta_entropy": 0.017447391912876475, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5948622137362112, "accuracy": 0, "tokens_in": 196, "tokens_out": 50, "tokens_total": 246, "latency_total": 0.015491920999920694, "latency_per_module": {"scorer": 0.007930898999802594, "prior": 0.0075610220001181006}, "tokens_per_module": {"scorer": {"tokens_in": 120, "tokens_out": 25, "tokens_total": 145}, "prior": {"tokens_in": 76, "tokens_out": 25, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.\nQuestion: Did Tim forgot the gift in his car until after her birthday. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.", "hypotheses": ["Tim forgot the gift in his car until after her birthday.", "Tim realized it would barely make it in time for her birthday."]}
{"example_id": "143", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Greg's lawyer made a compelling defense case. happen?", "a": "yes", "prior_probs": [0.47161095172450157, 0.5283890482754985], "posterior_probs": [0.35290403395556336, 0.6470959660444366], "prior_entropy": 0.6915344372670527, "posterior_entropy": 0.6492258384402443, "delta_entropy": 0.042308598826808375, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6470959660444366, "accuracy": 1, "tokens_in": 118, "tokens_out": 36, "tokens_total": 154, "latency_total": 0.01441812499979278, "latency_per_module": {"scorer": 0.007586599999740429, "prior": 0.006831525000052352}, "tokens_per_module": {"scorer": {"tokens_in": 78, "tokens_out": 18, "tokens_total": 96}, "prior": {"tokens_in": 40, "tokens_out": 18, "tokens_total": 58}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Greg was arrested for manslaughter. So Greg was convicted.\nQuestion: Did Greg's lawyer made a compelling defense case. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Greg was arrested for manslaughter. So Greg was convicted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Greg was arrested for manslaughter. So Greg was convicted.", "hypotheses": ["greg had not called the police right away.", "Greg's lawyer made a compelling defense case."]}
{"example_id": "144", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He made up a a rhyme that included his phone number. happen?", "a": "yes", "prior_probs": [0.44015820877619033, 0.5598417912238096], "posterior_probs": [0.30788121055288925, 0.6921187894471107], "prior_entropy": 0.6859679034141187, "posterior_entropy": 0.6173948730164754, "delta_entropy": 0.06857303039764329, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6921187894471107, "accuracy": 1, "tokens_in": 138, "tokens_out": 42, "tokens_total": 180, "latency_total": 0.014902876000633114, "latency_per_module": {"scorer": 0.007674287000554614, "prior": 0.0072285890000785}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}, "prior": {"tokens_in": 46, "tokens_out": 21, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe could not remember his address. After that he always remembered it.\nQuestion: Did He made up a a rhyme that included his phone number. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Joe could not remember his address. After that he always remembered it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe could not remember his address. After that he always remembered it.", "hypotheses": ["Joe could not have a pizza delivered.", "He made up a a rhyme that included his phone number."]}
{"example_id": "145", "dataset": "art", "method": "random_question", "asked": true, "q": "Did A friend bought Nadia a pair. happen?", "a": "yes", "prior_probs": [0.5366545194477929, 0.46334548055220715], "posterior_probs": [0.4273911146010637, 0.5726088853989363], "prior_entropy": 0.6904576609280619, "posterior_entropy": 0.6825657045439385, "delta_entropy": 0.007891956384123433, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5726088853989363, "accuracy": 1, "tokens_in": 144, "tokens_out": 40, "tokens_total": 184, "latency_total": 0.01532895499985898, "latency_per_module": {"scorer": 0.0079316920000565, "prior": 0.007397262999802479}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nadia needed new ballet shoes. After her first dance show, she paid him back.\nQuestion: Did A friend bought Nadia a pair. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Nadia needed new ballet shoes. After her first dance show, she paid him back.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nadia needed new ballet shoes. After her first dance show, she paid him back.", "hypotheses": ["Nadia's friend gifted her the money for new shoes.", "A friend bought Nadia a pair."]}
{"example_id": "146", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He practiced and tried out for a role. happen?", "a": "yes", "prior_probs": [0.5012941331574995, 0.49870586684250057], "posterior_probs": [0.3776037598086967, 0.6223962401913034], "prior_entropy": 0.6931438309929469, "posterior_entropy": 0.6628788556136773, "delta_entropy": 0.030264975379269532, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6223962401913034, "accuracy": 1, "tokens_in": 170, "tokens_out": 38, "tokens_total": 208, "latency_total": 0.01469677299974137, "latency_per_module": {"scorer": 0.007294612999430683, "prior": 0.007402160000310687}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}, "prior": {"tokens_in": 66, "tokens_out": 19, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.\nQuestion: Did He practiced and tried out for a role. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.", "hypotheses": ["Matthew read the basketball rules and practiced the game.", "He practiced and tried out for a role."]}
{"example_id": "147", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Joseph could tell Mike was playing too over-confidently. happen?", "a": "yes", "prior_probs": [0.5013264386531003, 0.4986735613468997], "posterior_probs": [0.2982723504724022, 0.7017276495275979], "prior_entropy": 0.693143661674817, "posterior_entropy": 0.6093933538774118, "delta_entropy": 0.08375030779740522, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7017276495275979, "accuracy": 1, "tokens_in": 152, "tokens_out": 40, "tokens_total": 192, "latency_total": 0.015273699001227214, "latency_per_module": {"scorer": 0.0076067010004408075, "prior": 0.007666998000786407}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.\nQuestion: Did Joseph could tell Mike was playing too over-confidently. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.", "hypotheses": ["Mike entered a contest partnering with Joseph.", "Joseph could tell Mike was playing too over-confidently."]}
{"example_id": "148", "dataset": "art", "method": "random_question", "asked": true, "q": "Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?", "a": "no", "prior_probs": [0.5719734763411052, 0.42802652365889493], "posterior_probs": [0.7629772617452668, 0.2370227382547333], "prior_entropy": 0.6827507390451305, "posterior_entropy": 0.547623731750972, "delta_entropy": 0.1351270072941585, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7629772617452668, "accuracy": 0, "tokens_in": 154, "tokens_out": 56, "tokens_total": 210, "latency_total": 0.015338436999627447, "latency_per_module": {"scorer": 0.007997037999302847, "prior": 0.0073413990003246}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 28, "tokens_total": 130}, "prior": {"tokens_in": 52, "tokens_out": 28, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.\nQuestion: Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.", "hypotheses": ["All flights were grounded to Lisa couldn't leave for a couple of days.", "Lucy shared supplies in art class with Lisa, they bonded."]}
{"example_id": "149", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Martha's boyfriend enrolled himself in cooking classes. happen?", "a": "no", "prior_probs": [0.5042689001377829, 0.4957310998622171], "posterior_probs": [0.6241150157597921, 0.37588498424020794], "prior_entropy": 0.6931107330983648, "posterior_entropy": 0.6620136404865824, "delta_entropy": 0.031097092611782395, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6241150157597921, "accuracy": 0, "tokens_in": 158, "tokens_out": 40, "tokens_total": 198, "latency_total": 0.020378176999656716, "latency_per_module": {"scorer": 0.010988082999574544, "prior": 0.009390094000082172}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.\nQuestion: Did Martha's boyfriend enrolled himself in cooking classes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.", "hypotheses": ["Martha's boyfriend enrolled himself in cooking classes.", "Martha worked hard to learn some recipies."]}
{"example_id": "150", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Her husband did not expect Lucy to be home yet. happen?", "a": "no", "prior_probs": [0.5310802956494481, 0.4689197043505518], "posterior_probs": [0.6117678949259431, 0.388232105074057], "prior_entropy": 0.6912139649068965, "posterior_entropy": 0.6679507134155024, "delta_entropy": 0.023263251491394077, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6117678949259431, "accuracy": 0, "tokens_in": 146, "tokens_out": 46, "tokens_total": 192, "latency_total": 0.016550096000173653, "latency_per_module": {"scorer": 0.007275318000210973, "prior": 0.00927477799996268}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 23, "tokens_total": 117}, "prior": {"tokens_in": 52, "tokens_out": 23, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.\nQuestion: Did Her husband did not expect Lucy to be home yet. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.", "hypotheses": ["Her husband did not expect Lucy to be home yet.", "she couldn't wait to tell him she wanted a divorce."]}
{"example_id": "151", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kelsi and Thomas met at school. happen?", "a": "no", "prior_probs": [0.5187758093381991, 0.4812241906618008], "posterior_probs": [0.5830028797887451, 0.41699712021125485], "prior_entropy": 0.692441952727644, "posterior_entropy": 0.6793042298106573, "delta_entropy": 0.013137722916986672, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5830028797887451, "accuracy": 0, "tokens_in": 182, "tokens_out": 44, "tokens_total": 226, "latency_total": 0.015051998999297211, "latency_per_module": {"scorer": 0.007253810999827692, "prior": 0.007798187999469519}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 22, "tokens_total": 132}, "prior": {"tokens_in": 72, "tokens_out": 22, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.\nQuestion: Did Kelsi and Thomas met at school. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.", "hypotheses": ["Kelsi and Thomas met at school.", "Kelsi saw Lucy was reading the same book she was."]}
{"example_id": "152", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I bought my dream cat. happen?", "a": "yes", "prior_probs": [0.5489156641466169, 0.45108433585338314], "posterior_probs": [0.4345953322906415, 0.5654046677093585], "prior_entropy": 0.6883540331646313, "posterior_entropy": 0.6845670718017357, "delta_entropy": 0.00378696136289558, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5654046677093585, "accuracy": 1, "tokens_in": 152, "tokens_out": 36, "tokens_total": 188, "latency_total": 0.013722437000069476, "latency_per_module": {"scorer": 0.006873968999570934, "prior": 0.006848468000498542}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}, "prior": {"tokens_in": 60, "tokens_out": 18, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I saved up money for a long time. I took the boat out on the lake and felt happy.\nQuestion: Did I bought my dream cat. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I saved up money for a long time. I took the boat out on the lake and felt happy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I saved up money for a long time. I took the boat out on the lake and felt happy.", "hypotheses": ["i had enough money to spend on food and extra stuff.", "I bought my dream cat."]}
{"example_id": "153", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jake immediately threw his new toy in the trash. happen?", "a": "no", "prior_probs": [0.5717033204360721, 0.428296679563928], "posterior_probs": [0.6419857770755857, 0.35801422292441437], "prior_entropy": 0.6828289102070211, "posterior_entropy": 0.6522670854142874, "delta_entropy": 0.030561824792733727, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6419857770755857, "accuracy": 0, "tokens_in": 184, "tokens_out": 36, "tokens_total": 220, "latency_total": 0.013424755999949411, "latency_per_module": {"scorer": 0.006665996999799972, "prior": 0.006758759000149439}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 18, "tokens_total": 130}, "prior": {"tokens_in": 72, "tokens_out": 18, "tokens_total": 90}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string\nQuestion: Did Jake immediately threw his new toy in the trash. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string", "hypotheses": ["Jake immediately threw his new toy in the trash.", "Jake decided Dan was the Green Goblin."]}
{"example_id": "154", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Margo sat down to watch. happen?", "a": "no", "prior_probs": [0.46568290051530103, 0.534317099484699], "posterior_probs": [0.5465878384641614, 0.45341216153583863], "prior_entropy": 0.6907900012428367, "posterior_entropy": 0.6888000242557888, "delta_entropy": 0.001989976987047948, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5465878384641614, "accuracy": 0, "tokens_in": 142, "tokens_out": 26, "tokens_total": 168, "latency_total": 0.013380012998823076, "latency_per_module": {"scorer": 0.006984674999330309, "prior": 0.006395337999492767}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 13, "tokens_total": 101}, "prior": {"tokens_in": 54, "tokens_out": 13, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.\nQuestion: Did Margo sat down to watch. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.", "hypotheses": ["Margo sat down to watch.", "Margo left the show."]}
{"example_id": "155", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Barry's team won the game today. happen?", "a": "no", "prior_probs": [0.4269841303580189, 0.573015869641981], "posterior_probs": [0.5814656780904943, 0.4185343219095057], "prior_entropy": 0.6824463218505652, "posterior_entropy": 0.6798145075761572, "delta_entropy": 0.002631814274407951, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5814656780904943, "accuracy": 0, "tokens_in": 120, "tokens_out": 38, "tokens_total": 158, "latency_total": 0.013829327000166813, "latency_per_module": {"scorer": 0.0073645610000312445, "prior": 0.006464766000135569}, "tokens_per_module": {"scorer": {"tokens_in": 78, "tokens_out": 19, "tokens_total": 97}, "prior": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Barry loves playing baseball. Barry also bought a hot dog.\nQuestion: Did Barry's team won the game today. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Barry loves playing baseball. Barry also bought a hot dog.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Barry loves playing baseball. Barry also bought a hot dog.", "hypotheses": ["Barry's team won the game today.", "Barry went to the bar for a game."]}
{"example_id": "156", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Erina gave it her all and did well. happen?", "a": "no", "prior_probs": [0.49943053747004224, 0.5005694625299577], "posterior_probs": [0.5920209925891655, 0.4079790074108345], "prior_entropy": 0.6931465319826591, "posterior_entropy": 0.6761145287895747, "delta_entropy": 0.0170320031930844, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5920209925891655, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.013862138000149571, "latency_per_module": {"scorer": 0.0069028070001877495, "prior": 0.006959330999961821}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Erina's first day at her new job was today. Her new boss complimented her on her performance.\nQuestion: Did Erina gave it her all and did well. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Erina's first day at her new job was today. Her new boss complimented her on her performance.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erina's first day at her new job was today. Her new boss complimented her on her performance.", "hypotheses": ["Erina gave it her all and did well.", "Erina took too many breaks the first day."]}
{"example_id": "157", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Penny dropped her game on accident. happen?", "a": "no", "prior_probs": [0.5264368049007686, 0.4735631950992314], "posterior_probs": [0.5944174596215034, 0.40558254037849656], "prior_entropy": 0.6917487192315404, "posterior_entropy": 0.6752103648191494, "delta_entropy": 0.016538354412390932, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5944174596215034, "accuracy": 0, "tokens_in": 174, "tokens_out": 30, "tokens_total": 204, "latency_total": 0.014309676000266336, "latency_per_module": {"scorer": 0.0072229170000355225, "prior": 0.007086759000230813}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 15, "tokens_total": 119}, "prior": {"tokens_in": 70, "tokens_out": 15, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.\nQuestion: Did Penny dropped her game on accident. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.", "hypotheses": ["Penny dropped her game on accident.", "She accidentally sold her game console."]}
{"example_id": "158", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?", "a": "no", "prior_probs": [0.5403909182322422, 0.45960908176775783], "posterior_probs": [0.6379185995365564, 0.36208140046344356], "prior_entropy": 0.6898807699753513, "posterior_entropy": 0.6546063643409491, "delta_entropy": 0.035274405634402206, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6379185995365564, "accuracy": 0, "tokens_in": 170, "tokens_out": 50, "tokens_total": 220, "latency_total": 0.016410662999987835, "latency_per_module": {"scorer": 0.009067623999726493, "prior": 0.007343039000261342}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 25, "tokens_total": 133}, "prior": {"tokens_in": 62, "tokens_out": 25, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.\nQuestion: Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.", "hypotheses": ["Amy robbed the frozen yogurt store and ate all of the yogurt.", "Amy was called back to work and her frozen yogurt melted."]}
{"example_id": "159", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ellen realized that she could. happen?", "a": "no", "prior_probs": [0.5027835081552867, 0.4972164918447132], "posterior_probs": [0.5437743636785437, 0.4562256363214563], "prior_entropy": 0.693131684642603, "posterior_entropy": 0.6893098799179234, "delta_entropy": 0.003821804724679634, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5437743636785437, "accuracy": 0, "tokens_in": 172, "tokens_out": 34, "tokens_total": 206, "latency_total": 0.02886749899971619, "latency_per_module": {"scorer": 0.014742385000317881, "prior": 0.014125113999398309}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 17, "tokens_total": 119}, "prior": {"tokens_in": 70, "tokens_out": 17, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.\nQuestion: Did Ellen realized that she could. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.", "hypotheses": ["Ellen realized that she could.", "Ellen was told there was a dress code."]}
{"example_id": "160", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Lily decided to make her costume a bear costume. happen?", "a": "no", "prior_probs": [0.6198132078037958, 0.3801867921962042], "posterior_probs": [0.6630818066285097, 0.3369181933714903], "prior_entropy": 0.664155496307155, "posterior_entropy": 0.6389701400853475, "delta_entropy": 0.02518535622180751, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6630818066285097, "accuracy": 0, "tokens_in": 136, "tokens_out": 36, "tokens_total": 172, "latency_total": 0.02887296700009756, "latency_per_module": {"scorer": 0.01388676199985639, "prior": 0.01498620500024117}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 18, "tokens_total": 106}, "prior": {"tokens_in": 48, "tokens_out": 18, "tokens_total": 66}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lily wanted a new Halloween costume. She ended up making a rabbit costume.\nQuestion: Did Lily decided to make her costume a bear costume. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lily wanted a new Halloween costume. She ended up making a rabbit costume.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lily wanted a new Halloween costume. She ended up making a rabbit costume.", "hypotheses": ["Lily decided to make her costume a bear costume.", "All the costumes were gone though."]}
{"example_id": "161", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She studied hard because she wanted to spell. happen?", "a": "no", "prior_probs": [0.464782121161704, 0.5352178788382961], "posterior_probs": [0.5822040033742816, 0.4177959966257184], "prior_entropy": 0.6906645273748359, "posterior_entropy": 0.6795706311899048, "delta_entropy": 0.011093896184931062, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5822040033742816, "accuracy": 0, "tokens_in": 174, "tokens_out": 40, "tokens_total": 214, "latency_total": 0.02648748099909426, "latency_per_module": {"scorer": 0.014488650999737729, "prior": 0.01199882999935653}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 20, "tokens_total": 126}, "prior": {"tokens_in": 68, "tokens_out": 20, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.\nQuestion: Did She studied hard because she wanted to spell. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.", "hypotheses": ["She studied hard because she wanted to spell.", "Mia entered the spelling bee but didn't practice."]}
{"example_id": "162", "dataset": "art", "method": "random_question", "asked": true, "q": "Did bus showed up early and I was late for class. happen?", "a": "yes", "prior_probs": [0.49817956298321636, 0.5018204370167837], "posterior_probs": [0.3826755227339434, 0.6173244772660567], "prior_entropy": 0.6931405525614377, "posterior_entropy": 0.665358745777568, "delta_entropy": 0.027781806783869656, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6173244772660567, "accuracy": 1, "tokens_in": 174, "tokens_out": 44, "tokens_total": 218, "latency_total": 0.022297498000625637, "latency_per_module": {"scorer": 0.011774600000535429, "prior": 0.010522898000090208}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 22, "tokens_total": 130}, "prior": {"tokens_in": 66, "tokens_out": 22, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.\nQuestion: Did bus showed up early and I was late for class. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.", "hypotheses": ["The bus was late and so was I to school.", "bus showed up early and I was late for class."]}
{"example_id": "163", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jean booked her trip and went. happen?", "a": "no", "prior_probs": [0.37469656598213685, 0.6253034340178631], "posterior_probs": [0.5065690075362256, 0.49343099246377436], "prior_entropy": 0.6614080398426275, "posterior_entropy": 0.6930608743549747, "delta_entropy": -0.03165283451234713, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5065690075362256, "accuracy": 0, "tokens_in": 146, "tokens_out": 34, "tokens_total": 180, "latency_total": 0.02306629299891938, "latency_per_module": {"scorer": 0.01143332899937377, "prior": 0.011632963999545609}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 17, "tokens_total": 107}, "prior": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.\nQuestion: Did Jean booked her trip and went. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.", "hypotheses": ["Jean booked her trip and went.", "Jean ended up having a bad time in Africa."]}
{"example_id": "164", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Mary ended up overcooking the pasta. happen?", "a": "no", "prior_probs": [0.5138874089170941, 0.486112591082906], "posterior_probs": [0.6518212145747361, 0.34817878542526387], "prior_entropy": 0.6927614106964033, "posterior_entropy": 0.6463119402236601, "delta_entropy": 0.04644947047274317, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6518212145747361, "accuracy": 0, "tokens_in": 134, "tokens_out": 48, "tokens_total": 182, "latency_total": 0.02398493199962104, "latency_per_module": {"scorer": 0.014458118000220566, "prior": 0.009526813999400474}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 24, "tokens_total": 110}, "prior": {"tokens_in": 48, "tokens_out": 24, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary had never made rice before. She resolved to read directions next time!\nQuestion: Did Mary ended up overcooking the pasta. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mary had never made rice before. She resolved to read directions next time!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary had never made rice before. She resolved to read directions next time!", "hypotheses": ["Mary ended up overcooking the pasta.", "The rice Mary put on the stove out over and burned on the stove."]}
{"example_id": "165", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He was in such a slow mood, he didn't dress correctly. happen?", "a": "yes", "prior_probs": [0.45114413817439564, 0.5488558618256044], "posterior_probs": [0.3193603533066797, 0.6806396466933202], "prior_entropy": 0.6883657645700085, "posterior_entropy": 0.6263863694597975, "delta_entropy": 0.061979395110211, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6806396466933202, "accuracy": 1, "tokens_in": 164, "tokens_out": 38, "tokens_total": 202, "latency_total": 0.022191649999513174, "latency_per_module": {"scorer": 0.010782710000057705, "prior": 0.01140893999945547}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 19, "tokens_total": 125}, "prior": {"tokens_in": 58, "tokens_out": 19, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.\nQuestion: Did He was in such a slow mood, he didn't dress correctly. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.", "hypotheses": ["Brad rushed to work.", "He was in such a slow mood, he didn't dress correctly."]}
{"example_id": "166", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I heard loud fire alarm. happen?", "a": "yes", "prior_probs": [0.5975131689009477, 0.40248683109905226], "posterior_probs": [0.44539684342247626, 0.5546031565775237], "prior_entropy": 0.6740071150648055, "posterior_entropy": 0.6871722617425527, "delta_entropy": -0.013165146677747264, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5546031565775237, "accuracy": 1, "tokens_in": 160, "tokens_out": 32, "tokens_total": 192, "latency_total": 0.024136017999808246, "latency_per_module": {"scorer": 0.012853463000283227, "prior": 0.011282554999525019}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}, "prior": {"tokens_in": 64, "tokens_out": 16, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.\nQuestion: Did I heard loud fire alarm. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.", "hypotheses": ["I left all my stuff out on the bed.", "I heard loud fire alarm."]}
{"example_id": "167", "dataset": "art", "method": "random_question", "asked": true, "q": "Did My shoes made a really loud sound in front of my boss. happen?", "a": "no", "prior_probs": [0.5050753995895266, 0.49492460041047337], "posterior_probs": [0.6468762456464059, 0.35312375435359417], "prior_entropy": 0.693095660311174, "posterior_entropy": 0.6493589488836923, "delta_entropy": 0.0437367114274817, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6468762456464059, "accuracy": 0, "tokens_in": 170, "tokens_out": 40, "tokens_total": 210, "latency_total": 0.020852135999120947, "latency_per_module": {"scorer": 0.010362549999626935, "prior": 0.010489585999494011}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 20, "tokens_total": 128}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: For a lark I started dragging my foot behind me at work. He told me to knock it off.\nQuestion: Did My shoes made a really loud sound in front of my boss. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: For a lark I started dragging my foot behind me at work. He told me to knock it off.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "For a lark I started dragging my foot behind me at work. He told me to knock it off.", "hypotheses": ["My shoes made a really loud sound in front of my boss.", "I fell and broke my leg."]}
{"example_id": "168", "dataset": "art", "method": "random_question", "asked": true, "q": "Did However, he was not very talented, and his hair did not sell very well. happen?", "a": "no", "prior_probs": [0.6146910714067012, 0.3853089285932988], "posterior_probs": [0.7134848862873541, 0.286515113712646], "prior_entropy": 0.666603394708587, "posterior_entropy": 0.5990018094877949, "delta_entropy": 0.06760158522079207, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7134848862873541, "accuracy": 0, "tokens_in": 154, "tokens_out": 60, "tokens_total": 214, "latency_total": 0.014132583999526105, "latency_per_module": {"scorer": 0.007321676000174193, "prior": 0.006810907999351912}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 30, "tokens_total": 134}, "prior": {"tokens_in": 50, "tokens_out": 30, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tony liked art. Tony then went back to school and found a different major.\nQuestion: Did However, he was not very talented, and his hair did not sell very well. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tony liked art. Tony then went back to school and found a different major.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tony liked art. Tony then went back to school and found a different major.", "hypotheses": ["However, he was not very talented, and his hair did not sell very well.", "Tony applies for a lot of art gallery jobs and got rejected."]}
{"example_id": "169", "dataset": "art", "method": "random_question", "asked": true, "q": "Did I returned the empty envelope to the person it was addressed to. happen?", "a": "yes", "prior_probs": [0.4112140779915971, 0.5887859220084029], "posterior_probs": [0.3200503300324284, 0.6799496699675716], "prior_entropy": 0.6772973833177964, "posterior_entropy": 0.6269073891092749, "delta_entropy": 0.05038999420852153, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6799496699675716, "accuracy": 1, "tokens_in": 146, "tokens_out": 46, "tokens_total": 192, "latency_total": 0.014207506000275316, "latency_per_module": {"scorer": 0.006986698999753571, "prior": 0.007220807000521745}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 23, "tokens_total": 119}, "prior": {"tokens_in": 50, "tokens_out": 23, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I found a $600 dollar envelope in the mail today. I am honest.\nQuestion: Did I returned the empty envelope to the person it was addressed to. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I found a $600 dollar envelope in the mail today. I am honest.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I found a $600 dollar envelope in the mail today. I am honest.", "hypotheses": ["I turned the $600 dollars to the authorities.", "I returned the empty envelope to the person it was addressed to."]}
{"example_id": "170", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Dominick moved his house away from Becky's house. happen?", "a": "no", "prior_probs": [0.6687183408185025, 0.3312816591814976], "posterior_probs": [0.684078457676746, 0.31592154232325403], "prior_entropy": 0.6350825753029137, "posterior_entropy": 0.6237569233425315, "delta_entropy": 0.011325651960382155, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.684078457676746, "accuracy": 0, "tokens_in": 154, "tokens_out": 44, "tokens_total": 198, "latency_total": 0.013529001999813772, "latency_per_module": {"scorer": 0.006632886999796028, "prior": 0.006896115000017744}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}, "prior": {"tokens_in": 56, "tokens_out": 22, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.\nQuestion: Did Dominick moved his house away from Becky's house. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.", "hypotheses": ["Dominick moved his house away from Becky's house.", "Dominick shifted their house near the Becky house so."]}
{"example_id": "171", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tom was unable to find work being in a wheelchair. happen?", "a": "yes", "prior_probs": [0.3802296600706219, 0.619770339929378], "posterior_probs": [0.28695952264025276, 0.7130404773597473], "prior_entropy": 0.6641764443158823, "posterior_entropy": 0.5994067919330996, "delta_entropy": 0.06476965238278265, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7130404773597473, "accuracy": 1, "tokens_in": 146, "tokens_out": 38, "tokens_total": 184, "latency_total": 0.013252931999886641, "latency_per_module": {"scorer": 0.006777939000130573, "prior": 0.0064749929997560685}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 19, "tokens_total": 113}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was accidentally shot by his teammate in the army. He ends up being homeless.\nQuestion: Did Tom was unable to find work being in a wheelchair. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tom was accidentally shot by his teammate in the army. He ends up being homeless.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was accidentally shot by his teammate in the army. He ends up being homeless.", "hypotheses": ["Tom developed emotional problems affecting his division.", "Tom was unable to find work being in a wheelchair."]}
{"example_id": "172", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tommy's friends didn't pay attention to him. happen?", "a": "yes", "prior_probs": [0.4065824365907642, 0.5934175634092357], "posterior_probs": [0.37690013254883376, 0.6230998674511662], "prior_entropy": 0.6755905096228538, "posterior_entropy": 0.6625261773213507, "delta_entropy": 0.013064332301503168, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6230998674511662, "accuracy": 1, "tokens_in": 124, "tokens_out": 38, "tokens_total": 162, "latency_total": 0.01753561199984688, "latency_per_module": {"scorer": 0.010436503000164521, "prior": 0.0070991089996823575}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 19, "tokens_total": 101}, "prior": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tommy was having a bad day. Tommy had good friends.\nQuestion: Did Tommy's friends didn't pay attention to him. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Tommy was having a bad day. Tommy had good friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tommy was having a bad day. Tommy had good friends.", "hypotheses": ["Tommy's friends cheered him up.", "Tommy's friends didn't pay attention to him."]}
{"example_id": "173", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jay wasn't sure how to vote. happen?", "a": "no", "prior_probs": [0.5449985742051293, 0.45500142579487063], "posterior_probs": [0.5733131693796836, 0.4266868306203164], "prior_entropy": 0.689091952601288, "posterior_entropy": 0.6823586855711019, "delta_entropy": 0.006733267030186063, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5733131693796836, "accuracy": 0, "tokens_in": 140, "tokens_out": 32, "tokens_total": 172, "latency_total": 0.016530856999452226, "latency_per_module": {"scorer": 0.0073730929998419015, "prior": 0.009157763999610324}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}, "prior": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jay wanted to vote. That is, until he got older and did it again.\nQuestion: Did Jay wasn't sure how to vote. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jay wanted to vote. That is, until he got older and did it again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jay wanted to vote. That is, until he got older and did it again.", "hypotheses": ["Jay wasn't sure how to vote.", "Jay swore he'd never vote again."]}
{"example_id": "174", "dataset": "art", "method": "random_question", "asked": true, "q": "Did My date threw up on me. happen?", "a": "no", "prior_probs": [0.484791977761282, 0.515208022238718], "posterior_probs": [0.5226265521931132, 0.47737344780688684], "prior_entropy": 0.6926845413276974, "posterior_entropy": 0.6921229090711682, "delta_entropy": 0.0005616322565291654, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5226265521931132, "accuracy": 0, "tokens_in": 154, "tokens_out": 28, "tokens_total": 182, "latency_total": 0.02502257699961774, "latency_per_module": {"scorer": 0.012968279999768129, "prior": 0.012054296999849612}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 14, "tokens_total": 108}, "prior": {"tokens_in": 60, "tokens_out": 14, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was really nervous before my first middle school dance. Now, she won't even talk to me.\nQuestion: Did My date threw up on me. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was really nervous before my first middle school dance. Now, she won't even talk to me.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was really nervous before my first middle school dance. Now, she won't even talk to me.", "hypotheses": ["My date threw up on me.", "I threw up on my date."]}
{"example_id": "175", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?", "a": "no", "prior_probs": [0.4577060736616809, 0.5422939263383191], "posterior_probs": [0.7175104258259337, 0.2824895741740662], "prior_entropy": 0.6895653495966495, "posterior_entropy": 0.5952892803901451, "delta_entropy": 0.09427606920650444, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7175104258259337, "accuracy": 0, "tokens_in": 186, "tokens_out": 64, "tokens_total": 250, "latency_total": 0.031679181000072276, "latency_per_module": {"scorer": 0.01491447100033838, "prior": 0.016764709999733896}, "tokens_per_module": {"scorer": {"tokens_in": 124, "tokens_out": 32, "tokens_total": 156}, "prior": {"tokens_in": 62, "tokens_out": 32, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.\nQuestion: Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.", "hypotheses": ["Today I was ready for him. When he came into our room a jumped out and tickled him.", "He would jump on our ear to get our attention."]}
{"example_id": "176", "dataset": "art", "method": "random_question", "asked": true, "q": "Did My husband was sad so I thought I would cheer her up. happen?", "a": "yes", "prior_probs": [0.39948043219530643, 0.6005195678046935], "posterior_probs": [0.3362910873785936, 0.6637089126214064], "prior_entropy": 0.6728004379125668, "posterior_entropy": 0.6385446723917367, "delta_entropy": 0.034255765520830095, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6637089126214064, "accuracy": 1, "tokens_in": 158, "tokens_out": 38, "tokens_total": 196, "latency_total": 0.02816521400018246, "latency_per_module": {"scorer": 0.013570240999797534, "prior": 0.014594973000384925}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}, "prior": {"tokens_in": 56, "tokens_out": 19, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I loved to make my wife laugh. I started making her laugh again and she became happy!\nQuestion: Did My husband was sad so I thought I would cheer her up. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: I loved to make my wife laugh. I started making her laugh again and she became happy!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I loved to make my wife laugh. I started making her laugh again and she became happy!", "hypotheses": ["The my wife became sad.", "My husband was sad so I thought I would cheer her up."]}
{"example_id": "177", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She cut her finger when chopping vegetables. happen?", "a": "yes", "prior_probs": [0.5202507998250527, 0.4797492001749473], "posterior_probs": [0.37297043624525356, 0.6270295637547465], "prior_entropy": 0.692326766386478, "posterior_entropy": 0.6605176911357957, "delta_entropy": 0.03180907525068233, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6270295637547465, "accuracy": 1, "tokens_in": 140, "tokens_out": 26, "tokens_total": 166, "latency_total": 0.025957343999834848, "latency_per_module": {"scorer": 0.012338376000116114, "prior": 0.013618967999718734}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 13, "tokens_total": 101}, "prior": {"tokens_in": 52, "tokens_out": 13, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Rachel was cooking dinner. Then she pulled herself together and took care of the cut.\nQuestion: Did She cut her finger when chopping vegetables. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Rachel was cooking dinner. Then she pulled herself together and took care of the cut.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Rachel was cooking dinner. Then she pulled herself together and took care of the cut.", "hypotheses": ["Rachel burned her hand.", "She cut her finger when chopping vegetables."]}
{"example_id": "178", "dataset": "art", "method": "random_question", "asked": true, "q": "Did She decided to go out and buy and entire outfit. happen?", "a": "yes", "prior_probs": [0.5855762530292931, 0.414423746970707], "posterior_probs": [0.3591307900119909, 0.640869209988009], "prior_entropy": 0.6784282315920453, "posterior_entropy": 0.6529164423553266, "delta_entropy": 0.025511789236718707, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.640869209988009, "accuracy": 1, "tokens_in": 130, "tokens_out": 40, "tokens_total": 170, "latency_total": 0.028149386000222876, "latency_per_module": {"scorer": 0.014865672999803792, "prior": 0.013283713000419084}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 20, "tokens_total": 106}, "prior": {"tokens_in": 44, "tokens_out": 20, "tokens_total": 64}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam loved striped clothes. She began to wear stripes every day!\nQuestion: Did She decided to go out and buy and entire outfit. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Sam loved striped clothes. She began to wear stripes every day!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam loved striped clothes. She began to wear stripes every day!", "hypotheses": ["Sam got lots of complaints about her clothing.", "She decided to go out and buy and entire outfit."]}
{"example_id": "179", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jimmy talked on the phone and wouldn't stop, delighting the professor. happen?", "a": "yes", "prior_probs": [0.5009149302761192, 0.49908506972388084], "posterior_probs": [0.3412686163823664, 0.6587313836176336], "prior_entropy": 0.6931455063621907, "posterior_entropy": 0.6418733599665647, "delta_entropy": 0.05127214639562605, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6587313836176336, "accuracy": 1, "tokens_in": 174, "tokens_out": 44, "tokens_total": 218, "latency_total": 0.027840735999234312, "latency_per_module": {"scorer": 0.014244273999793222, "prior": 0.01359646199944109}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 22, "tokens_total": 134}, "prior": {"tokens_in": 62, "tokens_out": 22, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jimmy's phone ringed in class. As a result, she called the security guard to take him away.\nQuestion: Did Jimmy talked on the phone and wouldn't stop, delighting the professor. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Jimmy's phone ringed in class. As a result, she called the security guard to take him away.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jimmy's phone ringed in class. As a result, she called the security guard to take him away.", "hypotheses": ["He ended up getting in trouble.", "Jimmy talked on the phone and wouldn't stop, delighting the professor."]}
{"example_id": "180", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Kelly's dinner was tasty. happen?", "a": "yes", "prior_probs": [0.457286767228584, 0.542713232771416], "posterior_probs": [0.4226017864107096, 0.5773982135892903], "prior_entropy": 0.6894938890322678, "posterior_entropy": 0.6811179011809236, "delta_entropy": 0.008375987851344191, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5773982135892903, "accuracy": 1, "tokens_in": 156, "tokens_out": 36, "tokens_total": 192, "latency_total": 0.02912298799947166, "latency_per_module": {"scorer": 0.014107446999332751, "prior": 0.01501554100013891}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.\nQuestion: Did Kelly's dinner was tasty. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.", "hypotheses": ["She severely undercooked the chicken and badly burned the potatoes.", "Kelly's dinner was tasty."]}
{"example_id": "181", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Amber talked to her academic advisor. happen?", "a": "no", "prior_probs": [0.5152120120544086, 0.4847879879455914], "posterior_probs": [0.5538230921646585, 0.4461769078353414], "prior_entropy": 0.6926842985121199, "posterior_entropy": 0.6873420883008954, "delta_entropy": 0.005342210211224496, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5538230921646585, "accuracy": 0, "tokens_in": 134, "tokens_out": 32, "tokens_total": 166, "latency_total": 0.03154763499969704, "latency_per_module": {"scorer": 0.01664301699929638, "prior": 0.01490461800040066}, "tokens_per_module": {"scorer": {"tokens_in": 84, "tokens_out": 16, "tokens_total": 100}, "prior": {"tokens_in": 50, "tokens_out": 16, "tokens_total": 66}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amber was scared about her future. She was no longer worried about her future.\nQuestion: Did Amber talked to her academic advisor. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amber was scared about her future. She was no longer worried about her future.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber was scared about her future. She was no longer worried about her future.", "hypotheses": ["Amber talked to her academic advisor.", "Amber talked to her psychic advisor."]}
{"example_id": "182", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Terry practiced for a long time. happen?", "a": "no", "prior_probs": [0.5951790614049827, 0.40482093859501733], "posterior_probs": [0.6306803666696139, 0.36931963333038603], "prior_entropy": 0.6749180336440842, "posterior_entropy": 0.6585925849294575, "delta_entropy": 0.01632544871462671, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6306803666696139, "accuracy": 0, "tokens_in": 158, "tokens_out": 26, "tokens_total": 184, "latency_total": 0.03232012900116388, "latency_per_module": {"scorer": 0.014931315000467293, "prior": 0.017388814000696584}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 13, "tokens_total": 109}, "prior": {"tokens_in": 62, "tokens_out": 13, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.\nQuestion: Did Terry practiced for a long time. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.", "hypotheses": ["Terry practiced for a long time.", "terry was so big."]}
{"example_id": "183", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Sara was outside looking at the neighbor's dog. happen?", "a": "no", "prior_probs": [0.5548732862741387, 0.4451267137258613], "posterior_probs": [0.5708015972471827, 0.42919840275281723], "prior_entropy": 0.6871128780618596, "posterior_entropy": 0.6830876714556835, "delta_entropy": 0.004025206606176113, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5708015972471827, "accuracy": 0, "tokens_in": 160, "tokens_out": 44, "tokens_total": 204, "latency_total": 0.03026212700024189, "latency_per_module": {"scorer": 0.01550853000026109, "prior": 0.0147535969999808}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 22, "tokens_total": 122}, "prior": {"tokens_in": 60, "tokens_out": 22, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.\nQuestion: Did Sara was outside looking at the neighbor's dog. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.", "hypotheses": ["Sara was outside looking at the neighbor's dog.", "Sara was outside looking at the neighbor's underwear."]}
{"example_id": "184", "dataset": "art", "method": "random_question", "asked": true, "q": "Did The librarian noticed Monica and her boyfriend were just hanging out reading. happen?", "a": "yes", "prior_probs": [0.3762754695839405, 0.6237245304160595], "posterior_probs": [0.2777498203381742, 0.7222501796618258], "prior_entropy": 0.6622113117156057, "posterior_entropy": 0.5908155306719938, "delta_entropy": 0.07139578104361188, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7222501796618258, "accuracy": 1, "tokens_in": 164, "tokens_out": 54, "tokens_total": 218, "latency_total": 0.031116203000237874, "latency_per_module": {"scorer": 0.01646963900020637, "prior": 0.014646564000031503}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 27, "tokens_total": 133}, "prior": {"tokens_in": 58, "tokens_out": 27, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Monica was at the library with her boyfriend. She kicked them out because they were loitering.\nQuestion: Did The librarian noticed Monica and her boyfriend were just hanging out reading. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Monica was at the library with her boyfriend. She kicked them out because they were loitering.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Monica was at the library with her boyfriend. She kicked them out because they were loitering.", "hypotheses": ["Children brought food and left their trash laying on the reading table.", "The librarian noticed Monica and her boyfriend were just hanging out reading."]}
{"example_id": "185", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jen's mother let her put on her own makeup but Jen's friends complimented her. happen?", "a": "yes", "prior_probs": [0.4431772394760715, 0.5568227605239285], "posterior_probs": [0.32102118613651204, 0.6789788138634879], "prior_entropy": 0.6866755555945623, "posterior_entropy": 0.6276368040432336, "delta_entropy": 0.05903875155132865, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6789788138634879, "accuracy": 1, "tokens_in": 160, "tokens_out": 60, "tokens_total": 220, "latency_total": 0.030938507000428217, "latency_per_module": {"scorer": 0.016337648999979137, "prior": 0.01460085800044908}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 30, "tokens_total": 138}, "prior": {"tokens_in": 52, "tokens_out": 30, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.\nQuestion: Did Jen's mother let her put on her own makeup but Jen's friends complimented her. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.", "hypotheses": ["Jen applied her mother's makeup and looked like a clown.", "Jen's mother let her put on her own makeup but Jen's friends complimented her."]}
{"example_id": "186", "dataset": "art", "method": "random_question", "asked": true, "q": "Did He would run outside but nobody would be there. happen?", "a": "no", "prior_probs": [0.42914630227747175, 0.5708536977225283], "posterior_probs": [0.5754890564148673, 0.4245109435851328], "prior_entropy": 0.6830728109033353, "posterior_entropy": 0.6817062869172263, "delta_entropy": 0.0013665239861089251, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5754890564148673, "accuracy": 0, "tokens_in": 168, "tokens_out": 64, "tokens_total": 232, "latency_total": 0.030713694000951364, "latency_per_module": {"scorer": 0.01519613200071035, "prior": 0.015517562000241014}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 32, "tokens_total": 136}, "prior": {"tokens_in": 64, "tokens_out": 32, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.\nQuestion: Did He would run outside but nobody would be there. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.", "hypotheses": ["He would run outside but nobody would be there.", "Ted got sick of being woken up, so he stayed up all night to moved out of his house."]}
{"example_id": "187", "dataset": "art", "method": "random_question", "asked": true, "q": "Did They saw the monster themselves. happen?", "a": "no", "prior_probs": [0.4651342558008747, 0.5348657441991254], "posterior_probs": [0.5139703141935946, 0.48602968580640526], "prior_entropy": 0.6907139661688809, "posterior_entropy": 0.6927567903964722, "delta_entropy": -0.002042824227591278, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5139703141935946, "accuracy": 0, "tokens_in": 176, "tokens_out": 30, "tokens_total": 206, "latency_total": 0.030991693000032683, "latency_per_module": {"scorer": 0.015262640000401007, "prior": 0.015729052999631676}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 15, "tokens_total": 119}, "prior": {"tokens_in": 72, "tokens_out": 15, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!\nQuestion: Did They saw the monster themselves. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!", "hypotheses": ["They saw the monster themselves.", "Neil asked the locals where to find it."]}
{"example_id": "188", "dataset": "art", "method": "random_question", "asked": true, "q": "Did It was a tornado outside. happen?", "a": "no", "prior_probs": [0.3994401099647912, 0.6005598900352088], "posterior_probs": [0.5027813624545244, 0.4972186375454755], "prior_entropy": 0.672783997955092, "posterior_entropy": 0.6931317085239437, "delta_entropy": -0.02034771056885165, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5027813624545244, "accuracy": 0, "tokens_in": 156, "tokens_out": 36, "tokens_total": 192, "latency_total": 0.03221251599916286, "latency_per_module": {"scorer": 0.016047398999944562, "prior": 0.0161651169992183}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.\nQuestion: Did It was a tornado outside. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.", "hypotheses": ["It was a tornado outside.", "It was raining and I did not want to get wet."]}
{"example_id": "189", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Jim's wife knew he was tired and made coffee. happen?", "a": "no", "prior_probs": [0.5614921399756847, 0.4385078600243153], "posterior_probs": [0.6450461098131087, 0.3549538901868913], "prior_entropy": 0.6855654335802226, "posterior_entropy": 0.650459471071086, "delta_entropy": 0.03510596250913656, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6450461098131087, "accuracy": 0, "tokens_in": 174, "tokens_out": 32, "tokens_total": 206, "latency_total": 0.03145406499970704, "latency_per_module": {"scorer": 0.01558540499991068, "prior": 0.015868659999796364}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 16, "tokens_total": 124}, "prior": {"tokens_in": 66, "tokens_out": 16, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.\nQuestion: Did Jim's wife knew he was tired and made coffee. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.", "hypotheses": ["Jim's wife knew he was tired and made coffee.", "jim made breakfast."]}
{"example_id": "190", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Popped a tire and spent their picnic time waiting for a tow-truck. happen?", "a": "yes", "prior_probs": [0.6073004646067773, 0.3926995353932227], "posterior_probs": [0.2892233631183233, 0.7107766368816768], "prior_entropy": 0.6699403183745066, "posterior_entropy": 0.6014548288383141, "delta_entropy": 0.06848548953619249, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7107766368816768, "accuracy": 1, "tokens_in": 158, "tokens_out": 50, "tokens_total": 208, "latency_total": 0.03147978000015428, "latency_per_module": {"scorer": 0.015580137000142713, "prior": 0.015899643000011565}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 25, "tokens_total": 131}, "prior": {"tokens_in": 52, "tokens_out": 25, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: The family prepared the food and packed it away. The family had a horrible day.\nQuestion: Did Popped a tire and spent their picnic time waiting for a tow-truck. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: The family prepared the food and packed it away. The family had a horrible day.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The family prepared the food and packed it away. The family had a horrible day.", "hypotheses": ["After the picnic, it started raining.", "Popped a tire and spent their picnic time waiting for a tow-truck."]}
{"example_id": "191", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Tim was able to break through the window. happen?", "a": "no", "prior_probs": [0.5497691534768302, 0.4502308465231699], "posterior_probs": [0.6053511541023504, 0.39464884589764965], "prior_entropy": 0.6881850301904786, "posterior_entropy": 0.6707822147272394, "delta_entropy": 0.017402815463239185, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6053511541023504, "accuracy": 0, "tokens_in": 138, "tokens_out": 34, "tokens_total": 172, "latency_total": 0.02785244899951067, "latency_per_module": {"scorer": 0.014744858000085515, "prior": 0.013107590999425156}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 17, "tokens_total": 105}, "prior": {"tokens_in": 50, "tokens_out": 17, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim never locked his bathroom window. Tim was glad he kept the window unlocked.\nQuestion: Did Tim was able to break through the window. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim never locked his bathroom window. Tim was glad he kept the window unlocked.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim never locked his bathroom window. Tim was glad he kept the window unlocked.", "hypotheses": ["Tim was able to break through the window.", "Tim lost his keys to the house."]}
{"example_id": "192", "dataset": "art", "method": "random_question", "asked": true, "q": "Did There is an old dog on the farm who has lost its mind and barks all day. happen?", "a": "yes", "prior_probs": [0.4674719487572201, 0.5325280512427799], "posterior_probs": [0.30614585980201975, 0.6938541401979802], "prior_entropy": 0.6910295370954334, "posterior_entropy": 0.6159820897794255, "delta_entropy": 0.07504744731600788, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6938541401979802, "accuracy": 1, "tokens_in": 182, "tokens_out": 68, "tokens_total": 250, "latency_total": 0.029694326998651377, "latency_per_module": {"scorer": 0.01487156699931802, "prior": 0.014822759999333357}, "tokens_per_module": {"scorer": {"tokens_in": 120, "tokens_out": 34, "tokens_total": 154}, "prior": {"tokens_in": 62, "tokens_out": 34, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.\nQuestion: Did There is an old dog on the farm who has lost its mind and barks all day. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.", "hypotheses": ["The people that live on the old farm has a dog that fears nothing.", "There is an old dog on the farm who has lost its mind and barks all day."]}
{"example_id": "193", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Oren got a part time job delivering pizza. happen?", "a": "no", "prior_probs": [0.5402261165773485, 0.4597738834226515], "posterior_probs": [0.5793395172699904, 0.4206604827300095], "prior_entropy": 0.6899073994065836, "posterior_entropy": 0.680504291173596, "delta_entropy": 0.009403108232987623, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5793395172699904, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.027133637999213533, "latency_per_module": {"scorer": 0.013727892999668256, "prior": 0.013405744999545277}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.\nQuestion: Did Oren got a part time job delivering pizza. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.", "hypotheses": ["Oren got a part time job delivering pizza.", "he went inside and got a part time job."]}
{"example_id": "194", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Beth went out and broke her phone with no ride home. happen?", "a": "yes", "prior_probs": [0.5162868080307631, 0.48371319196923696], "posterior_probs": [0.3511353152683664, 0.6488646847316336], "prior_entropy": 0.6926165664692046, "posterior_entropy": 0.648146612270792, "delta_entropy": 0.0444699541984126, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.6488646847316336, "accuracy": 1, "tokens_in": 192, "tokens_out": 42, "tokens_total": 234, "latency_total": 0.03121516200098995, "latency_per_module": {"scorer": 0.01575241000045935, "prior": 0.0154627520005306}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 21, "tokens_total": 139}, "prior": {"tokens_in": 74, "tokens_out": 21, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.\nQuestion: Did Beth went out and broke her phone with no ride home. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.", "hypotheses": ["Beth passed out from overeating.", "Beth went out and broke her phone with no ride home."]}
{"example_id": "195", "dataset": "art", "method": "random_question", "asked": true, "q": "Did When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview. happen?", "a": "yes", "prior_probs": [0.5288894510173028, 0.4711105489826972], "posterior_probs": [0.29496572754376327, 0.7050342724562367], "prior_entropy": 0.6914770498116267, "posterior_entropy": 0.6065382361201079, "delta_entropy": 0.08493881369151879, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.7050342724562367, "accuracy": 1, "tokens_in": 196, "tokens_out": 76, "tokens_total": 272, "latency_total": 0.03300381799999741, "latency_per_module": {"scorer": 0.017396313000062946, "prior": 0.015607504999934463}, "tokens_per_module": {"scorer": {"tokens_in": 134, "tokens_out": 38, "tokens_total": 172}, "prior": {"tokens_in": 62, "tokens_out": 38, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.\nQuestion: Did When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.", "hypotheses": ["Everyone realized that the power was out for a long time.", "When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview."]}
{"example_id": "196", "dataset": "art", "method": "random_question", "asked": true, "q": "Did They were both very shy, but were attracted to eachother without even knowing!. happen?", "a": "no", "prior_probs": [0.564453773628524, 0.435546226371476], "posterior_probs": [0.7247751687444844, 0.2752248312555156], "prior_entropy": 0.6848154375403865, "posterior_entropy": 0.5883866021491069, "delta_entropy": 0.09642883539127967, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7247751687444844, "accuracy": 0, "tokens_in": 168, "tokens_out": 46, "tokens_total": 214, "latency_total": 0.029915340000115975, "latency_per_module": {"scorer": 0.015480576000300061, "prior": 0.014434763999815914}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 23, "tokens_total": 133}, "prior": {"tokens_in": 58, "tokens_out": 23, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!\nQuestion: Did They were both very shy, but were attracted to eachother without even knowing!. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!", "hypotheses": ["They were both very shy, but were attracted to eachother without even knowing!.", "She went under to say hello."]}
{"example_id": "197", "dataset": "art", "method": "random_question", "asked": true, "q": "Did the man failed to get a job, until recently. happen?", "a": "no", "prior_probs": [0.5572402122655649, 0.44275978773443514], "posterior_probs": [0.6187027058522457, 0.38129729414775426], "prior_entropy": 0.6865799077622886, "posterior_entropy": 0.6646956445053707, "delta_entropy": 0.021884263256917946, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6187027058522457, "accuracy": 0, "tokens_in": 182, "tokens_out": 38, "tokens_total": 220, "latency_total": 0.030324601000756957, "latency_per_module": {"scorer": 0.014259992000006605, "prior": 0.01606460900075035}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 19, "tokens_total": 131}, "prior": {"tokens_in": 70, "tokens_out": 19, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.\nQuestion: Did the man failed to get a job, until recently. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.", "hypotheses": ["the man failed to get a job, until recently.", "The man eventually gave up on looking."]}
{"example_id": "198", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Chad didn't eat cause he had the flu. happen?", "a": "yes", "prior_probs": [0.5240675038940131, 0.47593249610598687], "posterior_probs": [0.41033261147014927, 0.5896673885298507], "prior_entropy": 0.6919882432894742, "posterior_entropy": 0.6769793771413128, "delta_entropy": 0.015008866148161326, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5896673885298507, "accuracy": 1, "tokens_in": 160, "tokens_out": 66, "tokens_total": 226, "latency_total": 0.030429474999436934, "latency_per_module": {"scorer": 0.013998078999975405, "prior": 0.01643139599946153}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 33, "tokens_total": 133}, "prior": {"tokens_in": 60, "tokens_out": 33, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.\nQuestion: Did Chad didn't eat cause he had the flu. happen?\nAnswer: yes\nHypothesis: ", "prior": "Observation: Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.", "hypotheses": ["Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake.", "Chad didn't eat cause he had the flu."]}
{"example_id": "199", "dataset": "art", "method": "random_question", "asked": true, "q": "Did Ambers friends wanted to see her at the library one night. happen?", "a": "no", "prior_probs": [0.38821194630071754, 0.6117880536992824], "posterior_probs": [0.6491642613563082, 0.3508357386436918], "prior_entropy": 0.667941545366203, "posterior_entropy": 0.6479624595089821, "delta_entropy": 0.019979085857220857, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6491642613563082, "accuracy": 0, "tokens_in": 130, "tokens_out": 46, "tokens_total": 176, "latency_total": 0.02863493900076719, "latency_per_module": {"scorer": 0.014479330000540358, "prior": 0.014155609000226832}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 23, "tokens_total": 111}, "prior": {"tokens_in": 42, "tokens_out": 23, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amber loves to read. She went to the library instead.\nQuestion: Did Ambers friends wanted to see her at the library one night. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amber loves to read. She went to the library instead.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber loves to read. She went to the library instead.", "hypotheses": ["Ambers friends wanted to see her at the library one night.", "Amber liked having a book in her hand."]}
{"example_id": "0", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ron ignores his bosses's orders and called him an idiot. happen?", "a": "no", "prior_probs": [0.4620790510502866, 0.5379209489497133], "posterior_probs": [0.578627754945582, 0.421372245054418], "prior_entropy": 0.690268420337182, "posterior_entropy": 0.680731060478617, "delta_entropy": 0.009537359858565075, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.578627754945582, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.014419600000110222, "latency_per_module": {"scorer": 0.006550510000124632, "prior": 0.00786908999998559}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.\nQuestion: Did Ron ignores his bosses's orders and called him an idiot. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.", "hypotheses": ["Ron ignores his bosses's orders and called him an idiot.", "Ron's boss called him an idiot."]}
{"example_id": "1", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did It stormed in New York. happen?", "a": "no", "prior_probs": [0.49074052349518316, 0.5092594765048167], "posterior_probs": [0.6287766711663705, 0.3712233288336295], "prior_entropy": 0.6929756949449963, "posterior_entropy": 0.6596035482432574, "delta_entropy": 0.033372146701738936, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6287766711663705, "accuracy": 0, "tokens_in": 108, "tokens_out": 24, "tokens_total": 132, "latency_total": 0.012950778000231367, "latency_per_module": {"scorer": 0.006982455999605008, "prior": 0.005968322000626358}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 12, "tokens_total": 82}, "prior": {"tokens_in": 38, "tokens_out": 12, "tokens_total": 50}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sandy lived in New York. Sandy was prepared.\nQuestion: Did It stormed in New York. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sandy lived in New York. Sandy was prepared.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sandy lived in New York. Sandy was prepared.", "hypotheses": ["It stormed in New York.", "She partied all night."]}
{"example_id": "2", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Mary and her mom decided to make chocolate covered frozen bananas to avoid waste. happen?", "a": "no", "prior_probs": [0.5653405163187792, 0.4346594836812207], "posterior_probs": [0.7032779368279838, 0.29672206317201616], "prior_entropy": 0.6845839433504841, "posterior_entropy": 0.6080612765378028, "delta_entropy": 0.07652266681268127, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7032779368279838, "accuracy": 0, "tokens_in": 186, "tokens_out": 46, "tokens_total": 232, "latency_total": 0.015484026000194717, "latency_per_module": {"scorer": 0.008212859000195749, "prior": 0.007271166999998968}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 23, "tokens_total": 141}, "prior": {"tokens_in": 68, "tokens_out": 23, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!\nQuestion: Did Mary and her mom decided to make chocolate covered frozen bananas to avoid waste. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!", "hypotheses": ["Mary and her mom decided to make chocolate covered frozen bananas to avoid waste.", "So Mary made pineapple splits for everyone."]}
{"example_id": "3", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jim found he was missing an item. happen?", "a": "no", "prior_probs": [0.5590702485504583, 0.44092975144954183], "posterior_probs": [0.6138264604825773, 0.3861735395174226], "prior_entropy": 0.6861522671508851, "posterior_entropy": 0.6670056546319258, "delta_entropy": 0.019146612518959216, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6138264604825773, "accuracy": 0, "tokens_in": 140, "tokens_out": 32, "tokens_total": 172, "latency_total": 0.01434901699940383, "latency_per_module": {"scorer": 0.007285277999471873, "prior": 0.007063738999931957}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}, "prior": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jim was working on a project. Luckily, he found it on a nearby shelf.\nQuestion: Did Jim found he was missing an item. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jim was working on a project. Luckily, he found it on a nearby shelf.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was working on a project. Luckily, he found it on a nearby shelf.", "hypotheses": ["Jim found he was missing an item.", "Jim needed a certain animal for it."]}
{"example_id": "4", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He noticed the chair leg was falling off. happen?", "a": "no", "prior_probs": [0.5075695691361044, 0.49243043086389565], "posterior_probs": [0.6052906056334861, 0.394709394366514], "prior_entropy": 0.6930325794262594, "posterior_entropy": 0.6708081104365207, "delta_entropy": 0.022224468989738733, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6052906056334861, "accuracy": 0, "tokens_in": 158, "tokens_out": 40, "tokens_total": 198, "latency_total": 0.01405283700023574, "latency_per_module": {"scorer": 0.006927964000169595, "prior": 0.007124873000066145}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sean was sitting at his desk. After a minute, he was able to put the chair back together.\nQuestion: Did He noticed the chair leg was falling off. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sean was sitting at his desk. After a minute, he was able to put the chair back together.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sean was sitting at his desk. After a minute, he was able to put the chair back together.", "hypotheses": ["He noticed the chair leg was falling off.", "He leaned too far back and his chair tipped over."]}
{"example_id": "5", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Pablo thought that worms were a delicious source of protein. happen?", "a": "no", "prior_probs": [0.6068721153854889, 0.39312788461451104], "posterior_probs": [0.663092299505226, 0.3369077004947741], "prior_entropy": 0.6701266849595834, "posterior_entropy": 0.6389630355504997, "delta_entropy": 0.03116364940908367, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.663092299505226, "accuracy": 0, "tokens_in": 130, "tokens_out": 42, "tokens_total": 172, "latency_total": 0.013742857999204716, "latency_per_module": {"scorer": 0.0073302629998579505, "prior": 0.006412594999346766}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 21, "tokens_total": 107}, "prior": {"tokens_in": 44, "tokens_out": 21, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Pablo likes to eat worms. Pablo does not enjoy eating worms.\nQuestion: Did Pablo thought that worms were a delicious source of protein. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Pablo likes to eat worms. Pablo does not enjoy eating worms.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pablo likes to eat worms. Pablo does not enjoy eating worms.", "hypotheses": ["Pablo thought that worms were a delicious source of protein.", "Pablo then learned what worms really are."]}
{"example_id": "6", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The scientist collected samples of the bacteria and tested them. happen?", "a": "no", "prior_probs": [0.5630861110677394, 0.4369138889322606], "posterior_probs": [0.6252826534032552, 0.3747173465967448], "prior_entropy": 0.6851662110791645, "posterior_entropy": 0.661418681099528, "delta_entropy": 0.023747529979636584, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6252826534032552, "accuracy": 0, "tokens_in": 146, "tokens_out": 38, "tokens_total": 184, "latency_total": 0.014465805999861914, "latency_per_module": {"scorer": 0.0074632300002122065, "prior": 0.007002575999649707}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 19, "tokens_total": 113}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.\nQuestion: Did The scientist collected samples of the bacteria and tested them. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.", "hypotheses": ["The scientist collected samples of the bacteria and tested them.", "He collected the bacteria and froze it."]}
{"example_id": "7", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did My commanding officer told me I wasn't doing bad at my job. happen?", "a": "no", "prior_probs": [0.6387544124977744, 0.36124558750222546], "posterior_probs": [0.7042901005113327, 0.2957098994886673], "prior_entropy": 0.6541314959287188, "posterior_entropy": 0.6071853672294203, "delta_entropy": 0.046946128699298484, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7042901005113327, "accuracy": 0, "tokens_in": 164, "tokens_out": 42, "tokens_total": 206, "latency_total": 0.01434715400046116, "latency_per_module": {"scorer": 0.007313926000279025, "prior": 0.0070332280001821346}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 21, "tokens_total": 127}, "prior": {"tokens_in": 58, "tokens_out": 21, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I joined the Navy. That angered me so I hit him and was arrested by the military police.\nQuestion: Did My commanding officer told me I wasn't doing bad at my job. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I joined the Navy. That angered me so I hit him and was arrested by the military police.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I joined the Navy. That angered me so I hit him and was arrested by the military police.", "hypotheses": ["My commanding officer told me I wasn't doing bad at my job.", "My drill sergeant insulted my mother."]}
{"example_id": "8", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Dotty ate something bad. happen?", "a": "no", "prior_probs": [0.5659128800175401, 0.43408711998245986], "posterior_probs": [0.6220588627167835, 0.3779411372832165], "prior_entropy": 0.6844328221404599, "posterior_entropy": 0.6630472116491068, "delta_entropy": 0.021385610491353013, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6220588627167835, "accuracy": 0, "tokens_in": 124, "tokens_out": 34, "tokens_total": 158, "latency_total": 0.013335547000679071, "latency_per_module": {"scorer": 0.007096992000697355, "prior": 0.006238554999981716}, "tokens_per_module": {"scorer": {"tokens_in": 78, "tokens_out": 17, "tokens_total": 95}, "prior": {"tokens_in": 46, "tokens_out": 17, "tokens_total": 63}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dotty was being very grumpy. She felt much better afterwards.\nQuestion: Did Dotty ate something bad. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Dotty was being very grumpy. She felt much better afterwards.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dotty was being very grumpy. She felt much better afterwards.", "hypotheses": ["Dotty ate something bad.", "Dotty call some close friends to chat."]}
{"example_id": "9", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ali did not want to take karate. happen?", "a": "no", "prior_probs": [0.5599978275854001, 0.4400021724145999], "posterior_probs": [0.6140151489071012, 0.3859848510928988], "prior_entropy": 0.6859303241447694, "posterior_entropy": 0.666918136520481, "delta_entropy": 0.019012187624288357, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6140151489071012, "accuracy": 0, "tokens_in": 170, "tokens_out": 34, "tokens_total": 204, "latency_total": 0.013439963000564603, "latency_per_module": {"scorer": 0.006396894000317843, "prior": 0.00704306900024676}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}, "prior": {"tokens_in": 66, "tokens_out": 17, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.\nQuestion: Did Ali did not want to take karate. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.", "hypotheses": ["Ali did not want to take karate.", "Ali did horribly in her last class."]}
{"example_id": "10", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Cory was teased by some of the kids in his classroom. happen?", "a": "no", "prior_probs": [0.4883579795936534, 0.5116420204063465], "posterior_probs": [0.6087945220562659, 0.391205477943734], "prior_entropy": 0.6928760827807745, "posterior_entropy": 0.6692842588175162, "delta_entropy": 0.023591823963258318, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6087945220562659, "accuracy": 0, "tokens_in": 172, "tokens_out": 50, "tokens_total": 222, "latency_total": 0.014478224999038503, "latency_per_module": {"scorer": 0.007361004999438592, "prior": 0.007117219999599911}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 25, "tokens_total": 133}, "prior": {"tokens_in": 64, "tokens_out": 25, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.\nQuestion: Did Cory was teased by some of the kids in his classroom. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.", "hypotheses": ["Cory was teased by some of the kids in his classroom.", "Cory ran away from home as fast as he could."]}
{"example_id": "11", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did People went to watch the band play. happen?", "a": "no", "prior_probs": [0.5310383771489922, 0.4689616228510078], "posterior_probs": [0.6190707065246792, 0.3809292934753208], "prior_entropy": 0.6912191794642322, "posterior_entropy": 0.6645172283722175, "delta_entropy": 0.026701951092014675, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6190707065246792, "accuracy": 0, "tokens_in": 160, "tokens_out": 36, "tokens_total": 196, "latency_total": 0.014483957000265946, "latency_per_module": {"scorer": 0.007429127000250446, "prior": 0.0070548300000154995}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 18, "tokens_total": 116}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.\nQuestion: Did People went to watch the band play. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.", "hypotheses": ["People went to watch the band play.", "Dennis has been a member for ten seconds."]}
{"example_id": "12", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Deb went to a matinee movie instead. happen?", "a": "no", "prior_probs": [0.42302664764035913, 0.5769733523596409], "posterior_probs": [0.6243510748762258, 0.3756489251237743], "prior_entropy": 0.6812501313089362, "posterior_entropy": 0.6618938275892394, "delta_entropy": 0.019356303719696766, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6243510748762258, "accuracy": 0, "tokens_in": 144, "tokens_out": 34, "tokens_total": 178, "latency_total": 0.014509087999613257, "latency_per_module": {"scorer": 0.007392772999992303, "prior": 0.007116314999620954}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 17, "tokens_total": 109}, "prior": {"tokens_in": 52, "tokens_out": 17, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Deb wanted to go shopping. She found everything she needed and had money left over.\nQuestion: Did Deb went to a matinee movie instead. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Deb wanted to go shopping. She found everything she needed and had money left over.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Deb wanted to go shopping. She found everything she needed and had money left over.", "hypotheses": ["Deb went to a matinee movie instead.", "Deb had a lot of coupons."]}
{"example_id": "13", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Kory stole from the airport. happen?", "a": "no", "prior_probs": [0.617982844061609, 0.38201715593839114], "posterior_probs": [0.642384061401763, 0.357615938598237], "prior_entropy": 0.6650429923305812, "posterior_entropy": 0.6520341448372071, "delta_entropy": 0.013008847493374054, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.642384061401763, "accuracy": 0, "tokens_in": 142, "tokens_out": 36, "tokens_total": 178, "latency_total": 0.014316495000457508, "latency_per_module": {"scorer": 0.007369877000201086, "prior": 0.0069466180002564215}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 18, "tokens_total": 106}, "prior": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My cousin Kory was working at the airport. He is now serving out his sentence.\nQuestion: Did Kory stole from the airport. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My cousin Kory was working at the airport. He is now serving out his sentence.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My cousin Kory was working at the airport. He is now serving out his sentence.", "hypotheses": ["Kory stole from the airport.", "He got caught anti-shoplifting from passengers."]}
{"example_id": "14", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He opened a lemonade stand. happen?", "a": "no", "prior_probs": [0.3696246176351904, 0.6303753823648096], "posterior_probs": [0.523637780949381, 0.476362219050619], "prior_entropy": 0.6587555935763323, "posterior_entropy": 0.6920292745481083, "delta_entropy": -0.033273680971775965, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.523637780949381, "accuracy": 0, "tokens_in": 194, "tokens_out": 38, "tokens_total": 232, "latency_total": 0.014515048999783176, "latency_per_module": {"scorer": 0.007263290999617311, "prior": 0.007251758000165864}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 19, "tokens_total": 133}, "prior": {"tokens_in": 80, "tokens_out": 19, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!\nQuestion: Did He opened a lemonade stand. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!", "hypotheses": ["He opened a lemonade stand.", "Daniel stayed home and didn't want to buy a plane."]}
{"example_id": "15", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Her neck pain stopped because of this. happen?", "a": "no", "prior_probs": [0.5233176960502864, 0.47668230394971345], "posterior_probs": [0.6492351193543503, 0.3507648806456497], "prior_entropy": 0.6920593561485264, "posterior_entropy": 0.6479188447660982, "delta_entropy": 0.04414051138242825, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6492351193543503, "accuracy": 0, "tokens_in": 156, "tokens_out": 32, "tokens_total": 188, "latency_total": 0.015554919999885897, "latency_per_module": {"scorer": 0.007558811999842874, "prior": 0.007996108000043023}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}, "prior": {"tokens_in": 60, "tokens_out": 16, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.\nQuestion: Did Her neck pain stopped because of this. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.", "hypotheses": ["Her neck pain stopped because of this.", "Jenna pulled a muscle lifting weights."]}
{"example_id": "16", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Kat went to get a salad. happen?", "a": "no", "prior_probs": [0.4115371464631088, 0.5884628535368912], "posterior_probs": [0.5245949450802393, 0.47540505491976065], "prior_entropy": 0.6774131328021895, "posterior_entropy": 0.6919368695478447, "delta_entropy": -0.014523736745655214, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5245949450802393, "accuracy": 0, "tokens_in": 146, "tokens_out": 34, "tokens_total": 180, "latency_total": 0.01496916899941425, "latency_per_module": {"scorer": 0.007320753999920271, "prior": 0.00764841499949398}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 17, "tokens_total": 107}, "prior": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.\nQuestion: Did Kat went to get a salad. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.", "hypotheses": ["Kat went to get a salad.", "Kat decided to take a nap instead of eating."]}
{"example_id": "17", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did His owner gave him a lower fat cat food. happen?", "a": "no", "prior_probs": [0.43587771834479677, 0.5641222816552033], "posterior_probs": [0.6431403144986843, 0.3568596855013158], "prior_entropy": 0.6849011558642406, "posterior_entropy": 0.6515899420076727, "delta_entropy": 0.03331121385656788, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6431403144986843, "accuracy": 0, "tokens_in": 140, "tokens_out": 38, "tokens_total": 178, "latency_total": 0.014229450000129873, "latency_per_module": {"scorer": 0.007348102999458206, "prior": 0.006881347000671667}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 19, "tokens_total": 109}, "prior": {"tokens_in": 50, "tokens_out": 19, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Cosmo was a pudgy cat. Now he's fit and muscular!\nQuestion: Did His owner gave him a lower fat cat food. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Cosmo was a pudgy cat. Now he's fit and muscular!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cosmo was a pudgy cat. Now he's fit and muscular!", "hypotheses": ["His owner gave him a lower fat cat food.", "The vet put Cosmo on a treadmill."]}
{"example_id": "18", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tim became very sick one day. happen?", "a": "no", "prior_probs": [0.4609254882025264, 0.5390745117974737], "posterior_probs": [0.5486926232379381, 0.45130737676206195], "prior_entropy": 0.6900904297666779, "posterior_entropy": 0.6883977135215502, "delta_entropy": 0.001692716245127679, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5486926232379381, "accuracy": 0, "tokens_in": 178, "tokens_out": 28, "tokens_total": 206, "latency_total": 0.013969469000585377, "latency_per_module": {"scorer": 0.006841162999990047, "prior": 0.00712830600059533}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 14, "tokens_total": 120}, "prior": {"tokens_in": 72, "tokens_out": 14, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.\nQuestion: Did Tim became very sick one day. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.", "hypotheses": ["Tim became very sick one day.", "Tim could not find his socks."]}
{"example_id": "19", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Adam's brother Christian was afraid of the guns. happen?", "a": "no", "prior_probs": [0.4207589138170007, 0.5792410861829993], "posterior_probs": [0.5732913563649253, 0.4267086436350746], "prior_entropy": 0.6805357754094549, "posterior_entropy": 0.6823651277658056, "delta_entropy": -0.001829352356350733, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5732913563649253, "accuracy": 0, "tokens_in": 140, "tokens_out": 42, "tokens_total": 182, "latency_total": 0.013793381001050875, "latency_per_module": {"scorer": 0.007103718000507797, "prior": 0.006689663000543078}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 21, "tokens_total": 111}, "prior": {"tokens_in": 50, "tokens_out": 21, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: One day Adam bought two BB guns. Adam took the gun away from Christian.\nQuestion: Did Adam's brother Christian was afraid of the guns. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: One day Adam bought two BB guns. Adam took the gun away from Christian.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "One day Adam bought two BB guns. Adam took the gun away from Christian.", "hypotheses": ["Adam's brother Christian was afraid of the guns.", "Christian grabbed the gun and shot Adam in the eye."]}
{"example_id": "20", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She set up a hunting blind in the woods. happen?", "a": "no", "prior_probs": [0.41679212274760574, 0.5832078772523943], "posterior_probs": [0.5958738373510231, 0.40412616264897694], "prior_entropy": 0.6792354461007641, "posterior_entropy": 0.6746492529530614, "delta_entropy": 0.0045861931477027, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5958738373510231, "accuracy": 0, "tokens_in": 136, "tokens_out": 44, "tokens_total": 180, "latency_total": 0.01369820099989738, "latency_per_module": {"scorer": 0.00705164999999397, "prior": 0.006646550999903411}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 22, "tokens_total": 110}, "prior": {"tokens_in": 48, "tokens_out": 22, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My friend is a hunter. The elk was nowhere to be found.\nQuestion: Did She set up a hunting blind in the woods. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My friend is a hunter. The elk was nowhere to be found.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend is a hunter. The elk was nowhere to be found.", "hypotheses": ["She set up a hunting blind in the woods.", "My friend who is a hunter found lots of elk."]}
{"example_id": "21", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I saw the string by the door. happen?", "a": "no", "prior_probs": [0.4435230981074238, 0.5564769018925761], "posterior_probs": [0.607592912066996, 0.392407087933004], "prior_entropy": 0.6867542648829669, "posterior_entropy": 0.6698126381309877, "delta_entropy": 0.01694162675197919, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.607592912066996, "accuracy": 0, "tokens_in": 120, "tokens_out": 32, "tokens_total": 152, "latency_total": 0.013035714999205084, "latency_per_module": {"scorer": 0.00709537900002033, "prior": 0.005940335999184754}, "tokens_per_module": {"scorer": {"tokens_in": 78, "tokens_out": 16, "tokens_total": 94}, "prior": {"tokens_in": 42, "tokens_out": 16, "tokens_total": 58}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I walked into my math class. I ended up failing.\nQuestion: Did I saw the string by the door. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I walked into my math class. I ended up failing.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I walked into my math class. I ended up failing.", "hypotheses": ["I saw the string by the door.", "I didn't study for the test."]}
{"example_id": "22", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did we bought the owners grandmother a new pc. happen?", "a": "no", "prior_probs": [0.5059038040919378, 0.4940961959080622], "posterior_probs": [0.6598127430352833, 0.3401872569647167], "prior_entropy": 0.693077469132524, "posterior_entropy": 0.6411596062170226, "delta_entropy": 0.05191786291550138, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6598127430352833, "accuracy": 0, "tokens_in": 174, "tokens_out": 34, "tokens_total": 208, "latency_total": 0.013247300001239637, "latency_per_module": {"scorer": 0.006361972000377136, "prior": 0.006885328000862501}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 17, "tokens_total": 123}, "prior": {"tokens_in": 68, "tokens_out": 17, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.\nQuestion: Did we bought the owners grandmother a new pc. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.", "hypotheses": ["we bought the owners grandmother a new pc.", "Our founder Rachel only uses the PC."]}
{"example_id": "23", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Mary wears two jackets. happen?", "a": "no", "prior_probs": [0.432870459419034, 0.5671295405809661], "posterior_probs": [0.5256158952280465, 0.4743841047719534], "prior_entropy": 0.6841071564298835, "posterior_entropy": 0.6918342576917953, "delta_entropy": -0.007727101261911784, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5256158952280465, "accuracy": 0, "tokens_in": 142, "tokens_out": 32, "tokens_total": 174, "latency_total": 0.013459784000588115, "latency_per_module": {"scorer": 0.007058868000058283, "prior": 0.006400916000529833}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 16, "tokens_total": 102}, "prior": {"tokens_in": 56, "tokens_out": 16, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary doesn't like cold weather. At least until she can afford to move to warmer state.\nQuestion: Did Mary wears two jackets. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mary doesn't like cold weather. At least until she can afford to move to warmer state.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary doesn't like cold weather. At least until she can afford to move to warmer state.", "hypotheses": ["Mary wears two jackets.", "It seemed that the cold weather stopped for two months."]}
{"example_id": "24", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did They started getting followed by a policeman, ran, and hid behind a building. happen?", "a": "no", "prior_probs": [0.5128077854715616, 0.48719221452843847], "posterior_probs": [0.6864489798689896, 0.3135510201310104], "prior_entropy": 0.6928190659326015, "posterior_entropy": 0.6219124898735855, "delta_entropy": 0.07090657605901596, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6864489798689896, "accuracy": 0, "tokens_in": 184, "tokens_out": 90, "tokens_total": 274, "latency_total": 0.0149003670003367, "latency_per_module": {"scorer": 0.008083382000222628, "prior": 0.006816985000114073}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 45, "tokens_total": 163}, "prior": {"tokens_in": 66, "tokens_out": 45, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.\nQuestion: Did They started getting followed by a policeman, ran, and hid behind a building. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.", "hypotheses": ["They started getting followed by a policeman, ran, and hid behind a building.", "The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers."]}
{"example_id": "25", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Bob got caught sneaking out. happen?", "a": "no", "prior_probs": [0.47422526177721863, 0.5257747382227814], "posterior_probs": [0.5519046509414592, 0.4480953490585407], "prior_entropy": 0.6918179172122214, "posterior_entropy": 0.6877492755067779, "delta_entropy": 0.004068641705443521, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5519046509414592, "accuracy": 0, "tokens_in": 148, "tokens_out": 26, "tokens_total": 174, "latency_total": 0.013508566999917093, "latency_per_module": {"scorer": 0.006976108999879216, "prior": 0.006532458000037877}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 13, "tokens_total": 103}, "prior": {"tokens_in": 58, "tokens_out": 13, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob's parents grounded him. He came back home but his parents didn't even know he left.\nQuestion: Did Bob got caught sneaking out. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Bob's parents grounded him. He came back home but his parents didn't even know he left.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob's parents grounded him. He came back home but his parents didn't even know he left.", "hypotheses": ["Bob got caught sneaking out.", "Bob got away with sneaking out."]}
{"example_id": "26", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Amy won an award for how much work she accomplished and was given the same quota. happen?", "a": "no", "prior_probs": [0.48297515281453285, 0.5170248471854672], "posterior_probs": [0.6638150680427155, 0.3361849319572845], "prior_entropy": 0.6925673776487646, "posterior_entropy": 0.6384724756259421, "delta_entropy": 0.05409490202282252, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6638150680427155, "accuracy": 0, "tokens_in": 178, "tokens_out": 54, "tokens_total": 232, "latency_total": 0.014657921000434726, "latency_per_module": {"scorer": 0.007815889000085008, "prior": 0.006842032000349718}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 27, "tokens_total": 143}, "prior": {"tokens_in": 62, "tokens_out": 27, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.\nQuestion: Did Amy won an award for how much work she accomplished and was given the same quota. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.", "hypotheses": ["Amy won an award for how much work she accomplished and was given the same quota.", "Amy's boss said she needed to do more."]}
{"example_id": "27", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He didn't let his inspiration go to waste, he trained and trained. happen?", "a": "no", "prior_probs": [0.6126871071764336, 0.3873128928235664], "posterior_probs": [0.7139302465013939, 0.28606975349860614], "prior_entropy": 0.6675309227274598, "posterior_entropy": 0.5985949909209831, "delta_entropy": 0.06893593180647672, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7139302465013939, "accuracy": 0, "tokens_in": 178, "tokens_out": 40, "tokens_total": 218, "latency_total": 0.014090307000515168, "latency_per_module": {"scorer": 0.007140207000702503, "prior": 0.0069500999998126645}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 20, "tokens_total": 134}, "prior": {"tokens_in": 64, "tokens_out": 20, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.\nQuestion: Did He didn't let his inspiration go to waste, he trained and trained. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.", "hypotheses": ["He didn't let his inspiration go to waste, he trained and trained.", "Jason learned to knit."]}
{"example_id": "28", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Erin, practiced drawing at home with no luck. happen?", "a": "no", "prior_probs": [0.4758078557354764, 0.5241921442645237], "posterior_probs": [0.5713159913147557, 0.4286840086852443], "prior_entropy": 0.6919762037360642, "posterior_entropy": 0.6829404661028381, "delta_entropy": 0.00903573763322607, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5713159913147557, "accuracy": 0, "tokens_in": 136, "tokens_out": 50, "tokens_total": 186, "latency_total": 0.014076212999498239, "latency_per_module": {"scorer": 0.007211406000351417, "prior": 0.006864806999146822}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 25, "tokens_total": 113}, "prior": {"tokens_in": 48, "tokens_out": 25, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Erin tried to learn how to draw. So she joined a drawing class.\nQuestion: Did Erin, practiced drawing at home with no luck. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Erin tried to learn how to draw. So she joined a drawing class.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erin tried to learn how to draw. So she joined a drawing class.", "hypotheses": ["Erin, practiced drawing at home with no luck.", "Erin, practiced drawing at home and became recognized for her talent."]}
{"example_id": "29", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jon crashed the police car into a telephone poll. happen?", "a": "no", "prior_probs": [0.47143509427231317, 0.5285649057276869], "posterior_probs": [0.6258173277287377, 0.37418267227126234], "prior_entropy": 0.6915143840109692, "posterior_entropy": 0.6611443006749416, "delta_entropy": 0.030370083336027642, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6258173277287377, "accuracy": 0, "tokens_in": 140, "tokens_out": 30, "tokens_total": 170, "latency_total": 0.01358619199891109, "latency_per_module": {"scorer": 0.00713169799928437, "prior": 0.00645449399962672}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 15, "tokens_total": 105}, "prior": {"tokens_in": 50, "tokens_out": 15, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jon decided to steal a police car. Jon went to prison for three years.\nQuestion: Did Jon crashed the police car into a telephone poll. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jon decided to steal a police car. Jon went to prison for three years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jon decided to steal a police car. Jon went to prison for three years.", "hypotheses": ["Jon crashed the police car into a telephone poll.", "Jon wasn't caught."]}
{"example_id": "30", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I failed a big test. happen?", "a": "no", "prior_probs": [0.47574630971646853, 0.5242536902835314], "posterior_probs": [0.5543480142754739, 0.4456519857245262], "prior_entropy": 0.6919702357676847, "posterior_entropy": 0.6872280794109229, "delta_entropy": 0.004742156356761784, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5543480142754739, "accuracy": 0, "tokens_in": 140, "tokens_out": 36, "tokens_total": 176, "latency_total": 0.013635850000355276, "latency_per_module": {"scorer": 0.0070958670003165025, "prior": 0.006539983000038774}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 18, "tokens_total": 104}, "prior": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I used to procrastinate about studying. Now, I never procrastinate studying.\nQuestion: Did I failed a big test. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I used to procrastinate about studying. Now, I never procrastinate studying.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used to procrastinate about studying. Now, I never procrastinate studying.", "hypotheses": ["I failed a big test.", "After getting a good grade, I learned an easy lesson."]}
{"example_id": "31", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jacob decided to buy himself a car. happen?", "a": "no", "prior_probs": [0.499290943621067, 0.500709056378933], "posterior_probs": [0.539850527857254, 0.46014947214274604], "prior_entropy": 0.6931461750357113, "posterior_entropy": 0.6899676802449393, "delta_entropy": 0.003178494790772035, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.539850527857254, "accuracy": 0, "tokens_in": 156, "tokens_out": 30, "tokens_total": 186, "latency_total": 0.01422282100065786, "latency_per_module": {"scorer": 0.007247511000059603, "prior": 0.006975310000598256}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 15, "tokens_total": 111}, "prior": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.\nQuestion: Did Jacob decided to buy himself a car. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.", "hypotheses": ["Jacob decided to buy himself a car.", "Jacob couldn't afford a car."]}
{"example_id": "32", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He yelled at the players for every home run. happen?", "a": "no", "prior_probs": [0.5412382362692145, 0.4587617637307855], "posterior_probs": [0.6658260573473478, 0.3341739426526522], "prior_entropy": 0.6897421297482087, "posterior_entropy": 0.6370952450353706, "delta_entropy": 0.05264688471283807, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6658260573473478, "accuracy": 0, "tokens_in": 160, "tokens_out": 34, "tokens_total": 194, "latency_total": 0.013424366000435839, "latency_per_module": {"scorer": 0.006631754000409273, "prior": 0.0067926120000265655}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 17, "tokens_total": 117}, "prior": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.\nQuestion: Did He yelled at the players for every home run. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.", "hypotheses": ["He yelled at the players for every home run.", "Roy made the other team uncomfortable."]}
{"example_id": "33", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Stan was out of school for a week with the stomach ache. happen?", "a": "no", "prior_probs": [0.5939922725940271, 0.40600772740597296], "posterior_probs": [0.6555961817365425, 0.3444038182634574], "prior_entropy": 0.6753725208367414, "posterior_entropy": 0.6439133824685718, "delta_entropy": 0.031459138368169604, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6555961817365425, "accuracy": 0, "tokens_in": 180, "tokens_out": 46, "tokens_total": 226, "latency_total": 0.01438018099997862, "latency_per_module": {"scorer": 0.007340739000028407, "prior": 0.007039441999950213}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 23, "tokens_total": 137}, "prior": {"tokens_in": 66, "tokens_out": 23, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.\nQuestion: Did Stan was out of school for a week with the stomach ache. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.", "hypotheses": ["Stan was out of school for a week with the stomach ache.", "The school nurse sent Stan home from school."]}
{"example_id": "34", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Lisa and Tim went to a fertility clinic to get pregnant. happen?", "a": "no", "prior_probs": [0.6792737606353848, 0.32072623936461514], "posterior_probs": [0.7102502042842097, 0.28974979571579024], "prior_entropy": 0.6274156648989593, "posterior_entropy": 0.6019275016193724, "delta_entropy": 0.025488163279586917, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7102502042842097, "accuracy": 0, "tokens_in": 136, "tokens_out": 52, "tokens_total": 188, "latency_total": 0.0137911219999296, "latency_per_module": {"scorer": 0.007144344000153069, "prior": 0.006646777999776532}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 26, "tokens_total": 116}, "prior": {"tokens_in": 46, "tokens_out": 26, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lisa and Tim had been married for a long time. It worked.\nQuestion: Did Lisa and Tim went to a fertility clinic to get pregnant. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lisa and Tim had been married for a long time. It worked.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa and Tim had been married for a long time. It worked.", "hypotheses": ["Lisa and Tim went to a fertility clinic to get pregnant.", "They decided to try the advice given in a book about guitar playing."]}
{"example_id": "35", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?", "a": "no", "prior_probs": [0.6086536919959997, 0.3913463080040002], "posterior_probs": [0.7024292513921555, 0.2975707486078445], "prior_entropy": 0.6693464989756049, "posterior_entropy": 0.6087919301335103, "delta_entropy": 0.06055456884209465, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7024292513921555, "accuracy": 0, "tokens_in": 182, "tokens_out": 54, "tokens_total": 236, "latency_total": 0.016088384000795486, "latency_per_module": {"scorer": 0.009088081000300008, "prior": 0.007000303000495478}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 27, "tokens_total": 145}, "prior": {"tokens_in": 64, "tokens_out": 27, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.\nQuestion: Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.", "hypotheses": ["Adam made himself a sandwich using bread, turkey, and a slice of American cheese.", "Adam made himself a pb&j sandwich."]}
{"example_id": "36", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tom got tired of painting after he finished. happen?", "a": "no", "prior_probs": [0.5647430126639348, 0.4352569873360652], "posterior_probs": [0.6830631411099708, 0.31693685889002915], "prior_entropy": 0.6847402799808633, "posterior_entropy": 0.6245389516875033, "delta_entropy": 0.06020132829336, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6830631411099708, "accuracy": 0, "tokens_in": 126, "tokens_out": 36, "tokens_total": 162, "latency_total": 0.014360250000208907, "latency_per_module": {"scorer": 0.007524136000029102, "prior": 0.006836114000179805}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 18, "tokens_total": 100}, "prior": {"tokens_in": 44, "tokens_out": 18, "tokens_total": 62}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was painting his fence. Tom left his fence half painted.\nQuestion: Did Tom got tired of painting after he finished. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tom was painting his fence. Tom left his fence half painted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was painting his fence. Tom left his fence half painted.", "hypotheses": ["Tom got tired of painting after he finished.", "Tom heard a game was on and left."]}
{"example_id": "37", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She would be with her friends out there. happen?", "a": "no", "prior_probs": [0.6175754766923246, 0.3824245233076754], "posterior_probs": [0.6861092164085281, 0.31389078359147204], "prior_entropy": 0.6652385826527232, "posterior_entropy": 0.62217845014932, "delta_entropy": 0.043060132503403126, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6861092164085281, "accuracy": 0, "tokens_in": 150, "tokens_out": 36, "tokens_total": 186, "latency_total": 0.014537787000335811, "latency_per_module": {"scorer": 0.007234182000502187, "prior": 0.007303604999833624}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.\nQuestion: Did She would be with her friends out there. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.", "hypotheses": ["She would be with her friends out there.", "Amy wanted to live by the beadch."]}
{"example_id": "38", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Roger overslept and lounged most the day. happen?", "a": "no", "prior_probs": [0.40261986142381756, 0.5973801385761824], "posterior_probs": [0.5991665286893899, 0.40083347131061015], "prior_entropy": 0.6740596404089629, "posterior_entropy": 0.6733481636381817, "delta_entropy": 0.0007114767707812053, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5991665286893899, "accuracy": 0, "tokens_in": 184, "tokens_out": 48, "tokens_total": 232, "latency_total": 0.01455767800052854, "latency_per_module": {"scorer": 0.007382802999927662, "prior": 0.007174875000600878}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 24, "tokens_total": 138}, "prior": {"tokens_in": 70, "tokens_out": 24, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.\nQuestion: Did Roger overslept and lounged most the day. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.", "hypotheses": ["Roger overslept and lounged most the day.", "Roger tried but he wasn't as good as his idol."]}
{"example_id": "39", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Barry did not tell anyone that Julie farted. happen?", "a": "no", "prior_probs": [0.5156470688694731, 0.484352931130527], "posterior_probs": [0.6214058532731264, 0.37859414672687375], "prior_entropy": 0.6926574390754046, "posterior_entropy": 0.6633716971305815, "delta_entropy": 0.02928574194482303, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6214058532731264, "accuracy": 0, "tokens_in": 156, "tokens_out": 44, "tokens_total": 200, "latency_total": 0.014766098000109196, "latency_per_module": {"scorer": 0.007219736000479315, "prior": 0.00754636199962988}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}, "prior": {"tokens_in": 58, "tokens_out": 22, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.\nQuestion: Did Barry did not tell anyone that Julie farted. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.", "hypotheses": ["Barry did not tell anyone that Julie farted.", "Barry laughed at Julie's unzipped pants."]}
{"example_id": "40", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Bob stopped in the middle of the hike because he had no bug spray. happen?", "a": "no", "prior_probs": [0.44667890952075856, 0.5533210904792414], "posterior_probs": [0.602920546917774, 0.397079453082226], "prior_entropy": 0.6874500759315252, "posterior_entropy": 0.6718097026081133, "delta_entropy": 0.015640373323411838, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.602920546917774, "accuracy": 0, "tokens_in": 158, "tokens_out": 56, "tokens_total": 214, "latency_total": 0.014047961999494873, "latency_per_module": {"scorer": 0.007012525999925856, "prior": 0.0070354359995690174}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 28, "tokens_total": 132}, "prior": {"tokens_in": 54, "tokens_out": 28, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.\nQuestion: Did Bob stopped in the middle of the hike because he had no bug spray. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.", "hypotheses": ["Bob stopped in the middle of the hike because he had no bug spray.", "Bob stopped in the middle of his hike to tie his shoes."]}
{"example_id": "41", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Lucy decided to make the pizzas at home. happen?", "a": "no", "prior_probs": [0.5715470914982475, 0.42845290850175255], "posterior_probs": [0.6069365033988582, 0.3930634966011417], "prior_entropy": 0.6828739799192332, "posterior_entropy": 0.6700987200822498, "delta_entropy": 0.012775259836983355, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6069365033988582, "accuracy": 0, "tokens_in": 180, "tokens_out": 36, "tokens_total": 216, "latency_total": 0.014632835999691451, "latency_per_module": {"scorer": 0.007747483999992255, "prior": 0.006885351999699196}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 18, "tokens_total": 128}, "prior": {"tokens_in": 70, "tokens_out": 18, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.\nQuestion: Did Lucy decided to make the pizzas at home. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.", "hypotheses": ["Lucy decided to make the pizzas at home.", "Lucy started ordering the pizza."]}
{"example_id": "42", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jim found his new hat in a storm. happen?", "a": "no", "prior_probs": [0.49518695439491683, 0.5048130456050831], "posterior_probs": [0.5759092465883465, 0.42409075341165353], "prior_entropy": 0.6931008490264108, "posterior_entropy": 0.6815780690437176, "delta_entropy": 0.011522779982693132, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5759092465883465, "accuracy": 0, "tokens_in": 154, "tokens_out": 36, "tokens_total": 190, "latency_total": 0.014412141999855521, "latency_per_module": {"scorer": 0.007451049999872339, "prior": 0.006961091999983182}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}, "prior": {"tokens_in": 58, "tokens_out": 18, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was a very windy day. Jim wished he hadn't gone out in his new hat.\nQuestion: Did Jim found his new hat in a storm. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: It was a very windy day. Jim wished he hadn't gone out in his new hat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was a very windy day. Jim wished he hadn't gone out in his new hat.", "hypotheses": ["Jim found his new hat in a storm.", "Jim's hat blew away in the wind."]}
{"example_id": "43", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The water was perfect for all levels of fishing. happen?", "a": "no", "prior_probs": [0.5889699375363152, 0.4110300624636848], "posterior_probs": [0.664761432879087, 0.335238567120913], "prior_entropy": 0.6772312612503355, "posterior_entropy": 0.6378266161575247, "delta_entropy": 0.03940464509281083, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.664761432879087, "accuracy": 0, "tokens_in": 168, "tokens_out": 34, "tokens_total": 202, "latency_total": 0.014136510999378515, "latency_per_module": {"scorer": 0.00715262999983679, "prior": 0.006983880999541725}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}, "prior": {"tokens_in": 64, "tokens_out": 17, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.\nQuestion: Did The water was perfect for all levels of fishing. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.", "hypotheses": ["The water was perfect for all levels of fishing.", "The water was spitting up poles."]}
{"example_id": "44", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?", "a": "no", "prior_probs": [0.5439580116016066, 0.4560419883983934], "posterior_probs": [0.6560598541048825, 0.34394014589511757], "prior_entropy": 0.6892775731215821, "posterior_entropy": 0.6436144263886251, "delta_entropy": 0.045663146732957016, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6560598541048825, "accuracy": 0, "tokens_in": 198, "tokens_out": 60, "tokens_total": 258, "latency_total": 0.015297064000151295, "latency_per_module": {"scorer": 0.007940993000374874, "prior": 0.007356070999776421}, "tokens_per_module": {"scorer": {"tokens_in": 128, "tokens_out": 30, "tokens_total": 158}, "prior": {"tokens_in": 70, "tokens_out": 30, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.\nQuestion: Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.", "hypotheses": ["I went to visit her and stepped out onto the balcony of her apartment with a great view.", "I looked down from her balcony to see the clouds."]}
{"example_id": "45", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Allister was still a novice at the bow. happen?", "a": "no", "prior_probs": [0.5420201553935746, 0.45797984460642543], "posterior_probs": [0.5608728440586146, 0.43912715594138535], "prior_entropy": 0.689611624953167, "posterior_entropy": 0.6857177571864664, "delta_entropy": 0.0038938677667005317, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5608728440586146, "accuracy": 0, "tokens_in": 168, "tokens_out": 38, "tokens_total": 206, "latency_total": 0.013537159000406973, "latency_per_module": {"scorer": 0.006563902999914717, "prior": 0.006973256000492256}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}, "prior": {"tokens_in": 64, "tokens_out": 19, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.\nQuestion: Did Allister was still a novice at the bow. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.", "hypotheses": ["Allister was still a novice at the bow.", "Allister was a pro at the bow."]}
{"example_id": "46", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Billy played games and forgot about cleaning until 5PM. happen?", "a": "no", "prior_probs": [0.36029998877311387, 0.6397000112268861], "posterior_probs": [0.6300455644900674, 0.3699544355099326], "prior_entropy": 0.6535906023014595, "posterior_entropy": 0.6589314260399759, "delta_entropy": -0.005340823738516409, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6300455644900674, "accuracy": 0, "tokens_in": 166, "tokens_out": 36, "tokens_total": 202, "latency_total": 0.013769959999990533, "latency_per_module": {"scorer": 0.006797140999879048, "prior": 0.006972819000111485}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.\nQuestion: Did Billy played games and forgot about cleaning until 5PM. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.", "hypotheses": ["Billy played games and forgot about cleaning until 5PM.", "Billy got home from work early."]}
{"example_id": "47", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She ended up falling into the river. happen?", "a": "no", "prior_probs": [0.5429731534759819, 0.4570268465240181], "posterior_probs": [0.5858130486295954, 0.4141869513704046], "prior_entropy": 0.6894492362019962, "posterior_entropy": 0.6783462540618717, "delta_entropy": 0.011102982140124507, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5858130486295954, "accuracy": 0, "tokens_in": 172, "tokens_out": 38, "tokens_total": 210, "latency_total": 0.013276528999995207, "latency_per_module": {"scorer": 0.006390321999788284, "prior": 0.006886207000206923}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}, "prior": {"tokens_in": 68, "tokens_out": 19, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.\nQuestion: Did She ended up falling into the river. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.", "hypotheses": ["She ended up falling into the river.", "Maya slipped on some rocks and broke her back."]}
{"example_id": "48", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She had to put in some broth. happen?", "a": "no", "prior_probs": [0.4976706673336924, 0.5023293326663075], "posterior_probs": [0.5352169298952173, 0.4647830701047827], "prior_entropy": 0.6931363289373521, "posterior_entropy": 0.6906646612738052, "delta_entropy": 0.002471667663546895, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5352169298952173, "accuracy": 0, "tokens_in": 140, "tokens_out": 32, "tokens_total": 172, "latency_total": 0.013560725999013812, "latency_per_module": {"scorer": 0.006972865999159694, "prior": 0.006587859999854118}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}, "prior": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Susan was making a soup. She did her best to cut away the bad parts.\nQuestion: Did She had to put in some broth. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Susan was making a soup. She did her best to cut away the bad parts.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Susan was making a soup. She did her best to cut away the bad parts.", "hypotheses": ["She had to put in some broth.", "She had to put in some chicken."]}
{"example_id": "49", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Lulu's daughter was going to go to school for the first time. happen?", "a": "no", "prior_probs": [0.5997206215358223, 0.40027937846417777], "posterior_probs": [0.629891948814377, 0.3701080511856229], "prior_entropy": 0.6731247826300334, "posterior_entropy": 0.6590131622967765, "delta_entropy": 0.014111620333256858, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.629891948814377, "accuracy": 0, "tokens_in": 170, "tokens_out": 64, "tokens_total": 234, "latency_total": 0.014402805000827357, "latency_per_module": {"scorer": 0.007507779000661685, "prior": 0.006895026000165672}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 32, "tokens_total": 142}, "prior": {"tokens_in": 60, "tokens_out": 32, "tokens_total": 92}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.\nQuestion: Did Lulu's daughter was going to go to school for the first time. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.", "hypotheses": ["Lulu's daughter was going to go to school for the first time.", "Lulu's mom was thinking of sending her to a new house despite her objections."]}
{"example_id": "50", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I programmed with Java for robot game. happen?", "a": "no", "prior_probs": [0.4704830929730231, 0.529516907026977], "posterior_probs": [0.4875540222339153, 0.5124459777660847], "prior_entropy": 0.6914036714465945, "posterior_entropy": 0.6928373438319164, "delta_entropy": -0.0014336723853218825, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5124459777660847, "accuracy": 1, "tokens_in": 144, "tokens_out": 40, "tokens_total": 184, "latency_total": 0.013986160999593267, "latency_per_module": {"scorer": 0.007191302999672189, "prior": 0.006794857999921078}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I wanted to create a video game. Indeed, Java was terrible for programming video games.\nQuestion: Did I programmed with Java for robot game. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I wanted to create a video game. Indeed, Java was terrible for programming video games.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I wanted to create a video game. Indeed, Java was terrible for programming video games.", "hypotheses": ["I programmed with Java for robot game.", "I programmed with Java for robot game because it was easy."]}
{"example_id": "51", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did My best friend visited me on a vacation. happen?", "a": "no", "prior_probs": [0.5536741528365954, 0.4463258471634046], "posterior_probs": [0.6009604072669581, 0.3990395927330419], "prior_entropy": 0.6873742336400734, "posterior_entropy": 0.6726203332274289, "delta_entropy": 0.014753900412644572, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6009604072669581, "accuracy": 0, "tokens_in": 178, "tokens_out": 32, "tokens_total": 210, "latency_total": 0.013462267999784672, "latency_per_module": {"scorer": 0.006319600000097125, "prior": 0.007142667999687546}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 16, "tokens_total": 124}, "prior": {"tokens_in": 70, "tokens_out": 16, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.\nQuestion: Did My best friend visited me on a vacation. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.", "hypotheses": ["My best friend visited me on a vacation.", "I went with her to celebrate."]}
{"example_id": "52", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tom was an elitist. happen?", "a": "no", "prior_probs": [0.6080432574413293, 0.39195674255867086], "posterior_probs": [0.6445600116953581, 0.3554399883046418], "prior_entropy": 0.6696153193660608, "posterior_entropy": 0.6507493180587725, "delta_entropy": 0.018866001307288327, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6445600116953581, "accuracy": 0, "tokens_in": 154, "tokens_out": 34, "tokens_total": 188, "latency_total": 0.013988085000164574, "latency_per_module": {"scorer": 0.00721899600011966, "prior": 0.006769089000044914}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 17, "tokens_total": 111}, "prior": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.\nQuestion: Did Tom was an elitist. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.", "hypotheses": ["Tom was an elitist.", "Tom bought costly ones but they broke right away."]}
{"example_id": "53", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did At the store, Kaya saw a very beautiful vase. happen?", "a": "no", "prior_probs": [0.531998472862934, 0.46800152713706594], "posterior_probs": [0.6159191484167477, 0.38408085158325234], "prior_entropy": 0.691097975897329, "posterior_entropy": 0.6660266062473641, "delta_entropy": 0.025071369649964925, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6159191484167477, "accuracy": 0, "tokens_in": 190, "tokens_out": 50, "tokens_total": 240, "latency_total": 0.014596078000067791, "latency_per_module": {"scorer": 0.0076278110000203014, "prior": 0.00696826700004749}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 25, "tokens_total": 143}, "prior": {"tokens_in": 72, "tokens_out": 25, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.\nQuestion: Did At the store, Kaya saw a very beautiful vase. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.", "hypotheses": ["At the store, Kaya saw a very beautiful vase.", "Kaya could not find a single thing at the store,."]}
{"example_id": "54", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Sam put a towel under the leaky fridge. happen?", "a": "no", "prior_probs": [0.5717724395920868, 0.4282275604079132], "posterior_probs": [0.6390454428289989, 0.360954557171001], "prior_entropy": 0.6828089385588626, "posterior_entropy": 0.6539654361392004, "delta_entropy": 0.02884350241966216, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6390454428289989, "accuracy": 0, "tokens_in": 172, "tokens_out": 36, "tokens_total": 208, "latency_total": 0.013177138999708404, "latency_per_module": {"scorer": 0.006200470999829122, "prior": 0.006976667999879282}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 66, "tokens_out": 18, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.\nQuestion: Did Sam put a towel under the leaky fridge. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.", "hypotheses": ["Sam put a towel under the leaky fridge.", "Sam dishwasher broke and was leaking."]}
{"example_id": "55", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Anna took a few asprins and laid down and took a nap. happen?", "a": "no", "prior_probs": [0.46141858598069874, 0.5385814140193013], "posterior_probs": [0.672859242674844, 0.32714075732515596], "prior_entropy": 0.6901671682063506, "posterior_entropy": 0.6321352477832707, "delta_entropy": 0.05803192042307992, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.672859242674844, "accuracy": 0, "tokens_in": 154, "tokens_out": 52, "tokens_total": 206, "latency_total": 0.014017894998687552, "latency_per_module": {"scorer": 0.007164975999330636, "prior": 0.006852918999356916}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 26, "tokens_total": 128}, "prior": {"tokens_in": 52, "tokens_out": 26, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Anna had a bad headache. Thankfully, when she awoke, the headache was gone.\nQuestion: Did Anna took a few asprins and laid down and took a nap. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Anna had a bad headache. Thankfully, when she awoke, the headache was gone.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Anna had a bad headache. Thankfully, when she awoke, the headache was gone.", "hypotheses": ["Anna took a few asprins and laid down and took a nap.", "She went to a concert to get rid of it."]}
{"example_id": "56", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did my dream was very real and I was on a fun bike tour. happen?", "a": "no", "prior_probs": [0.5567255632063237, 0.44327443679367623], "posterior_probs": [0.6731230080832663, 0.32687699191673364], "prior_entropy": 0.6866977243851222, "posterior_entropy": 0.6319448764571933, "delta_entropy": 0.054752847927928894, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6731230080832663, "accuracy": 0, "tokens_in": 148, "tokens_out": 40, "tokens_total": 188, "latency_total": 0.013379052000345837, "latency_per_module": {"scorer": 0.006738263000443112, "prior": 0.006640788999902725}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 50, "tokens_out": 20, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Last night I had a dream about biking. I woke up, bitterly disappointed.\nQuestion: Did my dream was very real and I was on a fun bike tour. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Last night I had a dream about biking. I woke up, bitterly disappointed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Last night I had a dream about biking. I woke up, bitterly disappointed.", "hypotheses": ["my dream was very real and I was on a fun bike tour.", "I actually have a bike."]}
{"example_id": "57", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She forgot what time it was and was home late. happen?", "a": "no", "prior_probs": [0.4345182363492293, 0.5654817636507706], "posterior_probs": [0.5734261958865692, 0.4265738041134308], "prior_entropy": 0.6845467737315923, "posterior_entropy": 0.6823252734878612, "delta_entropy": 0.0022215002437311338, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5734261958865692, "accuracy": 0, "tokens_in": 154, "tokens_out": 46, "tokens_total": 200, "latency_total": 0.013016535999668122, "latency_per_module": {"scorer": 0.0062068509996606736, "prior": 0.006809685000007448}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 23, "tokens_total": 121}, "prior": {"tokens_in": 56, "tokens_out": 23, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: It was starting to get late outside. Her parents grounded her for a week for being late.\nQuestion: Did She forgot what time it was and was home late. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: It was starting to get late outside. Her parents grounded her for a week for being late.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was starting to get late outside. Her parents grounded her for a week for being late.", "hypotheses": ["She forgot what time it was and was home late.", "She was supposed to be home an hour after she arrived."]}
{"example_id": "58", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Kim needed more money than she could get. happen?", "a": "no", "prior_probs": [0.574938188784644, 0.425061811215356], "posterior_probs": [0.5984736563405317, 0.40152634365946827], "prior_entropy": 0.6818732852598134, "posterior_entropy": 0.6736256945591717, "delta_entropy": 0.008247590700641738, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5984736563405317, "accuracy": 0, "tokens_in": 158, "tokens_out": 34, "tokens_total": 192, "latency_total": 0.013967355000204407, "latency_per_module": {"scorer": 0.00714950699966721, "prior": 0.006817848000537197}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 17, "tokens_total": 115}, "prior": {"tokens_in": 60, "tokens_out": 17, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.\nQuestion: Did Kim needed more money than she could get. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.", "hypotheses": ["Kim needed more money than she could get.", "Kim applied for jobs to make money."]}
{"example_id": "59", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Joe had plenty of time to cook something. happen?", "a": "no", "prior_probs": [0.49453006156178386, 0.5054699384382161], "posterior_probs": [0.5218400180201996, 0.47815998197980025], "prior_entropy": 0.6930873389112259, "posterior_entropy": 0.6921929041972648, "delta_entropy": 0.000894434713961112, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5218400180201996, "accuracy": 0, "tokens_in": 150, "tokens_out": 36, "tokens_total": 186, "latency_total": 0.014919636999366048, "latency_per_module": {"scorer": 0.008183348999409645, "prior": 0.006736287999956403}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!\nQuestion: Did Joe had plenty of time to cook something. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!", "hypotheses": ["Joe had plenty of time to cook something.", "Joe didn't have time to cook something."]}
{"example_id": "60", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He went to the near by super market. happen?", "a": "no", "prior_probs": [0.4444638647440158, 0.5555361352559842], "posterior_probs": [0.6486373170766571, 0.35136268292334283], "prior_entropy": 0.6869659093462148, "posterior_entropy": 0.6482861145182428, "delta_entropy": 0.038679794827972014, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6486373170766571, "accuracy": 0, "tokens_in": 142, "tokens_out": 40, "tokens_total": 182, "latency_total": 0.015852849999646423, "latency_per_module": {"scorer": 0.008167070000126841, "prior": 0.007685779999519582}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.\nQuestion: Did He went to the near by super market. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.", "hypotheses": ["He went to the near by super market.", "Tim looked for a long time in the messy fridge."]}
{"example_id": "61", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The rap tape was mean and rude. happen?", "a": "no", "prior_probs": [0.6524055812220552, 0.3475944187779449], "posterior_probs": [0.7030050701302726, 0.2969949298697275], "prior_entropy": 0.6459447581254353, "posterior_entropy": 0.6082965702015342, "delta_entropy": 0.03764818792390112, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7030050701302726, "accuracy": 0, "tokens_in": 156, "tokens_out": 32, "tokens_total": 188, "latency_total": 0.015729726000245137, "latency_per_module": {"scorer": 0.008035212999857322, "prior": 0.007694513000387815}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}, "prior": {"tokens_in": 60, "tokens_out": 16, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.\nQuestion: Did The rap tape was mean and rude. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.", "hypotheses": ["The rap tape was mean and rude.", "Gina  take their new tape."]}
{"example_id": "62", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did We ordered a dessert everyone would like. happen?", "a": "no", "prior_probs": [0.5428062460011205, 0.45719375399887957], "posterior_probs": [0.5853486189041158, 0.4146513810958843], "prior_entropy": 0.6894779411916492, "posterior_entropy": 0.6785068196410964, "delta_entropy": 0.010971121550552798, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5853486189041158, "accuracy": 0, "tokens_in": 144, "tokens_out": 32, "tokens_total": 176, "latency_total": 0.015355979000560183, "latency_per_module": {"scorer": 0.007835205000446877, "prior": 0.007520774000113306}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 16, "tokens_total": 106}, "prior": {"tokens_in": 54, "tokens_out": 16, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.\nQuestion: Did We ordered a dessert everyone would like. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.", "hypotheses": ["We ordered a dessert everyone would like.", "We ordered appetizers everyone would like."]}
{"example_id": "63", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ora decided to eat healthy for a month. happen?", "a": "no", "prior_probs": [0.5766461630064038, 0.4233538369935963], "posterior_probs": [0.6339982773888124, 0.36600172261118746], "prior_entropy": 0.6813514588057896, "posterior_entropy": 0.6567933882651372, "delta_entropy": 0.0245580705406524, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6339982773888124, "accuracy": 0, "tokens_in": 148, "tokens_out": 42, "tokens_total": 190, "latency_total": 0.015379678000499553, "latency_per_module": {"scorer": 0.007844755000405712, "prior": 0.0075349230000938405}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 21, "tokens_total": 115}, "prior": {"tokens_in": 54, "tokens_out": 21, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ora had always been overweight. With their help, Ora lost over twenty pounds!\nQuestion: Did Ora decided to eat healthy for a month. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ora had always been overweight. With their help, Ora lost over twenty pounds!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora had always been overweight. With their help, Ora lost over twenty pounds!", "hypotheses": ["Ora decided to eat healthy for a month.", "Ora decided she wanted to maintainer her weight."]}
{"example_id": "64", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She ordered two shrimp dishes. happen?", "a": "no", "prior_probs": [0.4374002057690711, 0.562599794230929], "posterior_probs": [0.5282452676885162, 0.47175473231148374], "prior_entropy": 0.6852891073100924, "posterior_entropy": 0.6915507405431092, "delta_entropy": -0.006261633233016761, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5282452676885162, "accuracy": 0, "tokens_in": 136, "tokens_out": 38, "tokens_total": 174, "latency_total": 0.014895445999172807, "latency_per_module": {"scorer": 0.007678701999793702, "prior": 0.007216743999379105}, "tokens_per_module": {"scorer": {"tokens_in": 84, "tokens_out": 19, "tokens_total": 103}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Priya decided to try a new restaurant. Priya thought her food was delicious.\nQuestion: Did She ordered two shrimp dishes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Priya decided to try a new restaurant. Priya thought her food was delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Priya decided to try a new restaurant. Priya thought her food was delicious.", "hypotheses": ["She ordered two shrimp dishes.", "The food that Priya ordered was microwaved and precooked."]}
{"example_id": "65", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jamie and Candice did not know what movie to see. happen?", "a": "no", "prior_probs": [0.43257818279028354, 0.5674218172097164], "posterior_probs": [0.5678040352906254, 0.4321959647093746], "prior_entropy": 0.6840280241266522, "posterior_entropy": 0.6839240155000972, "delta_entropy": 0.00010400862655501886, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5678040352906254, "accuracy": 0, "tokens_in": 152, "tokens_out": 46, "tokens_total": 198, "latency_total": 0.014420816000892955, "latency_per_module": {"scorer": 0.0069093580004846444, "prior": 0.007511458000408311}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 23, "tokens_total": 121}, "prior": {"tokens_in": 54, "tokens_out": 23, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jamie and Candice were going on a date. Finally, they settled on ice cream!\nQuestion: Did Jamie and Candice did not know what movie to see. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jamie and Candice were going on a date. Finally, they settled on ice cream!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jamie and Candice were going on a date. Finally, they settled on ice cream!", "hypotheses": ["Jamie and Candice did not know what movie to see.", "Jamie and Candice couldn't decide what to do."]}
{"example_id": "66", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Scott found a job in New York. happen?", "a": "no", "prior_probs": [0.6046598668084326, 0.39534013319156736], "posterior_probs": [0.6375940117979644, 0.3624059882020357], "prior_entropy": 0.6710769560047052, "posterior_entropy": 0.6547899638549723, "delta_entropy": 0.016286992149732837, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6375940117979644, "accuracy": 0, "tokens_in": 184, "tokens_out": 54, "tokens_total": 238, "latency_total": 0.015357315000073868, "latency_per_module": {"scorer": 0.007505900000069232, "prior": 0.007851415000004636}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 27, "tokens_total": 137}, "prior": {"tokens_in": 74, "tokens_out": 27, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.\nQuestion: Did Scott found a job in New York. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.", "hypotheses": ["Scott found a job in New York.", "The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living."]}
{"example_id": "67", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I got larger sizes. happen?", "a": "no", "prior_probs": [0.42338107670920266, 0.5766189232907972], "posterior_probs": [0.5586415163085086, 0.4413584836914915], "prior_entropy": 0.6813598749171441, "posterior_entropy": 0.6862536709103079, "delta_entropy": -0.004893795993163752, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5586415163085086, "accuracy": 0, "tokens_in": 150, "tokens_out": 30, "tokens_total": 180, "latency_total": 0.015271816000677063, "latency_per_module": {"scorer": 0.007787751000250864, "prior": 0.007484065000426199}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 15, "tokens_total": 105}, "prior": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I went to the store one day to buy clothes. I went home and the jeans fit much better.\nQuestion: Did I got larger sizes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I went to the store one day to buy clothes. I went home and the jeans fit much better.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I went to the store one day to buy clothes. I went home and the jeans fit much better.", "hypotheses": ["I got larger sizes.", "I bought jeans thought they were a bit expensive."]}
{"example_id": "68", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did They ran into a cute friend of Sam's on the way to dinner. happen?", "a": "no", "prior_probs": [0.5093751289457198, 0.49062487105428026], "posterior_probs": [0.6646352315268922, 0.3353647684731078], "prior_entropy": 0.6929713841707495, "posterior_entropy": 0.6379129760809052, "delta_entropy": 0.0550584080898443, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6646352315268922, "accuracy": 0, "tokens_in": 186, "tokens_out": 54, "tokens_total": 240, "latency_total": 0.015777501000229677, "latency_per_module": {"scorer": 0.008033238000280107, "prior": 0.007744262999949569}, "tokens_per_module": {"scorer": {"tokens_in": 118, "tokens_out": 27, "tokens_total": 145}, "prior": {"tokens_in": 68, "tokens_out": 27, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.\nQuestion: Did They ran into a cute friend of Sam's on the way to dinner. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.", "hypotheses": ["They ran into a cute friend of Sam's on the way to dinner.", "The date went bad, they went home on good terms."]}
{"example_id": "69", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Pam tried her best to be as honest as possible. happen?", "a": "no", "prior_probs": [0.5997088313230708, 0.4002911686769291], "posterior_probs": [0.6315549156136061, 0.368445084386394], "prior_entropy": 0.6731295491372684, "posterior_entropy": 0.6581229393318856, "delta_entropy": 0.01500660980538282, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6315549156136061, "accuracy": 0, "tokens_in": 182, "tokens_out": 42, "tokens_total": 224, "latency_total": 0.015006954000455153, "latency_per_module": {"scorer": 0.00744255300014629, "prior": 0.007564401000308862}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 21, "tokens_total": 133}, "prior": {"tokens_in": 70, "tokens_out": 21, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.\nQuestion: Did Pam tried her best to be as honest as possible. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.", "hypotheses": ["Pam tried her best to be as honest as possible.", "Pam wasted time with doing her surveys."]}
{"example_id": "70", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did In Lydia's dream, she was poor and lonely. happen?", "a": "no", "prior_probs": [0.4946844792302691, 0.505315520769731], "posterior_probs": [0.5413426026341418, 0.4586573973658582], "prior_entropy": 0.6930906699713483, "posterior_entropy": 0.6897248530792355, "delta_entropy": 0.003365816892112794, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5413426026341418, "accuracy": 0, "tokens_in": 138, "tokens_out": 44, "tokens_total": 182, "latency_total": 0.015330676000303356, "latency_per_module": {"scorer": 0.007945442000163894, "prior": 0.007385234000139462}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 22, "tokens_total": 112}, "prior": {"tokens_in": 48, "tokens_out": 22, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lydia had a strange dream last night. Lydia wished the dream were real.\nQuestion: Did In Lydia's dream, she was poor and lonely. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lydia had a strange dream last night. Lydia wished the dream were real.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lydia had a strange dream last night. Lydia wished the dream were real.", "hypotheses": ["In Lydia's dream, she was poor and lonely.", "In Lydia's dream, she was rich and famous."]}
{"example_id": "71", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Dana asked a neighbor to ride with her. happen?", "a": "no", "prior_probs": [0.486309255929223, 0.5136907440707771], "posterior_probs": [0.5635138992350016, 0.43648610076499844], "prior_entropy": 0.6927722607542264, "posterior_entropy": 0.6850573107898151, "delta_entropy": 0.007714949964411244, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5635138992350016, "accuracy": 0, "tokens_in": 138, "tokens_out": 42, "tokens_total": 180, "latency_total": 0.015419436999764002, "latency_per_module": {"scorer": 0.007879093999690667, "prior": 0.007540343000073335}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 21, "tokens_total": 109}, "prior": {"tokens_in": 50, "tokens_out": 21, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dana always wanted to ride a bike. They were riding around together within minutes.\nQuestion: Did Dana asked a neighbor to ride with her. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Dana always wanted to ride a bike. They were riding around together within minutes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dana always wanted to ride a bike. They were riding around together within minutes.", "hypotheses": ["Dana asked a neighbor to ride with her.", "Her friend asked Dana to teach her how to ride."]}
{"example_id": "72", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Kelly tried out for the soccer team but was cut. happen?", "a": "no", "prior_probs": [0.5442938108983753, 0.4557061891016247], "posterior_probs": [0.624728277636263, 0.37527172236373696], "prior_entropy": 0.6892181487243876, "posterior_entropy": 0.6617018834066446, "delta_entropy": 0.027516265317742983, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.624728277636263, "accuracy": 0, "tokens_in": 142, "tokens_out": 36, "tokens_total": 178, "latency_total": 0.015055256999403355, "latency_per_module": {"scorer": 0.007865366999794787, "prior": 0.0071898899996085675}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}, "prior": {"tokens_in": 50, "tokens_out": 18, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.\nQuestion: Did Kelly tried out for the soccer team but was cut. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.", "hypotheses": ["Kelly tried out for the soccer team but was cut.", "Kelly made it onto the team."]}
{"example_id": "73", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tom didn't check the grills gas. happen?", "a": "no", "prior_probs": [0.4592465255863792, 0.5407534744136208], "posterior_probs": [0.5867634754752247, 0.4132365245247753], "prior_entropy": 0.6898218015182185, "posterior_entropy": 0.6780148951101925, "delta_entropy": 0.011806906408025952, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5867634754752247, "accuracy": 0, "tokens_in": 150, "tokens_out": 36, "tokens_total": 186, "latency_total": 0.015349691999290371, "latency_per_module": {"scorer": 0.007913929999631364, "prior": 0.007435761999659007}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.\nQuestion: Did Tom didn't check the grills gas. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.", "hypotheses": ["Tom didn't check the grills gas.", "Tom didn't check the grill's safety."]}
{"example_id": "74", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Martins friends went fishing without martin. happen?", "a": "no", "prior_probs": [0.33492383062893677, 0.6650761693710633], "posterior_probs": [0.5865516441304067, 0.4134483558695933], "prior_entropy": 0.6376109297354657, "posterior_entropy": 0.6780890710074758, "delta_entropy": -0.040478141272010104, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5865516441304067, "accuracy": 0, "tokens_in": 146, "tokens_out": 36, "tokens_total": 182, "latency_total": 0.015157425001234515, "latency_per_module": {"scorer": 0.0078035570004431065, "prior": 0.007353868000791408}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}, "prior": {"tokens_in": 54, "tokens_out": 18, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Martin went to camp with his friends. Martin caught many fish so that everyone could eat.\nQuestion: Did Martins friends went fishing without martin. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Martin went to camp with his friends. Martin caught many fish so that everyone could eat.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martin went to camp with his friends. Martin caught many fish so that everyone could eat.", "hypotheses": ["Martins friends went fishing without martin.", "He was the best fisherman of the group."]}
{"example_id": "75", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did After the car wash Sam noticed his car seats were all soaking wet. happen?", "a": "no", "prior_probs": [0.44405487857142134, 0.5559451214285787], "posterior_probs": [0.6112216507022545, 0.3887783492977455], "prior_entropy": 0.6868743401477769, "posterior_entropy": 0.6681984897033622, "delta_entropy": 0.018675850444414777, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6112216507022545, "accuracy": 0, "tokens_in": 168, "tokens_out": 72, "tokens_total": 240, "latency_total": 0.01571303199943941, "latency_per_module": {"scorer": 0.007928764999633131, "prior": 0.0077842669998062775}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 36, "tokens_total": 144}, "prior": {"tokens_in": 60, "tokens_out": 36, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.\nQuestion: Did After the car wash Sam noticed his car seats were all soaking wet. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.", "hypotheses": ["After the car wash Sam noticed his car seats were all soaking wet.", "He rolled up all his windows and began daydreaming of how well his car would look after the wash."]}
{"example_id": "76", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did May sent out tweets looking for tickets. happen?", "a": "no", "prior_probs": [0.4737475754119093, 0.5262524245880907], "posterior_probs": [0.6188438713921466, 0.3811561286078534], "prior_entropy": 0.6917681669549507, "posterior_entropy": 0.6646272717223558, "delta_entropy": 0.027140895232594886, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6188438713921466, "accuracy": 0, "tokens_in": 188, "tokens_out": 30, "tokens_total": 218, "latency_total": 0.014779637999708939, "latency_per_module": {"scorer": 0.007109436000064306, "prior": 0.0076702019996446325}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 15, "tokens_total": 127}, "prior": {"tokens_in": 76, "tokens_out": 15, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!\nQuestion: Did May sent out tweets looking for tickets. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!", "hypotheses": ["May sent out tweets looking for tickets.", "So she bought 2 tickets online."]}
{"example_id": "77", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did When Mindy walked in the door, Jake was naked. happen?", "a": "no", "prior_probs": [0.486044932696252, 0.5139550673037481], "posterior_probs": [0.5926495053176007, 0.4073504946823992], "prior_entropy": 0.692757642168361, "posterior_entropy": 0.6758796990385001, "delta_entropy": 0.016877943129860906, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5926495053176007, "accuracy": 0, "tokens_in": 164, "tokens_out": 46, "tokens_total": 210, "latency_total": 0.014854823000860051, "latency_per_module": {"scorer": 0.007195631000286085, "prior": 0.007659192000573967}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}, "prior": {"tokens_in": 60, "tokens_out": 23, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mindy decided to go over jake's house. She panicked and ran screaming out of his house.\nQuestion: Did When Mindy walked in the door, Jake was naked. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mindy decided to go over jake's house. She panicked and ran screaming out of his house.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mindy decided to go over jake's house. She panicked and ran screaming out of his house.", "hypotheses": ["When Mindy walked in the door, Jake was naked.", "Mindy scared Jake when she came into the house."]}
{"example_id": "78", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The dog stole the slippers. happen?", "a": "no", "prior_probs": [0.4790031487503871, 0.5209968512496128], "posterior_probs": [0.5723299603901928, 0.42767003960980715], "prior_entropy": 0.6922651856976607, "posterior_entropy": 0.682647132103792, "delta_entropy": 0.009618053593868692, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5723299603901928, "accuracy": 0, "tokens_in": 146, "tokens_out": 28, "tokens_total": 174, "latency_total": 0.015301009999348025, "latency_per_module": {"scorer": 0.00787189299990132, "prior": 0.0074291169994467054}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 14, "tokens_total": 104}, "prior": {"tokens_in": 56, "tokens_out": 14, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was having trouble finding my comfortable slippers. Finders keepers, she told me.\nQuestion: Did The dog stole the slippers. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was having trouble finding my comfortable slippers. Finders keepers, she told me.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was having trouble finding my comfortable slippers. Finders keepers, she told me.", "hypotheses": ["The dog stole the slippers.", "My sister stole my slippers."]}
{"example_id": "79", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I used a marker to paint them bright pink. happen?", "a": "no", "prior_probs": [0.44394140244086727, 0.5560585975591328], "posterior_probs": [0.5793234224171974, 0.42067657758280264], "prior_entropy": 0.6868488135486308, "posterior_entropy": 0.6805094420031597, "delta_entropy": 0.006339371545471151, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5793234224171974, "accuracy": 0, "tokens_in": 180, "tokens_out": 52, "tokens_total": 232, "latency_total": 0.015629221999006404, "latency_per_module": {"scorer": 0.0076264649997028755, "prior": 0.008002756999303529}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 26, "tokens_total": 136}, "prior": {"tokens_in": 70, "tokens_out": 26, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.\nQuestion: Did I used a marker to paint them bright pink. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.", "hypotheses": ["I used a marker to paint them bright pink.", "I wanted to have normal lips. I painted them red and people liked it."]}
{"example_id": "80", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Their son was very well behaved. happen?", "a": "no", "prior_probs": [0.4492307602125571, 0.5507692397874429], "posterior_probs": [0.5824414098250077, 0.41755859017499225], "prior_entropy": 0.6879832542902835, "posterior_entropy": 0.6794917373061, "delta_entropy": 0.00849151698418349, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5824414098250077, "accuracy": 0, "tokens_in": 130, "tokens_out": 30, "tokens_total": 160, "latency_total": 0.019040698999560846, "latency_per_module": {"scorer": 0.01198277399998915, "prior": 0.007057924999571696}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 15, "tokens_total": 97}, "prior": {"tokens_in": 48, "tokens_out": 15, "tokens_total": 63}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A family went shopping together. The father bought the boy a new computer.\nQuestion: Did Their son was very well behaved. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A family went shopping together. The father bought the boy a new computer.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A family went shopping together. The father bought the boy a new computer.", "hypotheses": ["Their son was very well behaved.", "The boy needed a new cell phone."]}
{"example_id": "81", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?", "a": "no", "prior_probs": [0.47648300690444706, 0.5235169930955529], "posterior_probs": [0.6916962383653161, 0.3083037616346839], "prior_entropy": 0.6920406744505931, "posterior_entropy": 0.6177367389586781, "delta_entropy": 0.07430393549191505, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6916962383653161, "accuracy": 0, "tokens_in": 172, "tokens_out": 56, "tokens_total": 228, "latency_total": 0.0299486669991893, "latency_per_module": {"scorer": 0.016768687999501708, "prior": 0.013179978999687592}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 28, "tokens_total": 142}, "prior": {"tokens_in": 58, "tokens_out": 28, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was working on my laptop one day. After paying the bill, I no longer experienced issues.\nQuestion: Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was working on my laptop one day. After paying the bill, I no longer experienced issues.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was working on my laptop one day. After paying the bill, I no longer experienced issues.", "hypotheses": ["i got an e-mail saying my cable bill was current and service would be upgraded.", "The internet was very slow and then stopped completely."]}
{"example_id": "82", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Isa went to the country site and found some flowers. happen?", "a": "no", "prior_probs": [0.4668909333315746, 0.5331090666684254], "posterior_probs": [0.607676300037499, 0.39232369996250105], "prior_entropy": 0.6909531549137811, "posterior_entropy": 0.6697761658843993, "delta_entropy": 0.02117698902938181, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.607676300037499, "accuracy": 0, "tokens_in": 170, "tokens_out": 36, "tokens_total": 206, "latency_total": 0.025585771000805835, "latency_per_module": {"scorer": 0.011564936000468151, "prior": 0.014020835000337684}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 64, "tokens_out": 18, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!\nQuestion: Did Isa went to the country site and found some flowers. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!", "hypotheses": ["Isa went to the country site and found some flowers.", "Isa forgot about the bouquet."]}
{"example_id": "83", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did People destroyed the dam the beaver was building. happen?", "a": "no", "prior_probs": [0.4949569028203426, 0.5050430971796575], "posterior_probs": [0.6120771427496029, 0.38792285725039716], "prior_entropy": 0.6930963140371449, "posterior_entropy": 0.6678098817482663, "delta_entropy": 0.025286432288878613, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6120771427496029, "accuracy": 0, "tokens_in": 192, "tokens_out": 44, "tokens_total": 236, "latency_total": 0.029693432000385656, "latency_per_module": {"scorer": 0.01657285200053593, "prior": 0.013120579999849724}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 22, "tokens_total": 138}, "prior": {"tokens_in": 76, "tokens_out": 22, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.\nQuestion: Did People destroyed the dam the beaver was building. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.", "hypotheses": ["People destroyed the dam the beaver was building.", "Everyone was all over the hotel, trying to see him."]}
{"example_id": "84", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did We took a beautiful picture of Spain. happen?", "a": "no", "prior_probs": [0.48274284883258106, 0.5172571511674189], "posterior_probs": [0.5751762452410287, 0.42482375475897133], "prior_entropy": 0.6925514437149196, "posterior_entropy": 0.6818012696066166, "delta_entropy": 0.010750174108303034, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5751762452410287, "accuracy": 0, "tokens_in": 140, "tokens_out": 40, "tokens_total": 180, "latency_total": 0.03303678500014939, "latency_per_module": {"scorer": 0.015342498000791238, "prior": 0.017694286999358155}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 20, "tokens_total": 108}, "prior": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My family was on vacation in Italy. It was our favorite picture of the vacation.\nQuestion: Did We took a beautiful picture of Spain. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My family was on vacation in Italy. It was our favorite picture of the vacation.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My family was on vacation in Italy. It was our favorite picture of the vacation.", "hypotheses": ["We took a beautiful picture of Spain.", "We took a photo next to the coliseum in Rome."]}
{"example_id": "85", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did bill said he would be fine and left. happen?", "a": "no", "prior_probs": [0.5220601301779403, 0.47793986982205966], "posterior_probs": [0.6299223516894003, 0.3700776483105997], "prior_entropy": 0.6921735658547711, "posterior_entropy": 0.6589969934851203, "delta_entropy": 0.03317657236965077, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6299223516894003, "accuracy": 0, "tokens_in": 166, "tokens_out": 34, "tokens_total": 200, "latency_total": 0.030713703999936115, "latency_per_module": {"scorer": 0.015349223999692185, "prior": 0.01536448000024393}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 17, "tokens_total": 119}, "prior": {"tokens_in": 64, "tokens_out": 17, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.\nQuestion: Did bill said he would be fine and left. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.", "hypotheses": ["bill said he would be fine and left.", "Bill went to Lansing via air anyways."]}
{"example_id": "86", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Liv's mother signed her up. happen?", "a": "no", "prior_probs": [0.4965328533869331, 0.5034671466130668], "posterior_probs": [0.5902057955578565, 0.4097942044421436], "prior_entropy": 0.6931231381539927, "posterior_entropy": 0.6767835567307212, "delta_entropy": 0.01633958142327152, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5902057955578565, "accuracy": 0, "tokens_in": 130, "tokens_out": 28, "tokens_total": 158, "latency_total": 0.031563852999170194, "latency_per_module": {"scorer": 0.01608557599956839, "prior": 0.015478276999601803}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 14, "tokens_total": 96}, "prior": {"tokens_in": 48, "tokens_out": 14, "tokens_total": 62}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!\nQuestion: Did Liv's mother signed her up. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!", "hypotheses": ["Liv's mother signed her up.", "She was not very talented."]}
{"example_id": "87", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did One day a nurse said they should go to the coffee shop for a treat. happen?", "a": "no", "prior_probs": [0.48536694808139563, 0.5146330519186044], "posterior_probs": [0.6529475111829282, 0.34705248881707185], "prior_entropy": 0.6927188669867086, "posterior_entropy": 0.6456028950197156, "delta_entropy": 0.04711597196699302, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6529475111829282, "accuracy": 0, "tokens_in": 192, "tokens_out": 56, "tokens_total": 248, "latency_total": 0.028590124000402284, "latency_per_module": {"scorer": 0.013567381000029854, "prior": 0.01502274300037243}, "tokens_per_module": {"scorer": {"tokens_in": 122, "tokens_out": 28, "tokens_total": 150}, "prior": {"tokens_in": 70, "tokens_out": 28, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!\nQuestion: Did One day a nurse said they should go to the coffee shop for a treat. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!", "hypotheses": ["One day a nurse said they should go to the coffee shop for a treat.", "The baseball player looked out the window at a coffee shop."]}
{"example_id": "88", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jake ended up getting free from the mud. happen?", "a": "no", "prior_probs": [0.500141024585799, 0.49985897541420105], "posterior_probs": [0.6109633844449505, 0.38903661555504954], "prior_entropy": 0.6931471407820773, "posterior_entropy": 0.6683152020064588, "delta_entropy": 0.02483193877561851, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6109633844449505, "accuracy": 0, "tokens_in": 134, "tokens_out": 32, "tokens_total": 166, "latency_total": 0.016464457999973092, "latency_per_module": {"scorer": 0.008335958999850845, "prior": 0.008128499000122247}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 16, "tokens_total": 102}, "prior": {"tokens_in": 48, "tokens_out": 16, "tokens_total": 64}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jake was off roading. He had to get help to get out.\nQuestion: Did Jake ended up getting free from the mud. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jake was off roading. He had to get help to get out.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake was off roading. He had to get help to get out.", "hypotheses": ["Jake ended up getting free from the mud.", "Jake got stuff in the mud."]}
{"example_id": "89", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did It was so fun, I was a clown. happen?", "a": "no", "prior_probs": [0.4483684781669959, 0.551631521833004], "posterior_probs": [0.6041421385104238, 0.39585786148957625], "prior_entropy": 0.6878060363970406, "posterior_entropy": 0.6712963883434058, "delta_entropy": 0.016509648053634862, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6041421385104238, "accuracy": 0, "tokens_in": 140, "tokens_out": 38, "tokens_total": 178, "latency_total": 0.016745873000218126, "latency_per_module": {"scorer": 0.008781194000221149, "prior": 0.007964678999996977}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 19, "tokens_total": 109}, "prior": {"tokens_in": 50, "tokens_out": 19, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I got a new racing game yesterday. Finally after hours of playing I stopped.\nQuestion: Did It was so fun, I was a clown. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I got a new racing game yesterday. Finally after hours of playing I stopped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I got a new racing game yesterday. Finally after hours of playing I stopped.", "hypotheses": ["It was so fun, I was a clown.", "I sat down to test out the game."]}
{"example_id": "90", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She doesn't ask nicely to get things indefinite. happen?", "a": "no", "prior_probs": [0.3680791136694201, 0.63192088633058], "posterior_probs": [0.5439159007972553, 0.45608409920274473], "prior_entropy": 0.6579254323609798, "posterior_entropy": 0.6892849931420373, "delta_entropy": -0.031359560781057505, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5439159007972553, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.02173151199986023, "latency_per_module": {"scorer": 0.012942946000293887, "prior": 0.008788565999566345}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Freda is the boss of her office. Freda can't understand why people have a problem with her!\nQuestion: Did She doesn't ask nicely to get things indefinite. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Freda is the boss of her office. Freda can't understand why people have a problem with her!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Freda is the boss of her office. Freda can't understand why people have a problem with her!", "hypotheses": ["She doesn't ask nicely to get things indefinite.", "She doesn't ask nicely to get things done."]}
{"example_id": "91", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did One of them had a bad cough. happen?", "a": "no", "prior_probs": [0.5296516170952371, 0.4703483829047629], "posterior_probs": [0.6103227199271737, 0.38967728007282637], "prior_entropy": 0.6913877116125613, "posterior_entropy": 0.668603511370976, "delta_entropy": 0.02278420024158534, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6103227199271737, "accuracy": 0, "tokens_in": 184, "tokens_out": 40, "tokens_total": 224, "latency_total": 0.01673215800019534, "latency_per_module": {"scorer": 0.008249716000136686, "prior": 0.008482442000058654}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 20, "tokens_total": 130}, "prior": {"tokens_in": 74, "tokens_out": 20, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.\nQuestion: Did One of them had a bad cough. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.", "hypotheses": ["One of them had a bad cough.", "Carly, noticed her daughter had gotten into Poison Ivy."]}
{"example_id": "92", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Alexis made sure the tree wasn't under 20 feet tall. happen?", "a": "no", "prior_probs": [0.48982945320701626, 0.5101705467929837], "posterior_probs": [0.5861605045654602, 0.41383949543453974], "prior_entropy": 0.6929402862449963, "posterior_entropy": 0.6782255481044289, "delta_entropy": 0.01471473814056734, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5861605045654602, "accuracy": 0, "tokens_in": 184, "tokens_out": 46, "tokens_total": 230, "latency_total": 0.016565268000704236, "latency_per_module": {"scorer": 0.008326656999997795, "prior": 0.008238611000706442}, "tokens_per_module": {"scorer": {"tokens_in": 114, "tokens_out": 23, "tokens_total": 137}, "prior": {"tokens_in": 70, "tokens_out": 23, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.\nQuestion: Did Alexis made sure the tree wasn't under 20 feet tall. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.", "hypotheses": ["Alexis made sure the tree wasn't under 20 feet tall.", "Alexis was worried it would be too big."]}
{"example_id": "93", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Francine decided to go to school to pursue her dreams. happen?", "a": "no", "prior_probs": [0.6592424657664189, 0.3407575342335811], "posterior_probs": [0.6563665991731485, 0.34363340082685134], "prior_entropy": 0.6415366677654246, "posterior_entropy": 0.6434161266894474, "delta_entropy": -0.0018794589240227477, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6563665991731485, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.01575399000012112, "latency_per_module": {"scorer": 0.007808178000232147, "prior": 0.007945811999888974}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.\nQuestion: Did Francine decided to go to school to pursue her dreams. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.", "hypotheses": ["Francine decided to go to school to pursue her dreams.", "francine applied to business school."]}
{"example_id": "94", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jack caught a ball that bounced over the fence. happen?", "a": "no", "prior_probs": [0.5440713882680684, 0.45592861173193144], "posterior_probs": [0.6289758792893719, 0.37102412071062807], "prior_entropy": 0.6892575603602893, "posterior_entropy": 0.6594984860689397, "delta_entropy": 0.029759074291349608, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6289758792893719, "accuracy": 0, "tokens_in": 180, "tokens_out": 38, "tokens_total": 218, "latency_total": 0.015572074000374414, "latency_per_module": {"scorer": 0.007622193000315747, "prior": 0.007949881000058667}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 19, "tokens_total": 129}, "prior": {"tokens_in": 70, "tokens_out": 19, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.\nQuestion: Did Jack caught a ball that bounced over the fence. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.", "hypotheses": ["Jack caught a ball that bounced over the fence.", "Jack saw how wild the crowd was getting."]}
{"example_id": "95", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Nathan never got into a fight. happen?", "a": "no", "prior_probs": [0.5983544817378198, 0.40164551826218015], "posterior_probs": [0.6266960741546074, 0.3733039258453927], "prior_entropy": 0.6736732287085543, "posterior_entropy": 0.660690699410478, "delta_entropy": 0.012982529298076306, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6266960741546074, "accuracy": 0, "tokens_in": 154, "tokens_out": 30, "tokens_total": 184, "latency_total": 0.016448424000373052, "latency_per_module": {"scorer": 0.008364338999854226, "prior": 0.008084085000518826}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 15, "tokens_total": 109}, "prior": {"tokens_in": 60, "tokens_out": 15, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.\nQuestion: Did Nathan never got into a fight. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.", "hypotheses": ["Nathan never got into a fight.", "Nathan got detention in school."]}
{"example_id": "96", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Mike found pasta hard to make. happen?", "a": "no", "prior_probs": [0.6835728746419494, 0.3164271253580506], "posterior_probs": [0.7071798178427685, 0.29282018215723155], "prior_entropy": 0.624146934819759, "posterior_entropy": 0.6046575516987129, "delta_entropy": 0.01948938312104609, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7071798178427685, "accuracy": 0, "tokens_in": 146, "tokens_out": 28, "tokens_total": 174, "latency_total": 0.02308739899945067, "latency_per_module": {"scorer": 0.012446402000023227, "prior": 0.010640996999427443}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 14, "tokens_total": 104}, "prior": {"tokens_in": 56, "tokens_out": 14, "tokens_total": 70}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.\nQuestion: Did Mike found pasta hard to make. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.", "hypotheses": ["Mike found pasta hard to make.", "Mile loves italian food."]}
{"example_id": "97", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Randy knew the area well. happen?", "a": "no", "prior_probs": [0.5251973619984914, 0.47480263800150857], "posterior_probs": [0.5531787189913274, 0.4468212810086726], "prior_entropy": 0.6918768284318874, "posterior_entropy": 0.6874805164421323, "delta_entropy": 0.004396311989755097, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5531787189913274, "accuracy": 0, "tokens_in": 156, "tokens_out": 36, "tokens_total": 192, "latency_total": 0.023025055998914468, "latency_per_module": {"scorer": 0.011674165999465913, "prior": 0.011350889999448555}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.\nQuestion: Did Randy knew the area well. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.", "hypotheses": ["Randy knew the area well.", "He didn't know that part of town very good."]}
{"example_id": "98", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Cat sent a love note to the boy. happen?", "a": "no", "prior_probs": [0.4126552520352513, 0.5873447479647488], "posterior_probs": [0.5593052674525093, 0.4406947325474907], "prior_entropy": 0.6778104031708004, "posterior_entropy": 0.6860963640804533, "delta_entropy": -0.008285960909652834, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5593052674525093, "accuracy": 0, "tokens_in": 166, "tokens_out": 36, "tokens_total": 202, "latency_total": 0.021600031999696512, "latency_per_module": {"scorer": 0.010852517999410338, "prior": 0.010747514000286174}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 18, "tokens_total": 120}, "prior": {"tokens_in": 64, "tokens_out": 18, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!\nQuestion: Did Cat sent a love note to the boy. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!", "hypotheses": ["Cat sent a love note to the boy.", "She told him she did not like him."]}
{"example_id": "99", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Karen missed Lacy so much, she couldn't bear not talking to her friend. happen?", "a": "no", "prior_probs": [0.5060554166199094, 0.49394458338009056], "posterior_probs": [0.5292023292396653, 0.4707976707603348], "prior_entropy": 0.693073842624229, "posterior_entropy": 0.6914406575305173, "delta_entropy": 0.0016331850937116865, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5292023292396653, "accuracy": 0, "tokens_in": 166, "tokens_out": 70, "tokens_total": 236, "latency_total": 0.02490564199979417, "latency_per_module": {"scorer": 0.011289367999779643, "prior": 0.013616274000014528}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 35, "tokens_total": 145}, "prior": {"tokens_in": 56, "tokens_out": 35, "tokens_total": 91}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lacy and Karen got in a fight. Karen apologized too so they could be friends again.\nQuestion: Did Karen missed Lacy so much, she couldn't bear not talking to her friend. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lacy and Karen got in a fight. Karen apologized too so they could be friends again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lacy and Karen got in a fight. Karen apologized too so they could be friends again.", "hypotheses": ["Karen missed Lacy so much, she couldn't bear not talking to her friend.", "Lacy missed Karen so much, she couldn't bear not talking to her friend."]}
{"example_id": "100", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did My friend told us he had cancer and was expected to die in a week. happen?", "a": "no", "prior_probs": [0.6252029901029774, 0.3747970098970227], "posterior_probs": [0.6939831178540657, 0.30601688214593425], "prior_entropy": 0.6614594576997013, "posterior_entropy": 0.6158765210874091, "delta_entropy": 0.04558293661229218, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6939831178540657, "accuracy": 0, "tokens_in": 180, "tokens_out": 50, "tokens_total": 230, "latency_total": 0.0235326250003709, "latency_per_module": {"scorer": 0.011762246000216692, "prior": 0.011770379000154207}, "tokens_per_module": {"scorer": {"tokens_in": 116, "tokens_out": 25, "tokens_total": 141}, "prior": {"tokens_in": 64, "tokens_out": 25, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My friend had an announcement to make. So, I put on a smile and wished him the best of luck.\nQuestion: Did My friend told us he had cancer and was expected to die in a week. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My friend had an announcement to make. So, I put on a smile and wished him the best of luck.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend had an announcement to make. So, I put on a smile and wished him the best of luck.", "hypotheses": ["My friend told us he had cancer and was expected to die in a week.", "my friend usually talks about some business deal."]}
{"example_id": "101", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did George trained hard for two days before the fight. happen?", "a": "no", "prior_probs": [0.5037615781635802, 0.49623842183641964], "posterior_probs": [0.5963461437731568, 0.40365385622684324], "prior_entropy": 0.6931188813504345, "posterior_entropy": 0.6744653922613348, "delta_entropy": 0.018653489089099695, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5963461437731568, "accuracy": 0, "tokens_in": 160, "tokens_out": 54, "tokens_total": 214, "latency_total": 0.01771968499997456, "latency_per_module": {"scorer": 0.007925806999992346, "prior": 0.009793877999982215}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 27, "tokens_total": 127}, "prior": {"tokens_in": 60, "tokens_out": 27, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: George was about to participate in his first professional fight. George proved his skills and won his first match.\nQuestion: Did George trained hard for two days before the fight. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: George was about to participate in his first professional fight. George proved his skills and won his first match.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "George was about to participate in his first professional fight. George proved his skills and won his first match.", "hypotheses": ["George trained hard for two days before the fight.", "George was the underdog, but he had been training months to compete in this event."]}
{"example_id": "102", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The team that Bob and his kids like won. happen?", "a": "no", "prior_probs": [0.6479163582241244, 0.3520836417758756], "posterior_probs": [0.7038117543767948, 0.29618824562320534], "prior_entropy": 0.6487269617884174, "posterior_entropy": 0.607599932314811, "delta_entropy": 0.04112702947360636, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7038117543767948, "accuracy": 0, "tokens_in": 152, "tokens_out": 36, "tokens_total": 188, "latency_total": 0.015301886000088416, "latency_per_module": {"scorer": 0.007423721999657573, "prior": 0.007878164000430843}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}, "prior": {"tokens_in": 56, "tokens_out": 18, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Bob and his kids love football. Bob and his kids share a hug to celebrate the win.\nQuestion: Did The team that Bob and his kids like won. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Bob and his kids love football. Bob and his kids share a hug to celebrate the win.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob and his kids love football. Bob and his kids share a hug to celebrate the win.", "hypotheses": ["The team that Bob and his kids like won.", "They played basketball out back all afternoon."]}
{"example_id": "103", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He didn't see the toy he really wanted in any of the aisles. happen?", "a": "no", "prior_probs": [0.5873619102187421, 0.412638089781258], "posterior_probs": [0.688925036893191, 0.311074963106809], "prior_entropy": 0.67780434429697, "posterior_entropy": 0.6199580637323454, "delta_entropy": 0.0578462805646246, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.688925036893191, "accuracy": 0, "tokens_in": 170, "tokens_out": 50, "tokens_total": 220, "latency_total": 0.015273709000211966, "latency_per_module": {"scorer": 0.007719876999544795, "prior": 0.007553832000667171}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 25, "tokens_total": 137}, "prior": {"tokens_in": 58, "tokens_out": 25, "tokens_total": 83}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Alex was at target with his mom. He begged his mother to buy it until she gave in.\nQuestion: Did He didn't see the toy he really wanted in any of the aisles. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Alex was at target with his mom. He begged his mother to buy it until she gave in.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alex was at target with his mom. He begged his mother to buy it until she gave in.", "hypotheses": ["He didn't see the toy he really wanted in any of the aisles.", "Alex saw a game he really wanted."]}
{"example_id": "104", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Nova applied to a few dance schools but was denied by her first choice. happen?", "a": "no", "prior_probs": [0.6404063862747398, 0.3595936137252602], "posterior_probs": [0.6551086188003145, 0.34489138119968565], "prior_entropy": 0.6531840162431645, "posterior_entropy": 0.6442267151415673, "delta_entropy": 0.008957301101597204, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6551086188003145, "accuracy": 0, "tokens_in": 146, "tokens_out": 48, "tokens_total": 194, "latency_total": 0.014760610000848828, "latency_per_module": {"scorer": 0.0074117330004810356, "prior": 0.007348877000367793}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 24, "tokens_total": 122}, "prior": {"tokens_in": 48, "tokens_out": 24, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nova dreamed of being a professional dancer. Nova's second choice accepted her.\nQuestion: Did Nova applied to a few dance schools but was denied by her first choice. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nova dreamed of being a professional dancer. Nova's second choice accepted her.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nova dreamed of being a professional dancer. Nova's second choice accepted her.", "hypotheses": ["Nova applied to a few dance schools but was denied by her first choice.", "Nova applied to one dance school."]}
{"example_id": "105", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did An unexpected event happened when the tide didn't come in that day. happen?", "a": "no", "prior_probs": [0.45470671903037124, 0.5452932809696288], "posterior_probs": [0.6297319693761745, 0.3702680306238256], "prior_entropy": 0.6890385880217316, "posterior_entropy": 0.6590981770031632, "delta_entropy": 0.02994041101856837, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6297319693761745, "accuracy": 0, "tokens_in": 168, "tokens_out": 48, "tokens_total": 216, "latency_total": 0.015303149999454035, "latency_per_module": {"scorer": 0.007673910999983491, "prior": 0.0076292389994705445}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 24, "tokens_total": 132}, "prior": {"tokens_in": 60, "tokens_out": 24, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.\nQuestion: Did An unexpected event happened when the tide didn't come in that day. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.", "hypotheses": ["An unexpected event happened when the tide didn't come in that day.", "The sand castle was built right on the shore."]}
{"example_id": "106", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She wanted to be less involved and lose friends. happen?", "a": "no", "prior_probs": [0.43206894712477, 0.5679310528752299], "posterior_probs": [0.5932443470251968, 0.40675565297480315], "prior_entropy": 0.6838893198791889, "posterior_entropy": 0.6756559425715869, "delta_entropy": 0.008233377307601986, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5932443470251968, "accuracy": 0, "tokens_in": 184, "tokens_out": 36, "tokens_total": 220, "latency_total": 0.016216368000641523, "latency_per_module": {"scorer": 0.0075666020002245205, "prior": 0.008649766000417003}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 18, "tokens_total": 130}, "prior": {"tokens_in": 72, "tokens_out": 18, "tokens_total": 90}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.\nQuestion: Did She wanted to be less involved and lose friends. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.", "hypotheses": ["She wanted to be less involved and lose friends.", "Tami was tall for her age."]}
{"example_id": "107", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Donald is a selfless, wonderful person. happen?", "a": "no", "prior_probs": [0.4372740685760933, 0.5627259314239066], "posterior_probs": [0.5742594267861512, 0.42574057321384884], "prior_entropy": 0.6852573237333645, "posterior_entropy": 0.6820773478694779, "delta_entropy": 0.0031799758638865194, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5742594267861512, "accuracy": 0, "tokens_in": 122, "tokens_out": 38, "tokens_total": 160, "latency_total": 0.015461349000361224, "latency_per_module": {"scorer": 0.008310743000038201, "prior": 0.007150606000323023}, "tokens_per_module": {"scorer": {"tokens_in": 80, "tokens_out": 19, "tokens_total": 99}, "prior": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Donald is running for president. Hopefully he loses the election.\nQuestion: Did Donald is a selfless, wonderful person. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Donald is running for president. Hopefully he loses the election.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Donald is running for president. Hopefully he loses the election.", "hypotheses": ["Donald is a selfless, wonderful person.", "Donald is not the candidate I want for president."]}
{"example_id": "108", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I had rope that I used for jumping at home. happen?", "a": "no", "prior_probs": [0.5228436593353636, 0.4771563406646364], "posterior_probs": [0.6700584353828868, 0.32994156461711316], "prior_entropy": 0.6921031516315433, "posterior_entropy": 0.6341372449229019, "delta_entropy": 0.057965906708641435, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6700584353828868, "accuracy": 0, "tokens_in": 154, "tokens_out": 42, "tokens_total": 196, "latency_total": 0.02850094000041281, "latency_per_module": {"scorer": 0.01423772199996165, "prior": 0.014263218000451161}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 21, "tokens_total": 119}, "prior": {"tokens_in": 56, "tokens_out": 21, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was very out of shape. After weeks of jumping rope, I began to feel excellent.\nQuestion: Did I had rope that I used for jumping at home. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was very out of shape. After weeks of jumping rope, I began to feel excellent.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was very out of shape. After weeks of jumping rope, I began to feel excellent.", "hypotheses": ["I had rope that I used for jumping at home.", "I committed to exercise every month by jumping rope."]}
{"example_id": "109", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ora's family needed to use more gas. happen?", "a": "no", "prior_probs": [0.47234653950590905, 0.527653460494091], "posterior_probs": [0.5361633134794949, 0.4638366865205051], "prior_entropy": 0.6916169721313477, "posterior_entropy": 0.6905293248862685, "delta_entropy": 0.0010876472450791486, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5361633134794949, "accuracy": 0, "tokens_in": 176, "tokens_out": 54, "tokens_total": 230, "latency_total": 0.028980333000617975, "latency_per_module": {"scorer": 0.014440586000091571, "prior": 0.014539747000526404}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 27, "tokens_total": 135}, "prior": {"tokens_in": 68, "tokens_out": 27, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.\nQuestion: Did Ora's family needed to use more gas. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.", "hypotheses": ["Ora's family needed to use more gas.", "Ora's mom kicked her out of the house so their gas bill would reduce."]}
{"example_id": "110", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The puppy was given to me by a stork. happen?", "a": "no", "prior_probs": [0.5043134215278807, 0.4956865784721193], "posterior_probs": [0.598330244325901, 0.40166975567409896], "prior_entropy": 0.6931099688858192, "posterior_entropy": 0.6736828888448205, "delta_entropy": 0.01942708004099869, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.598330244325901, "accuracy": 0, "tokens_in": 202, "tokens_out": 48, "tokens_total": 250, "latency_total": 0.030494327999804227, "latency_per_module": {"scorer": 0.01532178399975237, "prior": 0.015172544000051857}, "tokens_per_module": {"scorer": {"tokens_in": 122, "tokens_out": 24, "tokens_total": 146}, "prior": {"tokens_in": 80, "tokens_out": 24, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.\nQuestion: Did The puppy was given to me by a stork. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.", "hypotheses": ["The puppy was given to me by a stork.", "I got up and tried to find out why they kept barking."]}
{"example_id": "111", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Dan ate a lot of food very slowly, hoping to win. happen?", "a": "no", "prior_probs": [0.5925861323348321, 0.4074138676651678], "posterior_probs": [0.6746455779099958, 0.3253544220900042], "prior_entropy": 0.6759034511018422, "posterior_entropy": 0.6308397848142369, "delta_entropy": 0.045063666287605386, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6746455779099958, "accuracy": 0, "tokens_in": 166, "tokens_out": 44, "tokens_total": 210, "latency_total": 0.02978933199938183, "latency_per_module": {"scorer": 0.014772341000025335, "prior": 0.015016990999356494}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 22, "tokens_total": 128}, "prior": {"tokens_in": 60, "tokens_out": 22, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.\nQuestion: Did Dan ate a lot of food very slowly, hoping to win. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.", "hypotheses": ["Dan ate a lot of food very slowly, hoping to win.", "Dan tried to eat 30 cold hot dogs."]}
{"example_id": "112", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Melissa was being rude. happen?", "a": "no", "prior_probs": [0.5464544162805514, 0.4535455837194486], "posterior_probs": [0.5784649224119455, 0.42153507758805453], "prior_entropy": 0.688824924075583, "posterior_entropy": 0.6807826472604681, "delta_entropy": 0.008042276815114935, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5784649224119455, "accuracy": 0, "tokens_in": 150, "tokens_out": 40, "tokens_total": 190, "latency_total": 0.02681494899934478, "latency_per_module": {"scorer": 0.013899018999836699, "prior": 0.01291592999950808}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.\nQuestion: Did Melissa was being rude. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.", "hypotheses": ["Melissa was being rude.", "Melissa's friend insisted they go out to eat somewhere Melissa hated."]}
{"example_id": "113", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Nell studied biology to draw an animal. happen?", "a": "no", "prior_probs": [0.4783050226856938, 0.5216949773143061], "posterior_probs": [0.5772547440568948, 0.42274525594310514], "prior_entropy": 0.6922055408777172, "posterior_entropy": 0.6811626361097147, "delta_entropy": 0.011042904768002515, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5772547440568948, "accuracy": 0, "tokens_in": 174, "tokens_out": 36, "tokens_total": 210, "latency_total": 0.02853736400084017, "latency_per_module": {"scorer": 0.013576547000411665, "prior": 0.014960817000428506}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 68, "tokens_out": 18, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.\nQuestion: Did Nell studied biology to draw an animal. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.", "hypotheses": ["Nell studied biology to draw an animal.", "Nell was told to do something unexpected."]}
{"example_id": "114", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did His dad asked Eli how to tie his shoes. happen?", "a": "no", "prior_probs": [0.527262563582004, 0.47273743641799604], "posterior_probs": [0.5579518583675479, 0.4420481416324521], "prior_entropy": 0.69165994837897, "posterior_entropy": 0.6864152247577697, "delta_entropy": 0.00524472362120032, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5579518583675479, "accuracy": 0, "tokens_in": 152, "tokens_out": 40, "tokens_total": 192, "latency_total": 0.02942032000009931, "latency_per_module": {"scorer": 0.013764301000264823, "prior": 0.015656018999834487}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 20, "tokens_total": 116}, "prior": {"tokens_in": 56, "tokens_out": 20, "tokens_total": 76}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.\nQuestion: Did His dad asked Eli how to tie his shoes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.", "hypotheses": ["His dad asked Eli how to tie his shoes.", "eli asked his dad how to tie his shoes."]}
{"example_id": "115", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She was anxious to buy some today. happen?", "a": "no", "prior_probs": [0.48110675756754917, 0.5188932424324508], "posterior_probs": [0.6185057717387646, 0.38149422826123536], "prior_entropy": 0.6924331013528291, "posterior_entropy": 0.6647908873843817, "delta_entropy": 0.02764221396844735, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6185057717387646, "accuracy": 0, "tokens_in": 172, "tokens_out": 36, "tokens_total": 208, "latency_total": 0.015371384999525617, "latency_per_module": {"scorer": 0.007356727999649593, "prior": 0.008014656999876024}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}, "prior": {"tokens_in": 68, "tokens_out": 18, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.\nQuestion: Did She was anxious to buy some today. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.", "hypotheses": ["She was anxious to buy some today.", "Francine saw bananas for sale at the fair."]}
{"example_id": "116", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ben spent hours sitting in the sun. happen?", "a": "no", "prior_probs": [0.47096512638183935, 0.5290348736181606], "posterior_probs": [0.5529913716988529, 0.447008628301147], "prior_entropy": 0.6914601839197501, "posterior_entropy": 0.6875204482928003, "delta_entropy": 0.003939735626949714, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5529913716988529, "accuracy": 0, "tokens_in": 148, "tokens_out": 44, "tokens_total": 192, "latency_total": 0.015399669000544236, "latency_per_module": {"scorer": 0.0076668050005537225, "prior": 0.0077328639999905135}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 22, "tokens_total": 114}, "prior": {"tokens_in": 56, "tokens_out": 22, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ben went to the beach on a sunny day. Ben crawled into his tent and napped.\nQuestion: Did Ben spent hours sitting in the sun. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ben went to the beach on a sunny day. Ben crawled into his tent and napped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ben went to the beach on a sunny day. Ben crawled into his tent and napped.", "hypotheses": ["Ben spent hours sitting in the sun.", "Ben pitched a large tent on the sand to block out the sun."]}
{"example_id": "117", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tili ran for the prison gate one night. happen?", "a": "no", "prior_probs": [0.6605205014879388, 0.33947949851206116], "posterior_probs": [0.7161064630653952, 0.2838935369346048], "prior_entropy": 0.6406896284443233, "posterior_entropy": 0.5965931208986868, "delta_entropy": 0.04409650754563654, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7161064630653952, "accuracy": 0, "tokens_in": 148, "tokens_out": 30, "tokens_total": 178, "latency_total": 0.01425751599981595, "latency_per_module": {"scorer": 0.007043726999654609, "prior": 0.007213789000161341}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 15, "tokens_total": 109}, "prior": {"tokens_in": 54, "tokens_out": 15, "tokens_total": 69}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.\nQuestion: Did Tili ran for the prison gate one night. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.", "hypotheses": ["Tili ran for the prison gate one night.", "Doug formulated a plan."]}
{"example_id": "118", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Sam lost his job and could not make the payment. happen?", "a": "no", "prior_probs": [0.47853895165243243, 0.5214610483475676], "posterior_probs": [0.571836827637379, 0.428163172362621], "prior_entropy": 0.6922257443158863, "posterior_entropy": 0.6827903163881975, "delta_entropy": 0.009435427927688811, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.571836827637379, "accuracy": 0, "tokens_in": 166, "tokens_out": 46, "tokens_total": 212, "latency_total": 0.013341449000108696, "latency_per_module": {"scorer": 0.006485203000011097, "prior": 0.006856246000097599}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}, "prior": {"tokens_in": 62, "tokens_out": 23, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.\nQuestion: Did Sam lost his job and could not make the payment. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.", "hypotheses": ["Sam lost his job and could not make the payment.", "Sam decided to work two jobs to pay off his debt."]}
{"example_id": "119", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The kitchen got so bad mold might grow. happen?", "a": "no", "prior_probs": [0.36083121223098397, 0.639168787769016], "posterior_probs": [0.6034664948670486, 0.39653350513295144], "prior_entropy": 0.6538949456312322, "posterior_entropy": 0.6715810653850098, "delta_entropy": -0.01768611975377765, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6034664948670486, "accuracy": 0, "tokens_in": 162, "tokens_out": 38, "tokens_total": 200, "latency_total": 0.013792192999062536, "latency_per_module": {"scorer": 0.007138690999454411, "prior": 0.006653501999608125}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 19, "tokens_total": 119}, "prior": {"tokens_in": 62, "tokens_out": 19, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.\nQuestion: Did The kitchen got so bad mold might grow. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.", "hypotheses": ["The kitchen got so bad mold might grow.", "Katie needed to hurry and get it clean."]}
{"example_id": "120", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The directions took Randy thru a great part of town. happen?", "a": "no", "prior_probs": [0.47581760802240475, 0.5241823919775953], "posterior_probs": [0.6577187940367402, 0.3422812059632599], "prior_entropy": 0.6919771479977935, "posterior_entropy": 0.6425370059674315, "delta_entropy": 0.049440142030362066, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6577187940367402, "accuracy": 0, "tokens_in": 174, "tokens_out": 40, "tokens_total": 214, "latency_total": 0.012926913999763201, "latency_per_module": {"scorer": 0.006126031999883708, "prior": 0.006800881999879493}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 20, "tokens_total": 128}, "prior": {"tokens_in": 66, "tokens_out": 20, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.\nQuestion: Did The directions took Randy thru a great part of town. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.", "hypotheses": ["The directions took Randy thru a great part of town.", "The house had boarded windows and looked bad."]}
{"example_id": "121", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I was worried about how to tow. happen?", "a": "no", "prior_probs": [0.41262664839655694, 0.5873733516034432], "posterior_probs": [0.5133367128800578, 0.4866632871199422], "prior_entropy": 0.6778003044142535, "posterior_entropy": 0.6927914025423878, "delta_entropy": -0.014991098128134261, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5133367128800578, "accuracy": 0, "tokens_in": 180, "tokens_out": 42, "tokens_total": 222, "latency_total": 0.014790297000217834, "latency_per_module": {"scorer": 0.0074020720003318274, "prior": 0.007388224999886006}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 21, "tokens_total": 129}, "prior": {"tokens_in": 72, "tokens_out": 21, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!\nQuestion: Did I was worried about how to tow. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!", "hypotheses": ["I was worried about how to tow.", "I called my insurance company to see if I could get assistance."]}
{"example_id": "122", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tia had dinner with her parents. happen?", "a": "no", "prior_probs": [0.5599508886754255, 0.4400491113245745], "posterior_probs": [0.5905579070366755, 0.40944209296332457], "prior_entropy": 0.6859416391441366, "posterior_entropy": 0.67665484443315, "delta_entropy": 0.00928679471098659, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5905579070366755, "accuracy": 0, "tokens_in": 176, "tokens_out": 36, "tokens_total": 212, "latency_total": 0.013861852999070834, "latency_per_module": {"scorer": 0.006640310999500798, "prior": 0.007221541999570036}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 70, "tokens_out": 18, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.\nQuestion: Did Tia had dinner with her parents. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.", "hypotheses": ["Tia had dinner with her parents.", "Tia thought something seemed good between her parents."]}
{"example_id": "123", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Joe tripped down the stairs with his shoes untied. happen?", "a": "no", "prior_probs": [0.554069221489842, 0.44593077851015805], "posterior_probs": [0.616268266930562, 0.3837317330694381], "prior_entropy": 0.6872887698399445, "posterior_entropy": 0.6658614729796433, "delta_entropy": 0.021427296860301204, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.616268266930562, "accuracy": 0, "tokens_in": 196, "tokens_out": 42, "tokens_total": 238, "latency_total": 0.019700861000274017, "latency_per_module": {"scorer": 0.01105876499968872, "prior": 0.008642096000585298}, "tokens_per_module": {"scorer": {"tokens_in": 120, "tokens_out": 21, "tokens_total": 141}, "prior": {"tokens_in": 76, "tokens_out": 21, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.\nQuestion: Did Joe tripped down the stairs with his shoes untied. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.", "hypotheses": ["Joe tripped down the stairs with his shoes untied.", "Joe tied them and fell down the stairs."]}
{"example_id": "124", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Nita practiced playing rummy with a dog. happen?", "a": "no", "prior_probs": [0.4648569698295527, 0.5351430301704473], "posterior_probs": [0.5494695799517437, 0.45053042004825633], "prior_entropy": 0.6906750776487804, "posterior_entropy": 0.688244685137449, "delta_entropy": 0.0024303925113314806, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5494695799517437, "accuracy": 0, "tokens_in": 172, "tokens_out": 40, "tokens_total": 212, "latency_total": 0.01838492699971539, "latency_per_module": {"scorer": 0.00933744200028741, "prior": 0.00904748499942798}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 20, "tokens_total": 126}, "prior": {"tokens_in": 66, "tokens_out": 20, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.\nQuestion: Did Nita practiced playing rummy with a dog. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.", "hypotheses": ["Nita practiced playing rummy with a dog.", "Nita was never able to beat her Dad."]}
{"example_id": "125", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Fred saw a child come over with a pin. happen?", "a": "no", "prior_probs": [0.5531757724693236, 0.44682422753067647], "posterior_probs": [0.6776444487220556, 0.32235555127794435], "prior_entropy": 0.6874811455731205, "posterior_entropy": 0.6286322725880162, "delta_entropy": 0.05884887298510422, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6776444487220556, "accuracy": 0, "tokens_in": 152, "tokens_out": 34, "tokens_total": 186, "latency_total": 0.02229886399982206, "latency_per_module": {"scorer": 0.010934771999927761, "prior": 0.011364091999894299}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}, "prior": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Fred had a job at the fair to fill the balloons. The balloon popped in his face!\nQuestion: Did Fred saw a child come over with a pin. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Fred had a job at the fair to fill the balloons. The balloon popped in his face!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Fred had a job at the fair to fill the balloons. The balloon popped in his face!", "hypotheses": ["Fred saw a child come over with a pin.", "Fred fill one balloon too small."]}
{"example_id": "126", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Samantha found some friends who gave her a ride. happen?", "a": "no", "prior_probs": [0.5468633476618596, 0.4531366523381404], "posterior_probs": [0.6306783119430895, 0.3693216880569105], "prior_entropy": 0.6887483802422558, "posterior_entropy": 0.6585936844799656, "delta_entropy": 0.03015469576229024, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6306783119430895, "accuracy": 0, "tokens_in": 176, "tokens_out": 42, "tokens_total": 218, "latency_total": 0.01659714099878329, "latency_per_module": {"scorer": 0.007915515999229683, "prior": 0.008681624999553605}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 21, "tokens_total": 129}, "prior": {"tokens_in": 68, "tokens_out": 21, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.\nQuestion: Did Samantha found some friends who gave her a ride. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.", "hypotheses": ["Samantha found some friends who gave her a ride.", "She had a flat tire and changed it."]}
{"example_id": "127", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The tree fell on my fort. happen?", "a": "no", "prior_probs": [0.4549968142922656, 0.5450031857077344], "posterior_probs": [0.5737565415321464, 0.4262434584678536], "prior_entropy": 0.6890911202623089, "posterior_entropy": 0.6822273196818269, "delta_entropy": 0.006863800580481971, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5737565415321464, "accuracy": 0, "tokens_in": 166, "tokens_out": 26, "tokens_total": 192, "latency_total": 0.015177889999904437, "latency_per_module": {"scorer": 0.007360207000601804, "prior": 0.007817682999302633}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 13, "tokens_total": 113}, "prior": {"tokens_in": 66, "tokens_out": 13, "tokens_total": 79}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.\nQuestion: Did The tree fell on my fort. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.", "hypotheses": ["The tree fell on my fort.", "It was the right size."]}
{"example_id": "128", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Mike was having blood drawn because he needed a routine checkup. happen?", "a": "no", "prior_probs": [0.5035711515654095, 0.49642884843459045], "posterior_probs": [0.6479735181300184, 0.3520264818699817], "prior_entropy": 0.6931216740940784, "posterior_entropy": 0.648692093209366, "delta_entropy": 0.04442958088471238, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6479735181300184, "accuracy": 0, "tokens_in": 162, "tokens_out": 44, "tokens_total": 206, "latency_total": 0.013759449000644963, "latency_per_module": {"scorer": 0.0065625340002952726, "prior": 0.007196915000349691}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 22, "tokens_total": 126}, "prior": {"tokens_in": 58, "tokens_out": 22, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike had to go to the doctor. All the blood work came back clear and he was relieved.\nQuestion: Did Mike was having blood drawn because he needed a routine checkup. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mike had to go to the doctor. All the blood work came back clear and he was relieved.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike had to go to the doctor. All the blood work came back clear and he was relieved.", "hypotheses": ["Mike was having blood drawn because he needed a routine checkup.", "Mike complained of soreness in his kidneys."]}
{"example_id": "129", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Rocky was very good at playing hockey. happen?", "a": "no", "prior_probs": [0.5061941554108962, 0.49380584458910376], "posterior_probs": [0.563181834938075, 0.436818165061925], "prior_entropy": 0.6930704434725565, "posterior_entropy": 0.685141907650995, "delta_entropy": 0.0079285358215615, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.563181834938075, "accuracy": 0, "tokens_in": 148, "tokens_out": 42, "tokens_total": 190, "latency_total": 0.014462170000115293, "latency_per_module": {"scorer": 0.007196995000413153, "prior": 0.00726517499970214}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}, "prior": {"tokens_in": 56, "tokens_out": 21, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.\nQuestion: Did Rocky was very good at playing hockey. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.", "hypotheses": ["Rocky was very good at playing hockey.", "Rocky was a rocket scientist, and he hated rockets."]}
{"example_id": "130", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Arnold saw a boy. happen?", "a": "no", "prior_probs": [0.4433643422955545, 0.5566356577044455], "posterior_probs": [0.5311127139057278, 0.4688872860942722], "prior_entropy": 0.686718195974517, "posterior_entropy": 0.6912099273179113, "delta_entropy": -0.004491731343394312, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5311127139057278, "accuracy": 0, "tokens_in": 110, "tokens_out": 34, "tokens_total": 144, "latency_total": 0.013236458999926981, "latency_per_module": {"scorer": 0.006820616999902995, "prior": 0.006415842000023986}, "tokens_per_module": {"scorer": {"tokens_in": 70, "tokens_out": 17, "tokens_total": 87}, "prior": {"tokens_in": 40, "tokens_out": 17, "tokens_total": 57}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Arnold was scared of girls. He nearly fainted.\nQuestion: Did Arnold saw a boy. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Arnold was scared of girls. He nearly fainted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Arnold was scared of girls. He nearly fainted.", "hypotheses": ["Arnold saw a boy.", "A girl came up to hug him one day."]}
{"example_id": "131", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Bobby found a cat in the garden. happen?", "a": "no", "prior_probs": [0.5233937045763193, 0.4766062954236807], "posterior_probs": [0.5847974362098407, 0.41520256379015924], "prior_entropy": 0.6920522500476836, "posterior_entropy": 0.678696224851346, "delta_entropy": 0.013356025196337606, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5847974362098407, "accuracy": 0, "tokens_in": 168, "tokens_out": 38, "tokens_total": 206, "latency_total": 0.013363885001126619, "latency_per_module": {"scorer": 0.006580976000805094, "prior": 0.006782909000321524}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}, "prior": {"tokens_in": 66, "tokens_out": 19, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.\nQuestion: Did Bobby found a cat in the garden. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.", "hypotheses": ["Bobby found a cat in the garden.", "Bobby begged his mom for a feral cat."]}
{"example_id": "132", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Lucky looked for them in one store. happen?", "a": "no", "prior_probs": [0.2999824166984342, 0.7000175833015658], "posterior_probs": [0.5238665030943123, 0.4761334969056878], "prior_entropy": 0.610849403022965, "posterior_entropy": 0.6920075276159368, "delta_entropy": -0.08115812459297178, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5238665030943123, "accuracy": 0, "tokens_in": 180, "tokens_out": 48, "tokens_total": 228, "latency_total": 0.0138288719999764, "latency_per_module": {"scorer": 0.006918220999978075, "prior": 0.0069106509999983246}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 24, "tokens_total": 132}, "prior": {"tokens_in": 72, "tokens_out": 24, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.\nQuestion: Did Lucky looked for them in one store. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.", "hypotheses": ["Lucky looked for them in one store.", "Lucy looked for the sandals everywhere, but could never find them."]}
{"example_id": "133", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did We love to go fishing. happen?", "a": "no", "prior_probs": [0.4819782992719845, 0.5180217007280155], "posterior_probs": [0.5355107934843084, 0.4644892065156916], "prior_entropy": 0.6924974764463473, "posterior_entropy": 0.6906230231441116, "delta_entropy": 0.001874453302235679, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5355107934843084, "accuracy": 0, "tokens_in": 172, "tokens_out": 30, "tokens_total": 202, "latency_total": 0.014978535000409465, "latency_per_module": {"scorer": 0.008209380000153033, "prior": 0.006769155000256433}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 15, "tokens_total": 117}, "prior": {"tokens_in": 70, "tokens_out": 15, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.\nQuestion: Did We love to go fishing. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.", "hypotheses": ["We love to go fishing.", "We found one of them in the backyard."]}
{"example_id": "134", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did We grew fond of this town. happen?", "a": "no", "prior_probs": [0.49344076594076103, 0.5065592340592391], "posterior_probs": [0.5772291439048569, 0.42277085609514303], "prior_entropy": 0.6930611309868531, "posterior_entropy": 0.681170609570151, "delta_entropy": 0.011890521416702104, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5772291439048569, "accuracy": 0, "tokens_in": 142, "tokens_out": 28, "tokens_total": 170, "latency_total": 0.020536865000394755, "latency_per_module": {"scorer": 0.01037583299967082, "prior": 0.010161032000723935}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 14, "tokens_total": 102}, "prior": {"tokens_in": 54, "tokens_out": 14, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: We decided to move to a new town next year. It will be a fun adventure.\nQuestion: Did We grew fond of this town. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: We decided to move to a new town next year. It will be a fun adventure.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "We decided to move to a new town next year. It will be a fun adventure.", "hypotheses": ["We grew fond of this town.", "We got sick of this town."]}
{"example_id": "135", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ryan was mad at Kim for stealing his phone. happen?", "a": "no", "prior_probs": [0.4610257841579939, 0.5389742158420062], "posterior_probs": [0.602280342496119, 0.39771965750388105], "prior_entropy": 0.6901061176167884, "posterior_entropy": 0.672076227528758, "delta_entropy": 0.018029890088030465, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.602280342496119, "accuracy": 0, "tokens_in": 144, "tokens_out": 38, "tokens_total": 182, "latency_total": 0.016656789000080607, "latency_per_module": {"scorer": 0.008015497000087635, "prior": 0.008641291999992973}, "tokens_per_module": {"scorer": {"tokens_in": 92, "tokens_out": 19, "tokens_total": 111}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.\nQuestion: Did Ryan was mad at Kim for stealing his phone. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.", "hypotheses": ["Ryan was mad at Kim for stealing his phone.", "Heather kept the phone away from Ryan."]}
{"example_id": "136", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tracy spends his days scrubbing and cleaning. happen?", "a": "no", "prior_probs": [0.5327502600876581, 0.46724973991234187], "posterior_probs": [0.6044295915605569, 0.3955704084394431], "prior_entropy": 0.6910004849454078, "posterior_entropy": 0.6711746935665772, "delta_entropy": 0.01982579137883056, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6044295915605569, "accuracy": 0, "tokens_in": 186, "tokens_out": 38, "tokens_total": 224, "latency_total": 0.013451104000523628, "latency_per_module": {"scorer": 0.006134434000159672, "prior": 0.007316670000363956}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 19, "tokens_total": 131}, "prior": {"tokens_in": 74, "tokens_out": 19, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.\nQuestion: Did Tracy spends his days scrubbing and cleaning. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.", "hypotheses": ["Tracy spends his days scrubbing and cleaning.", "I asked him how it is so clean."]}
{"example_id": "137", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Kya learned many new steak recipes. happen?", "a": "no", "prior_probs": [0.3580635427488348, 0.6419364572511652], "posterior_probs": [0.5579509175043711, 0.44204908249562896], "prior_entropy": 0.6522958825764628, "posterior_entropy": 0.6864154438396254, "delta_entropy": -0.03411956126316262, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5579509175043711, "accuracy": 0, "tokens_in": 140, "tokens_out": 40, "tokens_total": 180, "latency_total": 0.01336953500049276, "latency_per_module": {"scorer": 0.006936998000128369, "prior": 0.006432537000364391}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 20, "tokens_total": 108}, "prior": {"tokens_in": 52, "tokens_out": 20, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kya was trying to be vegan. Before long, being vegan was effortless.\nQuestion: Did Kya learned many new steak recipes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kya was trying to be vegan. Before long, being vegan was effortless.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kya was trying to be vegan. Before long, being vegan was effortless.", "hypotheses": ["Kya learned many new steak recipes.", "At first it was hard, but she persevered."]}
{"example_id": "138", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The system was very expensive to have installed. happen?", "a": "no", "prior_probs": [0.5818840765131251, 0.4181159234868748], "posterior_probs": [0.6456029362016825, 0.3543970637983174], "prior_entropy": 0.6796765813872114, "posterior_entropy": 0.6501261825380267, "delta_entropy": 0.029550398849184756, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6456029362016825, "accuracy": 0, "tokens_in": 162, "tokens_out": 48, "tokens_total": 210, "latency_total": 0.01475299800040375, "latency_per_module": {"scorer": 0.008023788000173226, "prior": 0.006729210000230523}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 24, "tokens_total": 124}, "prior": {"tokens_in": 62, "tokens_out": 24, "tokens_total": 86}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.\nQuestion: Did The system was very expensive to have installed. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.", "hypotheses": ["The system was very expensive to have installed.", "This month, my electric bill was double  what is used to be."]}
{"example_id": "139", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did We all had dinner at my big house. happen?", "a": "no", "prior_probs": [0.5041482688845201, 0.4958517311154799], "posterior_probs": [0.5288028366952952, 0.47119716330470485], "prior_entropy": 0.6931127638936314, "posterior_entropy": 0.6914870548785805, "delta_entropy": 0.0016257090150508802, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5288028366952952, "accuracy": 0, "tokens_in": 162, "tokens_out": 36, "tokens_total": 198, "latency_total": 0.013856967000720033, "latency_per_module": {"scorer": 0.007040660000711796, "prior": 0.006816307000008237}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 18, "tokens_total": 118}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.\nQuestion: Did We all had dinner at my big house. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.", "hypotheses": ["We all had dinner at my big house.", "We all had dinner at my tiny house."]}
{"example_id": "140", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did There wasn't any food on a plate. happen?", "a": "no", "prior_probs": [0.52242454564882, 0.47757545435118004], "posterior_probs": [0.555937587536829, 0.444062412463171], "prior_entropy": 0.6921411226333666, "posterior_entropy": 0.6868760330597594, "delta_entropy": 0.005265089573607162, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.555937587536829, "accuracy": 0, "tokens_in": 162, "tokens_out": 38, "tokens_total": 200, "latency_total": 0.01330030999997689, "latency_per_module": {"scorer": 0.006505207999907725, "prior": 0.006795102000069164}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 19, "tokens_total": 119}, "prior": {"tokens_in": 62, "tokens_out": 19, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.\nQuestion: Did There wasn't any food on a plate. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.", "hypotheses": ["There wasn't any food on a plate.", "there was a piece of steak on a plate."]}
{"example_id": "141", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Helen went to the store. happen?", "a": "no", "prior_probs": [0.5214472455295223, 0.47855275447047774], "posterior_probs": [0.570148339473299, 0.42985166052670093], "prior_entropy": 0.6922269295543793, "posterior_entropy": 0.6832730590304166, "delta_entropy": 0.008953870523962681, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.570148339473299, "accuracy": 0, "tokens_in": 140, "tokens_out": 26, "tokens_total": 166, "latency_total": 0.01290756400067039, "latency_per_module": {"scorer": 0.006851596000160498, "prior": 0.006055968000509893}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 13, "tokens_total": 99}, "prior": {"tokens_in": 54, "tokens_out": 13, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Helen hung up the stocking on the railing. And someone had put presents in her stocking!\nQuestion: Did Helen went to the store. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Helen hung up the stocking on the railing. And someone had put presents in her stocking!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Helen hung up the stocking on the railing. And someone had put presents in her stocking!", "hypotheses": ["Helen went to the store.", "Helen went to sleep."]}
{"example_id": "142", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tim forgot the gift in his car until after her birthday. happen?", "a": "no", "prior_probs": [0.4818382331647717, 0.5181617668352283], "posterior_probs": [0.5948622137362112, 0.4051377862637888], "prior_entropy": 0.6924873358643442, "posterior_entropy": 0.6750399439514677, "delta_entropy": 0.017447391912876475, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5948622137362112, "accuracy": 0, "tokens_in": 196, "tokens_out": 50, "tokens_total": 246, "latency_total": 0.014387049000106344, "latency_per_module": {"scorer": 0.007466863000445301, "prior": 0.006920185999661044}, "tokens_per_module": {"scorer": {"tokens_in": 120, "tokens_out": 25, "tokens_total": 145}, "prior": {"tokens_in": 76, "tokens_out": 25, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.\nQuestion: Did Tim forgot the gift in his car until after her birthday. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.", "hypotheses": ["Tim forgot the gift in his car until after her birthday.", "Tim realized it would barely make it in time for her birthday."]}
{"example_id": "143", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did greg had not called the police right away. happen?", "a": "no", "prior_probs": [0.47161095172450157, 0.5283890482754985], "posterior_probs": [0.6602976390354764, 0.3397023609645235], "prior_entropy": 0.6915344372670527, "posterior_entropy": 0.6408378582092928, "delta_entropy": 0.050696579057759816, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6602976390354764, "accuracy": 0, "tokens_in": 120, "tokens_out": 36, "tokens_total": 156, "latency_total": 0.01328981599999679, "latency_per_module": {"scorer": 0.006947472000319976, "prior": 0.006342343999676814}, "tokens_per_module": {"scorer": {"tokens_in": 80, "tokens_out": 18, "tokens_total": 98}, "prior": {"tokens_in": 40, "tokens_out": 18, "tokens_total": 58}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Greg was arrested for manslaughter. So Greg was convicted.\nQuestion: Did greg had not called the police right away. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Greg was arrested for manslaughter. So Greg was convicted.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Greg was arrested for manslaughter. So Greg was convicted.", "hypotheses": ["greg had not called the police right away.", "Greg's lawyer made a compelling defense case."]}
{"example_id": "144", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Joe could not have a pizza delivered. happen?", "a": "no", "prior_probs": [0.44015820877619033, 0.5598417912238096], "posterior_probs": [0.6339556217571986, 0.3660443782428015], "prior_entropy": 0.6859679034141187, "posterior_entropy": 0.6568168196982846, "delta_entropy": 0.02915108371583408, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6339556217571986, "accuracy": 0, "tokens_in": 128, "tokens_out": 42, "tokens_total": 170, "latency_total": 0.01332259700029681, "latency_per_module": {"scorer": 0.00702182500026538, "prior": 0.006300772000031429}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 21, "tokens_total": 103}, "prior": {"tokens_in": 46, "tokens_out": 21, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Joe could not remember his address. After that he always remembered it.\nQuestion: Did Joe could not have a pizza delivered. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Joe could not remember his address. After that he always remembered it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe could not remember his address. After that he always remembered it.", "hypotheses": ["Joe could not have a pizza delivered.", "He made up a a rhyme that included his phone number."]}
{"example_id": "145", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Nadia's friend gifted her the money for new shoes. happen?", "a": "no", "prior_probs": [0.5366545194477929, 0.46334548055220715], "posterior_probs": [0.6292983031262465, 0.37070169687375343], "prior_entropy": 0.6904576609280619, "posterior_entropy": 0.6593280796775458, "delta_entropy": 0.031129581250516125, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6292983031262465, "accuracy": 0, "tokens_in": 152, "tokens_out": 40, "tokens_total": 192, "latency_total": 0.01431796999895596, "latency_per_module": {"scorer": 0.006923606999407639, "prior": 0.007394362999548321}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Nadia needed new ballet shoes. After her first dance show, she paid him back.\nQuestion: Did Nadia's friend gifted her the money for new shoes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Nadia needed new ballet shoes. After her first dance show, she paid him back.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nadia needed new ballet shoes. After her first dance show, she paid him back.", "hypotheses": ["Nadia's friend gifted her the money for new shoes.", "A friend bought Nadia a pair."]}
{"example_id": "146", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Matthew read the basketball rules and practiced the game. happen?", "a": "no", "prior_probs": [0.5012941331574995, 0.49870586684250057], "posterior_probs": [0.6492430463533558, 0.3507569536466442], "prior_entropy": 0.6931438309929469, "posterior_entropy": 0.6479139641429985, "delta_entropy": 0.04522986684994834, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6492430463533558, "accuracy": 0, "tokens_in": 172, "tokens_out": 38, "tokens_total": 210, "latency_total": 0.02311061399996106, "latency_per_module": {"scorer": 0.013275123999846983, "prior": 0.009835490000114078}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 19, "tokens_total": 125}, "prior": {"tokens_in": 66, "tokens_out": 19, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.\nQuestion: Did Matthew read the basketball rules and practiced the game. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.", "hypotheses": ["Matthew read the basketball rules and practiced the game.", "He practiced and tried out for a role."]}
{"example_id": "147", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Mike entered a contest partnering with Joseph. happen?", "a": "no", "prior_probs": [0.5013264386531003, 0.4986735613468997], "posterior_probs": [0.640809066490289, 0.359190933509711], "prior_entropy": 0.693143661674817, "posterior_entropy": 0.6529512659277179, "delta_entropy": 0.0401923957470991, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.640809066490289, "accuracy": 0, "tokens_in": 144, "tokens_out": 40, "tokens_total": 184, "latency_total": 0.02442530200096371, "latency_per_module": {"scorer": 0.011944916000174999, "prior": 0.012480386000788712}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 20, "tokens_total": 110}, "prior": {"tokens_in": 54, "tokens_out": 20, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.\nQuestion: Did Mike entered a contest partnering with Joseph. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.", "hypotheses": ["Mike entered a contest partnering with Joseph.", "Joseph could tell Mike was playing too over-confidently."]}
{"example_id": "148", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?", "a": "no", "prior_probs": [0.5719734763411052, 0.42802652365889493], "posterior_probs": [0.7629772617452668, 0.2370227382547333], "prior_entropy": 0.6827507390451305, "posterior_entropy": 0.547623731750972, "delta_entropy": 0.1351270072941585, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7629772617452668, "accuracy": 0, "tokens_in": 154, "tokens_out": 56, "tokens_total": 210, "latency_total": 0.03022093399977166, "latency_per_module": {"scorer": 0.009964922000108345, "prior": 0.020256011999663315}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 28, "tokens_total": 130}, "prior": {"tokens_in": 52, "tokens_out": 28, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.\nQuestion: Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.", "hypotheses": ["All flights were grounded to Lisa couldn't leave for a couple of days.", "Lucy shared supplies in art class with Lisa, they bonded."]}
{"example_id": "149", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Martha's boyfriend enrolled himself in cooking classes. happen?", "a": "no", "prior_probs": [0.5042689001377829, 0.4957310998622171], "posterior_probs": [0.6241150157597921, 0.37588498424020794], "prior_entropy": 0.6931107330983648, "posterior_entropy": 0.6620136404865824, "delta_entropy": 0.031097092611782395, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6241150157597921, "accuracy": 0, "tokens_in": 158, "tokens_out": 40, "tokens_total": 198, "latency_total": 0.03383824300090055, "latency_per_module": {"scorer": 0.01885069600029965, "prior": 0.014987547000600898}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}, "prior": {"tokens_in": 60, "tokens_out": 20, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.\nQuestion: Did Martha's boyfriend enrolled himself in cooking classes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.", "hypotheses": ["Martha's boyfriend enrolled himself in cooking classes.", "Martha worked hard to learn some recipies."]}
{"example_id": "150", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Her husband did not expect Lucy to be home yet. happen?", "a": "no", "prior_probs": [0.5310802956494481, 0.4689197043505518], "posterior_probs": [0.6117678949259431, 0.388232105074057], "prior_entropy": 0.6912139649068965, "posterior_entropy": 0.6679507134155024, "delta_entropy": 0.023263251491394077, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6117678949259431, "accuracy": 0, "tokens_in": 146, "tokens_out": 46, "tokens_total": 192, "latency_total": 0.02937117399960698, "latency_per_module": {"scorer": 0.014437041999372013, "prior": 0.014934132000234968}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 23, "tokens_total": 117}, "prior": {"tokens_in": 52, "tokens_out": 23, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.\nQuestion: Did Her husband did not expect Lucy to be home yet. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.", "hypotheses": ["Her husband did not expect Lucy to be home yet.", "she couldn't wait to tell him she wanted a divorce."]}
{"example_id": "151", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Kelsi and Thomas met at school. happen?", "a": "no", "prior_probs": [0.5187758093381991, 0.4812241906618008], "posterior_probs": [0.5830028797887451, 0.41699712021125485], "prior_entropy": 0.692441952727644, "posterior_entropy": 0.6793042298106573, "delta_entropy": 0.013137722916986672, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5830028797887451, "accuracy": 0, "tokens_in": 182, "tokens_out": 44, "tokens_total": 226, "latency_total": 0.02897074199972849, "latency_per_module": {"scorer": 0.01501614199969481, "prior": 0.013954600000033679}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 22, "tokens_total": 132}, "prior": {"tokens_in": 72, "tokens_out": 22, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.\nQuestion: Did Kelsi and Thomas met at school. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.", "hypotheses": ["Kelsi and Thomas met at school.", "Kelsi saw Lucy was reading the same book she was."]}
{"example_id": "152", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did i had enough money to spend on food and extra stuff. happen?", "a": "no", "prior_probs": [0.5489156641466169, 0.45108433585338314], "posterior_probs": [0.6618535661213454, 0.33814643387865456], "prior_entropy": 0.6883540331646313, "posterior_entropy": 0.6397983560963837, "delta_entropy": 0.048555677068247594, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6618535661213454, "accuracy": 0, "tokens_in": 164, "tokens_out": 36, "tokens_total": 200, "latency_total": 0.02946823099955509, "latency_per_module": {"scorer": 0.01136330299959809, "prior": 0.018104927999957}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}, "prior": {"tokens_in": 60, "tokens_out": 18, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I saved up money for a long time. I took the boat out on the lake and felt happy.\nQuestion: Did i had enough money to spend on food and extra stuff. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I saved up money for a long time. I took the boat out on the lake and felt happy.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I saved up money for a long time. I took the boat out on the lake and felt happy.", "hypotheses": ["i had enough money to spend on food and extra stuff.", "I bought my dream cat."]}
{"example_id": "153", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jake immediately threw his new toy in the trash. happen?", "a": "no", "prior_probs": [0.5717033204360721, 0.428296679563928], "posterior_probs": [0.6419857770755857, 0.35801422292441437], "prior_entropy": 0.6828289102070211, "posterior_entropy": 0.6522670854142874, "delta_entropy": 0.030561824792733727, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6419857770755857, "accuracy": 0, "tokens_in": 184, "tokens_out": 36, "tokens_total": 220, "latency_total": 0.018305444000361604, "latency_per_module": {"scorer": 0.009274901000026148, "prior": 0.009030543000335456}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 18, "tokens_total": 130}, "prior": {"tokens_in": 72, "tokens_out": 18, "tokens_total": 90}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string\nQuestion: Did Jake immediately threw his new toy in the trash. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string", "hypotheses": ["Jake immediately threw his new toy in the trash.", "Jake decided Dan was the Green Goblin."]}
{"example_id": "154", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Margo sat down to watch. happen?", "a": "no", "prior_probs": [0.46568290051530103, 0.534317099484699], "posterior_probs": [0.5465878384641614, 0.45341216153583863], "prior_entropy": 0.6907900012428367, "posterior_entropy": 0.6888000242557888, "delta_entropy": 0.001989976987047948, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5465878384641614, "accuracy": 0, "tokens_in": 142, "tokens_out": 26, "tokens_total": 168, "latency_total": 0.020741272999657667, "latency_per_module": {"scorer": 0.009329693999461597, "prior": 0.01141157900019607}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 13, "tokens_total": 101}, "prior": {"tokens_in": 54, "tokens_out": 13, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.\nQuestion: Did Margo sat down to watch. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.", "hypotheses": ["Margo sat down to watch.", "Margo left the show."]}
{"example_id": "155", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Barry's team won the game today. happen?", "a": "no", "prior_probs": [0.4269841303580189, 0.573015869641981], "posterior_probs": [0.5814656780904943, 0.4185343219095057], "prior_entropy": 0.6824463218505652, "posterior_entropy": 0.6798145075761572, "delta_entropy": 0.002631814274407951, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5814656780904943, "accuracy": 0, "tokens_in": 120, "tokens_out": 38, "tokens_total": 158, "latency_total": 0.016103053999358963, "latency_per_module": {"scorer": 0.007788347999849066, "prior": 0.008314705999509897}, "tokens_per_module": {"scorer": {"tokens_in": 78, "tokens_out": 19, "tokens_total": 97}, "prior": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Barry loves playing baseball. Barry also bought a hot dog.\nQuestion: Did Barry's team won the game today. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Barry loves playing baseball. Barry also bought a hot dog.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Barry loves playing baseball. Barry also bought a hot dog.", "hypotheses": ["Barry's team won the game today.", "Barry went to the bar for a game."]}
{"example_id": "156", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Erina gave it her all and did well. happen?", "a": "no", "prior_probs": [0.49943053747004224, 0.5005694625299577], "posterior_probs": [0.5920209925891655, 0.4079790074108345], "prior_entropy": 0.6931465319826591, "posterior_entropy": 0.6761145287895747, "delta_entropy": 0.0170320031930844, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5920209925891655, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.01872598899899458, "latency_per_module": {"scorer": 0.010150673999305582, "prior": 0.008575314999688999}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Erina's first day at her new job was today. Her new boss complimented her on her performance.\nQuestion: Did Erina gave it her all and did well. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Erina's first day at her new job was today. Her new boss complimented her on her performance.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erina's first day at her new job was today. Her new boss complimented her on her performance.", "hypotheses": ["Erina gave it her all and did well.", "Erina took too many breaks the first day."]}
{"example_id": "157", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Penny dropped her game on accident. happen?", "a": "no", "prior_probs": [0.5264368049007686, 0.4735631950992314], "posterior_probs": [0.5944174596215034, 0.40558254037849656], "prior_entropy": 0.6917487192315404, "posterior_entropy": 0.6752103648191494, "delta_entropy": 0.016538354412390932, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5944174596215034, "accuracy": 0, "tokens_in": 174, "tokens_out": 30, "tokens_total": 204, "latency_total": 0.022024257999873953, "latency_per_module": {"scorer": 0.011150221999741916, "prior": 0.010874036000132037}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 15, "tokens_total": 119}, "prior": {"tokens_in": 70, "tokens_out": 15, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.\nQuestion: Did Penny dropped her game on accident. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.", "hypotheses": ["Penny dropped her game on accident.", "She accidentally sold her game console."]}
{"example_id": "158", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?", "a": "no", "prior_probs": [0.5403909182322422, 0.45960908176775783], "posterior_probs": [0.6379185995365564, 0.36208140046344356], "prior_entropy": 0.6898807699753513, "posterior_entropy": 0.6546063643409491, "delta_entropy": 0.035274405634402206, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6379185995365564, "accuracy": 0, "tokens_in": 170, "tokens_out": 50, "tokens_total": 220, "latency_total": 0.02217624100012472, "latency_per_module": {"scorer": 0.010963261999677343, "prior": 0.011212979000447376}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 25, "tokens_total": 133}, "prior": {"tokens_in": 62, "tokens_out": 25, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.\nQuestion: Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.", "hypotheses": ["Amy robbed the frozen yogurt store and ate all of the yogurt.", "Amy was called back to work and her frozen yogurt melted."]}
{"example_id": "159", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ellen realized that she could. happen?", "a": "no", "prior_probs": [0.5027835081552867, 0.4972164918447132], "posterior_probs": [0.5437743636785437, 0.4562256363214563], "prior_entropy": 0.693131684642603, "posterior_entropy": 0.6893098799179234, "delta_entropy": 0.003821804724679634, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5437743636785437, "accuracy": 0, "tokens_in": 172, "tokens_out": 34, "tokens_total": 206, "latency_total": 0.022724691999428615, "latency_per_module": {"scorer": 0.010983922999912465, "prior": 0.011740768999516149}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 17, "tokens_total": 119}, "prior": {"tokens_in": 70, "tokens_out": 17, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.\nQuestion: Did Ellen realized that she could. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.", "hypotheses": ["Ellen realized that she could.", "Ellen was told there was a dress code."]}
{"example_id": "160", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Lily decided to make her costume a bear costume. happen?", "a": "no", "prior_probs": [0.6198132078037958, 0.3801867921962042], "posterior_probs": [0.6630818066285097, 0.3369181933714903], "prior_entropy": 0.664155496307155, "posterior_entropy": 0.6389701400853475, "delta_entropy": 0.02518535622180751, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6630818066285097, "accuracy": 0, "tokens_in": 136, "tokens_out": 36, "tokens_total": 172, "latency_total": 0.020894738999231777, "latency_per_module": {"scorer": 0.010532188000070164, "prior": 0.010362550999161613}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 18, "tokens_total": 106}, "prior": {"tokens_in": 48, "tokens_out": 18, "tokens_total": 66}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Lily wanted a new Halloween costume. She ended up making a rabbit costume.\nQuestion: Did Lily decided to make her costume a bear costume. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Lily wanted a new Halloween costume. She ended up making a rabbit costume.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lily wanted a new Halloween costume. She ended up making a rabbit costume.", "hypotheses": ["Lily decided to make her costume a bear costume.", "All the costumes were gone though."]}
{"example_id": "161", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She studied hard because she wanted to spell. happen?", "a": "no", "prior_probs": [0.464782121161704, 0.5352178788382961], "posterior_probs": [0.5822040033742816, 0.4177959966257184], "prior_entropy": 0.6906645273748359, "posterior_entropy": 0.6795706311899048, "delta_entropy": 0.011093896184931062, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5822040033742816, "accuracy": 0, "tokens_in": 174, "tokens_out": 40, "tokens_total": 214, "latency_total": 0.021880909000174142, "latency_per_module": {"scorer": 0.011110250999990967, "prior": 0.010770658000183175}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 20, "tokens_total": 126}, "prior": {"tokens_in": 68, "tokens_out": 20, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.\nQuestion: Did She studied hard because she wanted to spell. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.", "hypotheses": ["She studied hard because she wanted to spell.", "Mia entered the spelling bee but didn't practice."]}
{"example_id": "162", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The bus was late and so was I to school. happen?", "a": "no", "prior_probs": [0.49817956298321636, 0.5018204370167837], "posterior_probs": [0.6030217438692923, 0.3969782561307076], "prior_entropy": 0.6931405525614377, "posterior_entropy": 0.6717674164109213, "delta_entropy": 0.021373136150516392, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6030217438692923, "accuracy": 0, "tokens_in": 174, "tokens_out": 44, "tokens_total": 218, "latency_total": 0.023623994999979914, "latency_per_module": {"scorer": 0.012108933000490651, "prior": 0.011515061999489262}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 22, "tokens_total": 130}, "prior": {"tokens_in": 66, "tokens_out": 22, "tokens_total": 88}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.\nQuestion: Did The bus was late and so was I to school. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.", "hypotheses": ["The bus was late and so was I to school.", "bus showed up early and I was late for class."]}
{"example_id": "163", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jean booked her trip and went. happen?", "a": "no", "prior_probs": [0.37469656598213685, 0.6253034340178631], "posterior_probs": [0.5065690075362256, 0.49343099246377436], "prior_entropy": 0.6614080398426275, "posterior_entropy": 0.6930608743549747, "delta_entropy": -0.03165283451234713, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5065690075362256, "accuracy": 0, "tokens_in": 146, "tokens_out": 34, "tokens_total": 180, "latency_total": 0.02438590900055715, "latency_per_module": {"scorer": 0.01311982200058992, "prior": 0.011266086999967229}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 17, "tokens_total": 107}, "prior": {"tokens_in": 56, "tokens_out": 17, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.\nQuestion: Did Jean booked her trip and went. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.", "hypotheses": ["Jean booked her trip and went.", "Jean ended up having a bad time in Africa."]}
{"example_id": "164", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Mary ended up overcooking the pasta. happen?", "a": "no", "prior_probs": [0.5138874089170941, 0.486112591082906], "posterior_probs": [0.6518212145747361, 0.34817878542526387], "prior_entropy": 0.6927614106964033, "posterior_entropy": 0.6463119402236601, "delta_entropy": 0.04644947047274317, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6518212145747361, "accuracy": 0, "tokens_in": 134, "tokens_out": 48, "tokens_total": 182, "latency_total": 0.026288310999916575, "latency_per_module": {"scorer": 0.014214906000233896, "prior": 0.01207340499968268}, "tokens_per_module": {"scorer": {"tokens_in": 86, "tokens_out": 24, "tokens_total": 110}, "prior": {"tokens_in": 48, "tokens_out": 24, "tokens_total": 72}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Mary had never made rice before. She resolved to read directions next time!\nQuestion: Did Mary ended up overcooking the pasta. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Mary had never made rice before. She resolved to read directions next time!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary had never made rice before. She resolved to read directions next time!", "hypotheses": ["Mary ended up overcooking the pasta.", "The rice Mary put on the stove out over and burned on the stove."]}
{"example_id": "165", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Brad rushed to work. happen?", "a": "no", "prior_probs": [0.45114413817439564, 0.5488558618256044], "posterior_probs": [0.5090889454442302, 0.49091105455576967], "prior_entropy": 0.6883657645700085, "posterior_entropy": 0.6929819535991859, "delta_entropy": -0.0046161890291773755, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5090889454442302, "accuracy": 0, "tokens_in": 146, "tokens_out": 38, "tokens_total": 184, "latency_total": 0.02925959000003786, "latency_per_module": {"scorer": 0.014357818999997107, "prior": 0.014901771000040753}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 19, "tokens_total": 107}, "prior": {"tokens_in": 58, "tokens_out": 19, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.\nQuestion: Did Brad rushed to work. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.", "hypotheses": ["Brad rushed to work.", "He was in such a slow mood, he didn't dress correctly."]}
{"example_id": "166", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I left all my stuff out on the bed. happen?", "a": "no", "prior_probs": [0.5975131689009477, 0.40248683109905226], "posterior_probs": [0.6641824667828291, 0.33581753321717095], "prior_entropy": 0.6740071150648055, "posterior_entropy": 0.6382222162787503, "delta_entropy": 0.035784898786055175, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6641824667828291, "accuracy": 0, "tokens_in": 168, "tokens_out": 32, "tokens_total": 200, "latency_total": 0.027709983000022476, "latency_per_module": {"scorer": 0.011777777000133938, "prior": 0.015932205999888538}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 16, "tokens_total": 120}, "prior": {"tokens_in": 64, "tokens_out": 16, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.\nQuestion: Did I left all my stuff out on the bed. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.", "hypotheses": ["I left all my stuff out on the bed.", "I heard loud fire alarm."]}
{"example_id": "167", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did My shoes made a really loud sound in front of my boss. happen?", "a": "no", "prior_probs": [0.5050753995895266, 0.49492460041047337], "posterior_probs": [0.6468762456464059, 0.35312375435359417], "prior_entropy": 0.693095660311174, "posterior_entropy": 0.6493589488836923, "delta_entropy": 0.0437367114274817, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6468762456464059, "accuracy": 0, "tokens_in": 170, "tokens_out": 40, "tokens_total": 210, "latency_total": 0.024242272999799752, "latency_per_module": {"scorer": 0.009930574999998498, "prior": 0.014311697999801254}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 20, "tokens_total": 128}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: For a lark I started dragging my foot behind me at work. He told me to knock it off.\nQuestion: Did My shoes made a really loud sound in front of my boss. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: For a lark I started dragging my foot behind me at work. He told me to knock it off.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "For a lark I started dragging my foot behind me at work. He told me to knock it off.", "hypotheses": ["My shoes made a really loud sound in front of my boss.", "I fell and broke my leg."]}
{"example_id": "168", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did However, he was not very talented, and his hair did not sell very well. happen?", "a": "no", "prior_probs": [0.6146910714067012, 0.3853089285932988], "posterior_probs": [0.7134848862873541, 0.286515113712646], "prior_entropy": 0.666603394708587, "posterior_entropy": 0.5990018094877949, "delta_entropy": 0.06760158522079207, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7134848862873541, "accuracy": 0, "tokens_in": 154, "tokens_out": 60, "tokens_total": 214, "latency_total": 0.02429040399965743, "latency_per_module": {"scorer": 0.012615799999366573, "prior": 0.011674604000290856}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 30, "tokens_total": 134}, "prior": {"tokens_in": 50, "tokens_out": 30, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tony liked art. Tony then went back to school and found a different major.\nQuestion: Did However, he was not very talented, and his hair did not sell very well. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tony liked art. Tony then went back to school and found a different major.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tony liked art. Tony then went back to school and found a different major.", "hypotheses": ["However, he was not very talented, and his hair did not sell very well.", "Tony applies for a lot of art gallery jobs and got rejected."]}
{"example_id": "169", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did I turned the $600 dollars to the authorities. happen?", "a": "no", "prior_probs": [0.4112140779915971, 0.5887859220084029], "posterior_probs": [0.5757534715926861, 0.42424652840731397], "prior_entropy": 0.6772973833177964, "posterior_entropy": 0.6816256869249477, "delta_entropy": -0.004328303607151307, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5757534715926861, "accuracy": 0, "tokens_in": 140, "tokens_out": 46, "tokens_total": 186, "latency_total": 0.023732501000267803, "latency_per_module": {"scorer": 0.01384549300018989, "prior": 0.009887008000077913}, "tokens_per_module": {"scorer": {"tokens_in": 90, "tokens_out": 23, "tokens_total": 113}, "prior": {"tokens_in": 50, "tokens_out": 23, "tokens_total": 73}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I found a $600 dollar envelope in the mail today. I am honest.\nQuestion: Did I turned the $600 dollars to the authorities. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I found a $600 dollar envelope in the mail today. I am honest.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I found a $600 dollar envelope in the mail today. I am honest.", "hypotheses": ["I turned the $600 dollars to the authorities.", "I returned the empty envelope to the person it was addressed to."]}
{"example_id": "170", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Dominick moved his house away from Becky's house. happen?", "a": "no", "prior_probs": [0.6687183408185025, 0.3312816591814976], "posterior_probs": [0.684078457676746, 0.31592154232325403], "prior_entropy": 0.6350825753029137, "posterior_entropy": 0.6237569233425315, "delta_entropy": 0.011325651960382155, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.684078457676746, "accuracy": 0, "tokens_in": 154, "tokens_out": 44, "tokens_total": 198, "latency_total": 0.020077593000678462, "latency_per_module": {"scorer": 0.010069357000247692, "prior": 0.01000823600043077}, "tokens_per_module": {"scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}, "prior": {"tokens_in": 56, "tokens_out": 22, "tokens_total": 78}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.\nQuestion: Did Dominick moved his house away from Becky's house. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.", "hypotheses": ["Dominick moved his house away from Becky's house.", "Dominick shifted their house near the Becky house so."]}
{"example_id": "171", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tom developed emotional problems affecting his division. happen?", "a": "no", "prior_probs": [0.3802296600706219, 0.619770339929378], "posterior_probs": [0.5706824377980868, 0.42931756220191314], "prior_entropy": 0.6641764443158823, "posterior_entropy": 0.683121617504789, "delta_entropy": -0.01894517318890665, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5706824377980868, "accuracy": 0, "tokens_in": 140, "tokens_out": 38, "tokens_total": 178, "latency_total": 0.015631761000804545, "latency_per_module": {"scorer": 0.007452585000464751, "prior": 0.008179176000339794}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 19, "tokens_total": 107}, "prior": {"tokens_in": 52, "tokens_out": 19, "tokens_total": 71}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tom was accidentally shot by his teammate in the army. He ends up being homeless.\nQuestion: Did Tom developed emotional problems affecting his division. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tom was accidentally shot by his teammate in the army. He ends up being homeless.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was accidentally shot by his teammate in the army. He ends up being homeless.", "hypotheses": ["Tom developed emotional problems affecting his division.", "Tom was unable to find work being in a wheelchair."]}
{"example_id": "172", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tommy's friends cheered him up. happen?", "a": "no", "prior_probs": [0.4065824365907642, 0.5934175634092357], "posterior_probs": [0.5159588470221572, 0.48404115297784284], "prior_entropy": 0.6755905096228538, "posterior_entropy": 0.692637724440335, "delta_entropy": -0.01704721481748117, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5159588470221572, "accuracy": 0, "tokens_in": 118, "tokens_out": 38, "tokens_total": 156, "latency_total": 0.01364103800005978, "latency_per_module": {"scorer": 0.007066307999593846, "prior": 0.006574730000465934}, "tokens_per_module": {"scorer": {"tokens_in": 76, "tokens_out": 19, "tokens_total": 95}, "prior": {"tokens_in": 42, "tokens_out": 19, "tokens_total": 61}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tommy was having a bad day. Tommy had good friends.\nQuestion: Did Tommy's friends cheered him up. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tommy was having a bad day. Tommy had good friends.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tommy was having a bad day. Tommy had good friends.", "hypotheses": ["Tommy's friends cheered him up.", "Tommy's friends didn't pay attention to him."]}
{"example_id": "173", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jay wasn't sure how to vote. happen?", "a": "no", "prior_probs": [0.5449985742051293, 0.45500142579487063], "posterior_probs": [0.5733131693796836, 0.4266868306203164], "prior_entropy": 0.689091952601288, "posterior_entropy": 0.6823586855711019, "delta_entropy": 0.006733267030186063, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5733131693796836, "accuracy": 0, "tokens_in": 140, "tokens_out": 32, "tokens_total": 172, "latency_total": 0.013201120999838167, "latency_per_module": {"scorer": 0.006634233000113454, "prior": 0.006566887999724713}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}, "prior": {"tokens_in": 52, "tokens_out": 16, "tokens_total": 68}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jay wanted to vote. That is, until he got older and did it again.\nQuestion: Did Jay wasn't sure how to vote. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jay wanted to vote. That is, until he got older and did it again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jay wanted to vote. That is, until he got older and did it again.", "hypotheses": ["Jay wasn't sure how to vote.", "Jay swore he'd never vote again."]}
{"example_id": "174", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did My date threw up on me. happen?", "a": "no", "prior_probs": [0.484791977761282, 0.515208022238718], "posterior_probs": [0.5226265521931132, 0.47737344780688684], "prior_entropy": 0.6926845413276974, "posterior_entropy": 0.6921229090711682, "delta_entropy": 0.0005616322565291654, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5226265521931132, "accuracy": 0, "tokens_in": 154, "tokens_out": 28, "tokens_total": 182, "latency_total": 0.013840972999787482, "latency_per_module": {"scorer": 0.006894899999679183, "prior": 0.006946073000108299}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 14, "tokens_total": 108}, "prior": {"tokens_in": 60, "tokens_out": 14, "tokens_total": 74}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I was really nervous before my first middle school dance. Now, she won't even talk to me.\nQuestion: Did My date threw up on me. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I was really nervous before my first middle school dance. Now, she won't even talk to me.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was really nervous before my first middle school dance. Now, she won't even talk to me.", "hypotheses": ["My date threw up on me.", "I threw up on my date."]}
{"example_id": "175", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?", "a": "no", "prior_probs": [0.4577060736616809, 0.5422939263383191], "posterior_probs": [0.7175104258259337, 0.2824895741740662], "prior_entropy": 0.6895653495966495, "posterior_entropy": 0.5952892803901451, "delta_entropy": 0.09427606920650444, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7175104258259337, "accuracy": 0, "tokens_in": 186, "tokens_out": 64, "tokens_total": 250, "latency_total": 0.015329601999837905, "latency_per_module": {"scorer": 0.007906935999926645, "prior": 0.00742266599991126}, "tokens_per_module": {"scorer": {"tokens_in": 124, "tokens_out": 32, "tokens_total": 156}, "prior": {"tokens_in": 62, "tokens_out": 32, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.\nQuestion: Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.", "hypotheses": ["Today I was ready for him. When he came into our room a jumped out and tickled him.", "He would jump on our ear to get our attention."]}
{"example_id": "176", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The my wife became sad. happen?", "a": "no", "prior_probs": [0.39948043219530643, 0.6005195678046935], "posterior_probs": [0.4633265096239606, 0.5366734903760394], "prior_entropy": 0.6728004379125668, "posterior_entropy": 0.6904548737244576, "delta_entropy": -0.017654435811890745, "eig_estimate": 0.0, "pred": 1, "gold": 1, "confidence": 0.5366734903760394, "accuracy": 1, "tokens_in": 144, "tokens_out": 38, "tokens_total": 182, "latency_total": 0.013470257999870228, "latency_per_module": {"scorer": 0.006726415999764868, "prior": 0.00674384200010536}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 19, "tokens_total": 107}, "prior": {"tokens_in": 56, "tokens_out": 19, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I loved to make my wife laugh. I started making her laugh again and she became happy!\nQuestion: Did The my wife became sad. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I loved to make my wife laugh. I started making her laugh again and she became happy!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I loved to make my wife laugh. I started making her laugh again and she became happy!", "hypotheses": ["The my wife became sad.", "My husband was sad so I thought I would cheer her up."]}
{"example_id": "177", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Rachel burned her hand. happen?", "a": "no", "prior_probs": [0.5202507998250527, 0.4797492001749473], "posterior_probs": [0.5709239023816695, 0.42907609761833054], "prior_entropy": 0.692326766386478, "posterior_entropy": 0.6830527689929038, "delta_entropy": 0.009273997393574263, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5709239023816695, "accuracy": 0, "tokens_in": 134, "tokens_out": 26, "tokens_total": 160, "latency_total": 0.014540833999490133, "latency_per_module": {"scorer": 0.008078951999777928, "prior": 0.0064618819997122046}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 13, "tokens_total": 95}, "prior": {"tokens_in": 52, "tokens_out": 13, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Rachel was cooking dinner. Then she pulled herself together and took care of the cut.\nQuestion: Did Rachel burned her hand. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Rachel was cooking dinner. Then she pulled herself together and took care of the cut.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Rachel was cooking dinner. Then she pulled herself together and took care of the cut.", "hypotheses": ["Rachel burned her hand.", "She cut her finger when chopping vegetables."]}
{"example_id": "178", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Sam got lots of complaints about her clothing. happen?", "a": "no", "prior_probs": [0.5855762530292931, 0.414423746970707], "posterior_probs": [0.6439300762833287, 0.35606992371667134], "prior_entropy": 0.6784282315920453, "posterior_entropy": 0.6511233970641828, "delta_entropy": 0.027304834527862587, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6439300762833287, "accuracy": 0, "tokens_in": 126, "tokens_out": 40, "tokens_total": 166, "latency_total": 0.015855715000725468, "latency_per_module": {"scorer": 0.007603894000567379, "prior": 0.008251821000158088}, "tokens_per_module": {"scorer": {"tokens_in": 82, "tokens_out": 20, "tokens_total": 102}, "prior": {"tokens_in": 44, "tokens_out": 20, "tokens_total": 64}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sam loved striped clothes. She began to wear stripes every day!\nQuestion: Did Sam got lots of complaints about her clothing. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sam loved striped clothes. She began to wear stripes every day!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam loved striped clothes. She began to wear stripes every day!", "hypotheses": ["Sam got lots of complaints about her clothing.", "She decided to go out and buy and entire outfit."]}
{"example_id": "179", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He ended up getting in trouble. happen?", "a": "no", "prior_probs": [0.5009149302761192, 0.49908506972388084], "posterior_probs": [0.5821405570906332, 0.4178594429093668], "prior_entropy": 0.6931455063621907, "posterior_entropy": 0.6795916761461047, "delta_entropy": 0.013553830216086027, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5821405570906332, "accuracy": 0, "tokens_in": 158, "tokens_out": 44, "tokens_total": 202, "latency_total": 0.013931224999396363, "latency_per_module": {"scorer": 0.006844278999778908, "prior": 0.0070869459996174555}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 22, "tokens_total": 118}, "prior": {"tokens_in": 62, "tokens_out": 22, "tokens_total": 84}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jimmy's phone ringed in class. As a result, she called the security guard to take him away.\nQuestion: Did He ended up getting in trouble. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jimmy's phone ringed in class. As a result, she called the security guard to take him away.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jimmy's phone ringed in class. As a result, she called the security guard to take him away.", "hypotheses": ["He ended up getting in trouble.", "Jimmy talked on the phone and wouldn't stop, delighting the professor."]}
{"example_id": "180", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did She severely undercooked the chicken and badly burned the potatoes. happen?", "a": "no", "prior_probs": [0.457286767228584, 0.542713232771416], "posterior_probs": [0.6607183873691748, 0.3392816126308252], "prior_entropy": 0.6894938890322678, "posterior_entropy": 0.6405578253862159, "delta_entropy": 0.048936063646051964, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6607183873691748, "accuracy": 0, "tokens_in": 168, "tokens_out": 36, "tokens_total": 204, "latency_total": 0.014573361999282497, "latency_per_module": {"scorer": 0.007981287999427877, "prior": 0.00659207399985462}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.\nQuestion: Did She severely undercooked the chicken and badly burned the potatoes. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.", "hypotheses": ["She severely undercooked the chicken and badly burned the potatoes.", "Kelly's dinner was tasty."]}
{"example_id": "181", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Amber talked to her academic advisor. happen?", "a": "no", "prior_probs": [0.5152120120544086, 0.4847879879455914], "posterior_probs": [0.5538230921646585, 0.4461769078353414], "prior_entropy": 0.6926842985121199, "posterior_entropy": 0.6873420883008954, "delta_entropy": 0.005342210211224496, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5538230921646585, "accuracy": 0, "tokens_in": 134, "tokens_out": 32, "tokens_total": 166, "latency_total": 0.020845714000643056, "latency_per_module": {"scorer": 0.011153266000292206, "prior": 0.00969244800035085}, "tokens_per_module": {"scorer": {"tokens_in": 84, "tokens_out": 16, "tokens_total": 100}, "prior": {"tokens_in": 50, "tokens_out": 16, "tokens_total": 66}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amber was scared about her future. She was no longer worried about her future.\nQuestion: Did Amber talked to her academic advisor. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amber was scared about her future. She was no longer worried about her future.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber was scared about her future. She was no longer worried about her future.", "hypotheses": ["Amber talked to her academic advisor.", "Amber talked to her psychic advisor."]}
{"example_id": "182", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Terry practiced for a long time. happen?", "a": "no", "prior_probs": [0.5951790614049827, 0.40482093859501733], "posterior_probs": [0.6306803666696139, 0.36931963333038603], "prior_entropy": 0.6749180336440842, "posterior_entropy": 0.6585925849294575, "delta_entropy": 0.01632544871462671, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6306803666696139, "accuracy": 0, "tokens_in": 158, "tokens_out": 26, "tokens_total": 184, "latency_total": 0.022244456999942486, "latency_per_module": {"scorer": 0.010642293999808317, "prior": 0.011602163000134169}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 13, "tokens_total": 109}, "prior": {"tokens_in": 62, "tokens_out": 13, "tokens_total": 75}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.\nQuestion: Did Terry practiced for a long time. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.", "hypotheses": ["Terry practiced for a long time.", "terry was so big."]}
{"example_id": "183", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Sara was outside looking at the neighbor's dog. happen?", "a": "no", "prior_probs": [0.5548732862741387, 0.4451267137258613], "posterior_probs": [0.5708015972471827, 0.42919840275281723], "prior_entropy": 0.6871128780618596, "posterior_entropy": 0.6830876714556835, "delta_entropy": 0.004025206606176113, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5708015972471827, "accuracy": 0, "tokens_in": 160, "tokens_out": 44, "tokens_total": 204, "latency_total": 0.020328428000539134, "latency_per_module": {"scorer": 0.011435646000791166, "prior": 0.008892781999747967}, "tokens_per_module": {"scorer": {"tokens_in": 100, "tokens_out": 22, "tokens_total": 122}, "prior": {"tokens_in": 60, "tokens_out": 22, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.\nQuestion: Did Sara was outside looking at the neighbor's dog. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.", "hypotheses": ["Sara was outside looking at the neighbor's dog.", "Sara was outside looking at the neighbor's underwear."]}
{"example_id": "184", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Children brought food and left their trash laying on the reading table. happen?", "a": "no", "prior_probs": [0.3762754695839405, 0.6237245304160595], "posterior_probs": [0.6458342491845168, 0.3541657508154832], "prior_entropy": 0.6622113117156057, "posterior_entropy": 0.6499873317680396, "delta_entropy": 0.012223979947566077, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6458342491845168, "accuracy": 0, "tokens_in": 162, "tokens_out": 54, "tokens_total": 216, "latency_total": 0.022220333000404935, "latency_per_module": {"scorer": 0.012403089000144973, "prior": 0.009817244000259961}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 27, "tokens_total": 131}, "prior": {"tokens_in": 58, "tokens_out": 27, "tokens_total": 85}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Monica was at the library with her boyfriend. She kicked them out because they were loitering.\nQuestion: Did Children brought food and left their trash laying on the reading table. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Monica was at the library with her boyfriend. She kicked them out because they were loitering.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Monica was at the library with her boyfriend. She kicked them out because they were loitering.", "hypotheses": ["Children brought food and left their trash laying on the reading table.", "The librarian noticed Monica and her boyfriend were just hanging out reading."]}
{"example_id": "185", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jen applied her mother's makeup and looked like a clown. happen?", "a": "no", "prior_probs": [0.4431772394760715, 0.5568227605239285], "posterior_probs": [0.5929800193687262, 0.40701998063127376], "prior_entropy": 0.6866755555945623, "posterior_entropy": 0.675755553406536, "delta_entropy": 0.010920002188026245, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5929800193687262, "accuracy": 0, "tokens_in": 148, "tokens_out": 60, "tokens_total": 208, "latency_total": 0.02860770200004481, "latency_per_module": {"scorer": 0.01368670700048824, "prior": 0.01492099499955657}, "tokens_per_module": {"scorer": {"tokens_in": 96, "tokens_out": 30, "tokens_total": 126}, "prior": {"tokens_in": 52, "tokens_out": 30, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.\nQuestion: Did Jen applied her mother's makeup and looked like a clown. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.", "hypotheses": ["Jen applied her mother's makeup and looked like a clown.", "Jen's mother let her put on her own makeup but Jen's friends complimented her."]}
{"example_id": "186", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did He would run outside but nobody would be there. happen?", "a": "no", "prior_probs": [0.42914630227747175, 0.5708536977225283], "posterior_probs": [0.5754890564148673, 0.4245109435851328], "prior_entropy": 0.6830728109033353, "posterior_entropy": 0.6817062869172263, "delta_entropy": 0.0013665239861089251, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5754890564148673, "accuracy": 0, "tokens_in": 168, "tokens_out": 64, "tokens_total": 232, "latency_total": 0.024978864999866346, "latency_per_module": {"scorer": 0.012261435999789683, "prior": 0.012717429000076663}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 32, "tokens_total": 136}, "prior": {"tokens_in": 64, "tokens_out": 32, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.\nQuestion: Did He would run outside but nobody would be there. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.", "hypotheses": ["He would run outside but nobody would be there.", "Ted got sick of being woken up, so he stayed up all night to moved out of his house."]}
{"example_id": "187", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did They saw the monster themselves. happen?", "a": "no", "prior_probs": [0.4651342558008747, 0.5348657441991254], "posterior_probs": [0.5139703141935946, 0.48602968580640526], "prior_entropy": 0.6907139661688809, "posterior_entropy": 0.6927567903964722, "delta_entropy": -0.002042824227591278, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5139703141935946, "accuracy": 0, "tokens_in": 176, "tokens_out": 30, "tokens_total": 206, "latency_total": 0.0242122739991828, "latency_per_module": {"scorer": 0.012015568999231618, "prior": 0.01219670499995118}, "tokens_per_module": {"scorer": {"tokens_in": 104, "tokens_out": 15, "tokens_total": 119}, "prior": {"tokens_in": 72, "tokens_out": 15, "tokens_total": 87}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!\nQuestion: Did They saw the monster themselves. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!", "hypotheses": ["They saw the monster themselves.", "Neil asked the locals where to find it."]}
{"example_id": "188", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did It was a tornado outside. happen?", "a": "no", "prior_probs": [0.3994401099647912, 0.6005598900352088], "posterior_probs": [0.5027813624545244, 0.4972186375454755], "prior_entropy": 0.672783997955092, "posterior_entropy": 0.6931317085239437, "delta_entropy": -0.02034771056885165, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5027813624545244, "accuracy": 0, "tokens_in": 156, "tokens_out": 36, "tokens_total": 192, "latency_total": 0.027067839000665117, "latency_per_module": {"scorer": 0.013248792000013054, "prior": 0.013819047000652063}, "tokens_per_module": {"scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}, "prior": {"tokens_in": 62, "tokens_out": 18, "tokens_total": 80}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.\nQuestion: Did It was a tornado outside. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.", "hypotheses": ["It was a tornado outside.", "It was raining and I did not want to get wet."]}
{"example_id": "189", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Jim's wife knew he was tired and made coffee. happen?", "a": "no", "prior_probs": [0.5614921399756847, 0.4385078600243153], "posterior_probs": [0.6450461098131087, 0.3549538901868913], "prior_entropy": 0.6855654335802226, "posterior_entropy": 0.650459471071086, "delta_entropy": 0.03510596250913656, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6450461098131087, "accuracy": 0, "tokens_in": 174, "tokens_out": 32, "tokens_total": 206, "latency_total": 0.03573744099958276, "latency_per_module": {"scorer": 0.01421427799959929, "prior": 0.02152316299998347}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 16, "tokens_total": 124}, "prior": {"tokens_in": 66, "tokens_out": 16, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.\nQuestion: Did Jim's wife knew he was tired and made coffee. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.", "hypotheses": ["Jim's wife knew he was tired and made coffee.", "jim made breakfast."]}
{"example_id": "190", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did After the picnic, it started raining. happen?", "a": "no", "prior_probs": [0.6073004646067773, 0.3926995353932227], "posterior_probs": [0.6531930787063197, 0.3468069212936803], "prior_entropy": 0.6699403183745066, "posterior_entropy": 0.6454475581868361, "delta_entropy": 0.024492760187670548, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6531930787063197, "accuracy": 0, "tokens_in": 140, "tokens_out": 50, "tokens_total": 190, "latency_total": 0.025577262000297196, "latency_per_module": {"scorer": 0.0123166030007269, "prior": 0.013260658999570296}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 25, "tokens_total": 113}, "prior": {"tokens_in": 52, "tokens_out": 25, "tokens_total": 77}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: The family prepared the food and packed it away. The family had a horrible day.\nQuestion: Did After the picnic, it started raining. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: The family prepared the food and packed it away. The family had a horrible day.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The family prepared the food and packed it away. The family had a horrible day.", "hypotheses": ["After the picnic, it started raining.", "Popped a tire and spent their picnic time waiting for a tow-truck."]}
{"example_id": "191", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Tim was able to break through the window. happen?", "a": "no", "prior_probs": [0.5497691534768302, 0.4502308465231699], "posterior_probs": [0.6053511541023504, 0.39464884589764965], "prior_entropy": 0.6881850301904786, "posterior_entropy": 0.6707822147272394, "delta_entropy": 0.017402815463239185, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6053511541023504, "accuracy": 0, "tokens_in": 138, "tokens_out": 34, "tokens_total": 172, "latency_total": 0.02717193900116399, "latency_per_module": {"scorer": 0.01375480500064441, "prior": 0.013417134000519582}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 17, "tokens_total": 105}, "prior": {"tokens_in": 50, "tokens_out": 17, "tokens_total": 67}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Tim never locked his bathroom window. Tim was glad he kept the window unlocked.\nQuestion: Did Tim was able to break through the window. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Tim never locked his bathroom window. Tim was glad he kept the window unlocked.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim never locked his bathroom window. Tim was glad he kept the window unlocked.", "hypotheses": ["Tim was able to break through the window.", "Tim lost his keys to the house."]}
{"example_id": "192", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did The people that live on the old farm has a dog that fears nothing. happen?", "a": "no", "prior_probs": [0.4674719487572201, 0.5325280512427799], "posterior_probs": [0.6272346166209998, 0.37276538337900017], "prior_entropy": 0.6910295370954334, "posterior_entropy": 0.6604110773902402, "delta_entropy": 0.03061845970519317, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6272346166209998, "accuracy": 0, "tokens_in": 174, "tokens_out": 68, "tokens_total": 242, "latency_total": 0.029780167000353686, "latency_per_module": {"scorer": 0.015685733000282198, "prior": 0.014094434000071487}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 34, "tokens_total": 146}, "prior": {"tokens_in": 62, "tokens_out": 34, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.\nQuestion: Did The people that live on the old farm has a dog that fears nothing. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.", "hypotheses": ["The people that live on the old farm has a dog that fears nothing.", "There is an old dog on the farm who has lost its mind and barks all day."]}
{"example_id": "193", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Oren got a part time job delivering pizza. happen?", "a": "no", "prior_probs": [0.5402261165773485, 0.4597738834226515], "posterior_probs": [0.5793395172699904, 0.4206604827300095], "prior_entropy": 0.6899073994065836, "posterior_entropy": 0.680504291173596, "delta_entropy": 0.009403108232987623, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5793395172699904, "accuracy": 0, "tokens_in": 164, "tokens_out": 40, "tokens_total": 204, "latency_total": 0.03035099899989291, "latency_per_module": {"scorer": 0.01407327200013242, "prior": 0.01627772699976049}, "tokens_per_module": {"scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}, "prior": {"tokens_in": 62, "tokens_out": 20, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.\nQuestion: Did Oren got a part time job delivering pizza. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.", "hypotheses": ["Oren got a part time job delivering pizza.", "he went inside and got a part time job."]}
{"example_id": "194", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Beth passed out from overeating. happen?", "a": "no", "prior_probs": [0.5162868080307631, 0.48371319196923696], "posterior_probs": [0.5850686264142406, 0.41493137358575943], "prior_entropy": 0.6926165664692046, "posterior_entropy": 0.6786031910189998, "delta_entropy": 0.014013375450204757, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.5850686264142406, "accuracy": 0, "tokens_in": 182, "tokens_out": 42, "tokens_total": 224, "latency_total": 0.028426575000594312, "latency_per_module": {"scorer": 0.01485744900037389, "prior": 0.013569126000220422}, "tokens_per_module": {"scorer": {"tokens_in": 108, "tokens_out": 21, "tokens_total": 129}, "prior": {"tokens_in": 74, "tokens_out": 21, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.\nQuestion: Did Beth passed out from overeating. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.", "hypotheses": ["Beth passed out from overeating.", "Beth went out and broke her phone with no ride home."]}
{"example_id": "195", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Everyone realized that the power was out for a long time. happen?", "a": "no", "prior_probs": [0.5288894510173028, 0.4711105489826972], "posterior_probs": [0.6031929538281342, 0.3968070461718658], "prior_entropy": 0.6914770498116267, "posterior_entropy": 0.6716957771364067, "delta_entropy": 0.019781272675220007, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6031929538281342, "accuracy": 0, "tokens_in": 168, "tokens_out": 76, "tokens_total": 244, "latency_total": 0.028924274000019068, "latency_per_module": {"scorer": 0.01459006999994017, "prior": 0.014334204000078898}, "tokens_per_module": {"scorer": {"tokens_in": 106, "tokens_out": 38, "tokens_total": 144}, "prior": {"tokens_in": 62, "tokens_out": 38, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.\nQuestion: Did Everyone realized that the power was out for a long time. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.", "hypotheses": ["Everyone realized that the power was out for a long time.", "When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview."]}
{"example_id": "196", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did They were both very shy, but were attracted to eachother without even knowing!. happen?", "a": "no", "prior_probs": [0.564453773628524, 0.435546226371476], "posterior_probs": [0.7247751687444844, 0.2752248312555156], "prior_entropy": 0.6848154375403865, "posterior_entropy": 0.5883866021491069, "delta_entropy": 0.09642883539127967, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7247751687444844, "accuracy": 0, "tokens_in": 168, "tokens_out": 46, "tokens_total": 214, "latency_total": 0.026094960000591527, "latency_per_module": {"scorer": 0.012335021000581037, "prior": 0.01375993900001049}, "tokens_per_module": {"scorer": {"tokens_in": 110, "tokens_out": 23, "tokens_total": 133}, "prior": {"tokens_in": 58, "tokens_out": 23, "tokens_total": 81}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!\nQuestion: Did They were both very shy, but were attracted to eachother without even knowing!. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!", "hypotheses": ["They were both very shy, but were attracted to eachother without even knowing!.", "She went under to say hello."]}
{"example_id": "197", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did the man failed to get a job, until recently. happen?", "a": "no", "prior_probs": [0.5572402122655649, 0.44275978773443514], "posterior_probs": [0.6187027058522457, 0.38129729414775426], "prior_entropy": 0.6865799077622886, "posterior_entropy": 0.6646956445053707, "delta_entropy": 0.021884263256917946, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6187027058522457, "accuracy": 0, "tokens_in": 182, "tokens_out": 38, "tokens_total": 220, "latency_total": 0.021737054000368516, "latency_per_module": {"scorer": 0.009372339000037755, "prior": 0.012364715000330762}, "tokens_per_module": {"scorer": {"tokens_in": 112, "tokens_out": 19, "tokens_total": 131}, "prior": {"tokens_in": 70, "tokens_out": 19, "tokens_total": 89}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.\nQuestion: Did the man failed to get a job, until recently. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.", "hypotheses": ["the man failed to get a job, until recently.", "The man eventually gave up on looking."]}
{"example_id": "198", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake. happen?", "a": "no", "prior_probs": [0.5240675038940131, 0.47593249610598687], "posterior_probs": [0.7034604471341279, 0.296539552865872], "prior_entropy": 0.6919882432894742, "posterior_entropy": 0.6079036983005313, "delta_entropy": 0.08408454498894291, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.7034604471341279, "accuracy": 0, "tokens_in": 182, "tokens_out": 66, "tokens_total": 248, "latency_total": 0.01452388300003804, "latency_per_module": {"scorer": 0.007187782000073639, "prior": 0.007336100999964401}, "tokens_per_module": {"scorer": {"tokens_in": 122, "tokens_out": 33, "tokens_total": 155}, "prior": {"tokens_in": 60, "tokens_out": 33, "tokens_total": 93}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.\nQuestion: Did Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.", "hypotheses": ["Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake.", "Chad didn't eat cause he had the flu."]}
{"example_id": "199", "dataset": "art", "method": "generic_clarify", "asked": true, "q": "Did Ambers friends wanted to see her at the library one night. happen?", "a": "no", "prior_probs": [0.38821194630071754, 0.6117880536992824], "posterior_probs": [0.6491642613563082, 0.3508357386436918], "prior_entropy": 0.667941545366203, "posterior_entropy": 0.6479624595089821, "delta_entropy": 0.019979085857220857, "eig_estimate": 0.0, "pred": 0, "gold": 1, "confidence": 0.6491642613563082, "accuracy": 0, "tokens_in": 130, "tokens_out": 46, "tokens_total": 176, "latency_total": 0.01286670699937531, "latency_per_module": {"scorer": 0.007050330999845755, "prior": 0.005816375999529555}, "tokens_per_module": {"scorer": {"tokens_in": 88, "tokens_out": 23, "tokens_total": 111}, "prior": {"tokens_in": 42, "tokens_out": 23, "tokens_total": 65}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"scorer": "Observation: Amber loves to read. She went to the library instead.\nQuestion: Did Ambers friends wanted to see her at the library one night. happen?\nAnswer: no\nHypothesis: ", "prior": "Observation: Amber loves to read. She went to the library instead.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber loves to read. She went to the library instead.", "hypotheses": ["Ambers friends wanted to see her at the library one night.", "Amber liked having a book in her hand."]}
{"example_id": "0", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ron ignores his bosses's orders and called him an idiot. happen?", "a": "no", "prior_probs": [0.4620790510502866, 0.5379209489497133], "posterior_probs": [0.578627754945582, 0.421372245054418], "prior_entropy": 0.690268420337182, "posterior_entropy": 0.680731060478617, "delta_entropy": 0.009537359858565075, "eig_estimate": 0.009537359858565075, "pred": 0, "gold": 1, "confidence": 0.578627754945582, "accuracy": 0, "tokens_in": 152, "tokens_out": 340, "tokens_total": 492, "latency_total": 0.3731344369989529, "latency_per_module": {"eig": [0.25735866600007284, 0.10936676499932219], "scorer": 0.0064090059995578486}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ron ignores his bosses's orders and called him an idiot. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ron's boss called him an idiot. happen?\nAnswer:"], "scorer": "Observation: Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.\nQuestion: Did Ron ignores his bosses's orders and called him an idiot. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ron started his new job as a landscaper today. Ron is immediately fired for insubordination.", "hypotheses": ["Ron ignores his bosses's orders and called him an idiot.", "Ron's boss called him an idiot."]}
{"example_id": "1", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She partied all night. happen?", "a": "yes", "prior_probs": [0.49074052349518316, 0.5092594765048167], "posterior_probs": [0.36364258354358314, 0.6363574164564169], "prior_entropy": 0.6929756949449963, "posterior_entropy": 0.6554852545740804, "delta_entropy": 0.03749044037091598, "eig_estimate": 0.0368114243276287, "pred": 1, "gold": 1, "confidence": 0.6363574164564169, "accuracy": 1, "tokens_in": 110, "tokens_out": 332, "tokens_total": 442, "latency_total": 0.20965599400096835, "latency_per_module": {"eig": [0.10159711400046945, 0.10156689799987362], "scorer": 0.006491982000625285}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}, {"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}], "scorer": {"tokens_in": 70, "tokens_out": 12, "tokens_total": 82}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did It stormed in New York. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She partied all night. happen?\nAnswer:"], "scorer": "Observation: Sandy lived in New York. Sandy was prepared.\nQuestion: Did She partied all night. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sandy lived in New York. Sandy was prepared.", "hypotheses": ["It stormed in New York.", "She partied all night."]}
{"example_id": "2", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Mary and her mom decided to make chocolate covered frozen bananas to avoid waste. happen?", "a": "no", "prior_probs": [0.5653405163187792, 0.4346594836812207], "posterior_probs": [0.7032779368279838, 0.29672206317201616], "prior_entropy": 0.6845839433504841, "posterior_entropy": 0.6080612765378028, "delta_entropy": 0.07652266681268127, "eig_estimate": 0.07675513828992697, "pred": 0, "gold": 1, "confidence": 0.7032779368279838, "accuracy": 0, "tokens_in": 169, "tokens_out": 342, "tokens_total": 511, "latency_total": 0.20749667900054192, "latency_per_module": {"eig": [0.10216385300009279, 0.09865442299997085], "scorer": 0.0066784030004782835}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 118, "tokens_out": 23, "tokens_total": 141}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Mary and her mom decided to make chocolate covered frozen bananas to avoid waste. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did So Mary made pineapple splits for everyone. happen?\nAnswer:"], "scorer": "Observation: Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!\nQuestion: Did Mary and her mom decided to make chocolate covered frozen bananas to avoid waste. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary's mom came home with more bananas than they could possibly eat. That was the best way ever to eat a banana!", "hypotheses": ["Mary and her mom decided to make chocolate covered frozen bananas to avoid waste.", "So Mary made pineapple splits for everyone."]}
{"example_id": "3", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jim found he was missing an item. happen?", "a": "no", "prior_probs": [0.5590702485504583, 0.44092975144954183], "posterior_probs": [0.6138264604825773, 0.3861735395174226], "prior_entropy": 0.6861522671508851, "posterior_entropy": 0.6670056546319258, "delta_entropy": 0.019146612518959216, "eig_estimate": 0.01921185087913657, "pred": 0, "gold": 1, "confidence": 0.6138264604825773, "accuracy": 0, "tokens_in": 132, "tokens_out": 335, "tokens_total": 467, "latency_total": 0.20472778600014863, "latency_per_module": {"eig": [0.10110171900032583, 0.09746218599957501], "scorer": 0.006163881000247784}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jim found he was missing an item. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jim needed a certain animal for it. happen?\nAnswer:"], "scorer": "Observation: Jim was working on a project. Luckily, he found it on a nearby shelf.\nQuestion: Did Jim found he was missing an item. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was working on a project. Luckily, he found it on a nearby shelf.", "hypotheses": ["Jim found he was missing an item.", "Jim needed a certain animal for it."]}
{"example_id": "4", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He leaned too far back and his chair tipped over. happen?", "a": "yes", "prior_probs": [0.5075695691361044, 0.49243043086389565], "posterior_probs": [0.35525806242094615, 0.6447419375790538], "prior_entropy": 0.6930325794262594, "posterior_entropy": 0.6506409614420963, "delta_entropy": 0.04239161798416313, "eig_estimate": 0.04239161798416313, "pred": 1, "gold": 1, "confidence": 0.6447419375790538, "accuracy": 1, "tokens_in": 150, "tokens_out": 340, "tokens_total": 490, "latency_total": 0.21715991099972598, "latency_per_module": {"eig": [0.10771976200067002, 0.10356325899920193], "scorer": 0.005876889999854029}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He noticed the chair leg was falling off. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He leaned too far back and his chair tipped over. happen?\nAnswer:"], "scorer": "Observation: Sean was sitting at his desk. After a minute, he was able to put the chair back together.\nQuestion: Did He leaned too far back and his chair tipped over. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sean was sitting at his desk. After a minute, he was able to put the chair back together.", "hypotheses": ["He noticed the chair leg was falling off.", "He leaned too far back and his chair tipped over."]}
{"example_id": "5", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Pablo thought that worms were a delicious source of protein. happen?", "a": "no", "prior_probs": [0.6068721153854889, 0.39312788461451104], "posterior_probs": [0.663092299505226, 0.3369077004947741], "prior_entropy": 0.6701266849595834, "posterior_entropy": 0.6389630355504997, "delta_entropy": 0.03116364940908367, "eig_estimate": 0.031185470326482823, "pred": 0, "gold": 1, "confidence": 0.663092299505226, "accuracy": 0, "tokens_in": 133, "tokens_out": 342, "tokens_total": 475, "latency_total": 0.20121706599911704, "latency_per_module": {"eig": [0.09724557299978187, 0.09778399099923263], "scorer": 0.0061875020001025405}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 161, "tokens_total": 186}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 86, "tokens_out": 21, "tokens_total": 107}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Pablo thought that worms were a delicious source of protein. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Pablo then learned what worms really are. happen?\nAnswer:"], "scorer": "Observation: Pablo likes to eat worms. Pablo does not enjoy eating worms.\nQuestion: Did Pablo thought that worms were a delicious source of protein. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pablo likes to eat worms. Pablo does not enjoy eating worms.", "hypotheses": ["Pablo thought that worms were a delicious source of protein.", "Pablo then learned what worms really are."]}
{"example_id": "6", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The scientist collected samples of the bacteria and tested them. happen?", "a": "no", "prior_probs": [0.5630861110677394, 0.4369138889322606], "posterior_probs": [0.6252826534032552, 0.3747173465967448], "prior_entropy": 0.6851662110791645, "posterior_entropy": 0.661418681099528, "delta_entropy": 0.023747529979636584, "eig_estimate": 0.023931889598275125, "pred": 0, "gold": 1, "confidence": 0.6252826534032552, "accuracy": 0, "tokens_in": 141, "tokens_out": 339, "tokens_total": 480, "latency_total": 0.20713428299950465, "latency_per_module": {"eig": [0.10175762299968483, 0.09914576899973326], "scorer": 0.006230891000086558}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 94, "tokens_out": 19, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The scientist collected samples of the bacteria and tested them. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He collected the bacteria and froze it. happen?\nAnswer:"], "scorer": "Observation: There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.\nQuestion: Did The scientist collected samples of the bacteria and tested them. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a scientist who wanted to collect bacteria samples. The bacteria was non harmful.", "hypotheses": ["The scientist collected samples of the bacteria and tested them.", "He collected the bacteria and froze it."]}
{"example_id": "7", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My commanding officer told me I wasn't doing bad at my job. happen?", "a": "no", "prior_probs": [0.6387544124977744, 0.36124558750222546], "posterior_probs": [0.7042901005113327, 0.2957098994886673], "prior_entropy": 0.6541314959287188, "posterior_entropy": 0.6071853672294203, "delta_entropy": 0.046946128699298484, "eig_estimate": 0.04711078463459617, "pred": 0, "gold": 1, "confidence": 0.7042901005113327, "accuracy": 0, "tokens_in": 155, "tokens_out": 340, "tokens_total": 495, "latency_total": 0.20610107999891625, "latency_per_module": {"eig": [0.10193695199996, 0.09800281099978747], "scorer": 0.0061613169991687755}, "tokens_per_module": {"eig": [{"tokens_in": 28, "tokens_out": 159, "tokens_total": 187}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 106, "tokens_out": 21, "tokens_total": 127}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did My commanding officer told me I wasn't doing bad at my job. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did My drill sergeant insulted my mother. happen?\nAnswer:"], "scorer": "Observation: I joined the Navy. That angered me so I hit him and was arrested by the military police.\nQuestion: Did My commanding officer told me I wasn't doing bad at my job. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I joined the Navy. That angered me so I hit him and was arrested by the military police.", "hypotheses": ["My commanding officer told me I wasn't doing bad at my job.", "My drill sergeant insulted my mother."]}
{"example_id": "8", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Dotty ate something bad. happen?", "a": "no", "prior_probs": [0.5659128800175401, 0.43408711998245986], "posterior_probs": [0.6220588627167835, 0.3779411372832165], "prior_entropy": 0.6844328221404599, "posterior_entropy": 0.6630472116491068, "delta_entropy": 0.021385610491353013, "eig_estimate": 0.021385610491353013, "pred": 0, "gold": 1, "confidence": 0.6220588627167835, "accuracy": 0, "tokens_in": 121, "tokens_out": 334, "tokens_total": 455, "latency_total": 0.205851186999098, "latency_per_module": {"eig": [0.10039013499954308, 0.09725182100009988], "scorer": 0.00820923099945503}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 158, "tokens_total": 178}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 78, "tokens_out": 17, "tokens_total": 95}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Dotty ate something bad. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Dotty call some close friends to chat. happen?\nAnswer:"], "scorer": "Observation: Dotty was being very grumpy. She felt much better afterwards.\nQuestion: Did Dotty ate something bad. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dotty was being very grumpy. She felt much better afterwards.", "hypotheses": ["Dotty ate something bad.", "Dotty call some close friends to chat."]}
{"example_id": "9", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ali did not want to take karate. happen?", "a": "no", "prior_probs": [0.5599978275854001, 0.4400021724145999], "posterior_probs": [0.6140151489071012, 0.3859848510928988], "prior_entropy": 0.6859303241447694, "posterior_entropy": 0.666918136520481, "delta_entropy": 0.019012187624288357, "eig_estimate": 0.018819807233161748, "pred": 0, "gold": 1, "confidence": 0.6140151489071012, "accuracy": 0, "tokens_in": 149, "tokens_out": 337, "tokens_total": 486, "latency_total": 0.20878459600044152, "latency_per_module": {"eig": [0.10383819199978461, 0.09916078800051764], "scorer": 0.005785616000139271}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ali did not want to take karate. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ali did horribly in her last class. happen?\nAnswer:"], "scorer": "Observation: Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.\nQuestion: Did Ali did not want to take karate. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ali's mom enrolled her in a karate class. Ali was so embarrassed she didn't tell any of her friends.", "hypotheses": ["Ali did not want to take karate.", "Ali did horribly in her last class."]}
{"example_id": "10", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Cory was teased by some of the kids in his classroom. happen?", "a": "no", "prior_probs": [0.4883579795936534, 0.5116420204063465], "posterior_probs": [0.6087945220562659, 0.391205477943734], "prior_entropy": 0.6928760827807745, "posterior_entropy": 0.6692842588175162, "delta_entropy": 0.023591823963258318, "eig_estimate": 0.023720938857637888, "pred": 0, "gold": 1, "confidence": 0.6087945220562659, "accuracy": 0, "tokens_in": 159, "tokens_out": 345, "tokens_total": 504, "latency_total": 0.20807582600082242, "latency_per_module": {"eig": [0.10173793400008435, 0.10008434600058536], "scorer": 0.00625354600015271}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 108, "tokens_out": 25, "tokens_total": 133}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Cory was teased by some of the kids in his classroom. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Cory ran away from home as fast as he could. happen?\nAnswer:"], "scorer": "Observation: A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.\nQuestion: Did Cory was teased by some of the kids in his classroom. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A sob escaped Cory's lips. When he got home, he curled into a ball in the corner and cried.", "hypotheses": ["Cory was teased by some of the kids in his classroom.", "Cory ran away from home as fast as he could."]}
{"example_id": "11", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Dennis has been a member for ten seconds. happen?", "a": "yes", "prior_probs": [0.5310383771489922, 0.4689616228510078], "posterior_probs": [0.375237514920285, 0.624762485079715], "prior_entropy": 0.6912191794642322, "posterior_entropy": 0.6616844465249947, "delta_entropy": 0.029534732939237474, "eig_estimate": 0.029244852592456506, "pred": 1, "gold": 1, "confidence": 0.624762485079715, "accuracy": 1, "tokens_in": 145, "tokens_out": 337, "tokens_total": 482, "latency_total": 0.2080723310000394, "latency_per_module": {"eig": [0.10192633800033946, 0.0999244530003125], "scorer": 0.00622153999938746}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 100, "tokens_out": 18, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did People went to watch the band play. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Dennis has been a member for ten seconds. happen?\nAnswer:"], "scorer": "Observation: Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.\nQuestion: Did Dennis has been a member for ten seconds. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Many young adults play in the marching band in school. Dennis loves his marching band and enjoys entertaining the fans.", "hypotheses": ["People went to watch the band play.", "Dennis has been a member for ten seconds."]}
{"example_id": "12", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Deb had a lot of coupons. happen?", "a": "yes", "prior_probs": [0.42302664764035913, 0.5769733523596409], "posterior_probs": [0.361789197358995, 0.6382108026410049], "prior_entropy": 0.6812501313089362, "posterior_entropy": 0.6544406927062933, "delta_entropy": 0.026809438602642843, "eig_estimate": 0.026391103990608535, "pred": 1, "gold": 1, "confidence": 0.6382108026410049, "accuracy": 1, "tokens_in": 131, "tokens_out": 337, "tokens_total": 468, "latency_total": 0.20532895199994528, "latency_per_module": {"eig": [0.10027296400039631, 0.09886881700003869], "scorer": 0.006187170999510272}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 86, "tokens_out": 17, "tokens_total": 103}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Deb went to a matinee movie instead. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Deb had a lot of coupons. happen?\nAnswer:"], "scorer": "Observation: Deb wanted to go shopping. She found everything she needed and had money left over.\nQuestion: Did Deb had a lot of coupons. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Deb wanted to go shopping. She found everything she needed and had money left over.", "hypotheses": ["Deb went to a matinee movie instead.", "Deb had a lot of coupons."]}
{"example_id": "13", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Kory stole from the airport. happen?", "a": "no", "prior_probs": [0.617982844061609, 0.38201715593839114], "posterior_probs": [0.642384061401763, 0.357615938598237], "prior_entropy": 0.6650429923305812, "posterior_entropy": 0.6520341448372071, "delta_entropy": 0.013008847493374054, "eig_estimate": 0.013477976558888605, "pred": 0, "gold": 1, "confidence": 0.642384061401763, "accuracy": 0, "tokens_in": 134, "tokens_out": 338, "tokens_total": 472, "latency_total": 0.22598416799974075, "latency_per_module": {"eig": [0.11670658099956199, 0.10292993599978217], "scorer": 0.006347651000396581}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 88, "tokens_out": 18, "tokens_total": 106}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Kory stole from the airport. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He got caught anti-shoplifting from passengers. happen?\nAnswer:"], "scorer": "Observation: My cousin Kory was working at the airport. He is now serving out his sentence.\nQuestion: Did Kory stole from the airport. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My cousin Kory was working at the airport. He is now serving out his sentence.", "hypotheses": ["Kory stole from the airport.", "He got caught anti-shoplifting from passengers."]}
{"example_id": "14", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Daniel stayed home and didn't want to buy a plane. happen?", "a": "yes", "prior_probs": [0.3696246176351904, 0.6303753823648096], "posterior_probs": [0.34661078515060306, 0.6533892148493969], "prior_entropy": 0.6587555935763323, "posterior_entropy": 0.6453232985827884, "delta_entropy": 0.013432294993543925, "eig_estimate": 0.0129046940590116, "pred": 1, "gold": 1, "confidence": 0.6533892148493969, "accuracy": 1, "tokens_in": 171, "tokens_out": 339, "tokens_total": 510, "latency_total": 0.21154275500066433, "latency_per_module": {"eig": [0.10625982700003078, 0.09859139000036521], "scorer": 0.006691538000268338}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 124, "tokens_out": 19, "tokens_total": 143}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He opened a lemonade stand. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Daniel stayed home and didn't want to buy a plane. happen?\nAnswer:"], "scorer": "Observation: Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!\nQuestion: Did Daniel stayed home and didn't want to buy a plane. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Daniel wanted to buy a toy plane, but he didn't have any money. He bought his toy plane, and kept working so he could buy another!", "hypotheses": ["He opened a lemonade stand.", "Daniel stayed home and didn't want to buy a plane."]}
{"example_id": "15", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Her neck pain stopped because of this. happen?", "a": "no", "prior_probs": [0.5233176960502864, 0.47668230394971345], "posterior_probs": [0.6492351193543503, 0.3507648806456497], "prior_entropy": 0.6920593561485264, "posterior_entropy": 0.6479188447660982, "delta_entropy": 0.04414051138242825, "eig_estimate": 0.044274190612613215, "pred": 0, "gold": 1, "confidence": 0.6492351193543503, "accuracy": 0, "tokens_in": 139, "tokens_out": 335, "tokens_total": 474, "latency_total": 0.20511409699975047, "latency_per_module": {"eig": [0.09842690999994375, 0.1003808260002188], "scorer": 0.006306360999587923}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}], "scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Her neck pain stopped because of this. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jenna pulled a muscle lifting weights. happen?\nAnswer:"], "scorer": "Observation: Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.\nQuestion: Did Her neck pain stopped because of this. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jenna hit the weight hard in the gym. She took a cold bath in order to alleviate her pain.", "hypotheses": ["Her neck pain stopped because of this.", "Jenna pulled a muscle lifting weights."]}
{"example_id": "16", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Kat decided to take a nap instead of eating. happen?", "a": "yes", "prior_probs": [0.4115371464631088, 0.5884628535368912], "posterior_probs": [0.37231178719893504, 0.6276882128010649], "prior_entropy": 0.6774131328021895, "posterior_entropy": 0.6601745988312963, "delta_entropy": 0.017238533970893255, "eig_estimate": 0.0174701261910726, "pred": 1, "gold": 1, "confidence": 0.6276882128010649, "accuracy": 1, "tokens_in": 141, "tokens_out": 338, "tokens_total": 479, "latency_total": 0.204673922999973, "latency_per_module": {"eig": [0.10061423600018315, 0.09778186299990921], "scorer": 0.0062778239998806384}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 24, "tokens_out": 161, "tokens_total": 185}], "scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Kat went to get a salad. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Kat decided to take a nap instead of eating. happen?\nAnswer:"], "scorer": "Observation: It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.\nQuestion: Did Kat decided to take a nap instead of eating. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was lunch time and Kat was hungry. Kat and her coworkers enjoyed a nice lunch outside.", "hypotheses": ["Kat went to get a salad.", "Kat decided to take a nap instead of eating."]}
{"example_id": "17", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The vet put Cosmo on a treadmill. happen?", "a": "yes", "prior_probs": [0.43587771834479677, 0.5641222816552033], "posterior_probs": [0.34180141518527535, 0.6581985848147246], "prior_entropy": 0.6849011558642406, "posterior_entropy": 0.6422231217133171, "delta_entropy": 0.04267803415092353, "eig_estimate": 0.04267803415092353, "pred": 1, "gold": 1, "confidence": 0.6581985848147246, "accuracy": 1, "tokens_in": 135, "tokens_out": 339, "tokens_total": 474, "latency_total": 0.20729810600005294, "latency_per_module": {"eig": [0.10180237300028239, 0.0992693660000441], "scorer": 0.0062263669997264515}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 88, "tokens_out": 19, "tokens_total": 107}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did His owner gave him a lower fat cat food. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The vet put Cosmo on a treadmill. happen?\nAnswer:"], "scorer": "Observation: Cosmo was a pudgy cat. Now he's fit and muscular!\nQuestion: Did The vet put Cosmo on a treadmill. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cosmo was a pudgy cat. Now he's fit and muscular!", "hypotheses": ["His owner gave him a lower fat cat food.", "The vet put Cosmo on a treadmill."]}
{"example_id": "18", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tim could not find his socks. happen?", "a": "yes", "prior_probs": [0.4609254882025264, 0.5390745117974737], "posterior_probs": [0.42281367909483364, 0.5771863209051663], "prior_entropy": 0.6900904297666779, "posterior_entropy": 0.6811839412802924, "delta_entropy": 0.008906488486385533, "eig_estimate": 0.008906488486385533, "pred": 1, "gold": 1, "confidence": 0.5771863209051663, "accuracy": 1, "tokens_in": 148, "tokens_out": 333, "tokens_total": 481, "latency_total": 0.2043003119997593, "latency_per_module": {"eig": [0.09868294399984734, 0.09982068499994057], "scorer": 0.005796682999971381}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}], "scorer": {"tokens_in": 106, "tokens_out": 14, "tokens_total": 120}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tim became very sick one day. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tim could not find his socks. happen?\nAnswer:"], "scorer": "Observation: Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.\nQuestion: Did Tim could not find his socks. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim was a boy scout, and went on many camping trips with his friends. Eventually, Tim had to go home without any socks.", "hypotheses": ["Tim became very sick one day.", "Tim could not find his socks."]}
{"example_id": "19", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Christian grabbed the gun and shot Adam in the eye. happen?", "a": "yes", "prior_probs": [0.4207589138170007, 0.5792410861829993], "posterior_probs": [0.3602046531786242, 0.6397953468213757], "prior_entropy": 0.6805357754094549, "posterior_entropy": 0.6535358540085932, "delta_entropy": 0.026999921400861715, "eig_estimate": 0.026994429550135535, "pred": 1, "gold": 1, "confidence": 0.6397953468213757, "accuracy": 1, "tokens_in": 141, "tokens_out": 339, "tokens_total": 480, "latency_total": 0.2043323080015398, "latency_per_module": {"eig": [0.10050359800061415, 0.09755014900019887], "scorer": 0.006278561000726768}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 158, "tokens_total": 182}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Adam's brother Christian was afraid of the guns. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Christian grabbed the gun and shot Adam in the eye. happen?\nAnswer:"], "scorer": "Observation: One day Adam bought two BB guns. Adam took the gun away from Christian.\nQuestion: Did Christian grabbed the gun and shot Adam in the eye. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "One day Adam bought two BB guns. Adam took the gun away from Christian.", "hypotheses": ["Adam's brother Christian was afraid of the guns.", "Christian grabbed the gun and shot Adam in the eye."]}
{"example_id": "20", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My friend who is a hunter found lots of elk. happen?", "a": "yes", "prior_probs": [0.41679212274760574, 0.5832078772523943], "posterior_probs": [0.3513299182613752, 0.6486700817386248], "prior_entropy": 0.6792354461007641, "posterior_entropy": 0.6482660256313024, "delta_entropy": 0.030969420469461717, "eig_estimate": 0.03081201603322128, "pred": 1, "gold": 1, "confidence": 0.6486700817386248, "accuracy": 1, "tokens_in": 142, "tokens_out": 341, "tokens_total": 483, "latency_total": 0.20509381800002302, "latency_per_module": {"eig": [0.10022691400081385, 0.09862679499929072], "scorer": 0.006240108999918448}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}], "scorer": {"tokens_in": 92, "tokens_out": 22, "tokens_total": 114}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She set up a hunting blind in the woods. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did My friend who is a hunter found lots of elk. happen?\nAnswer:"], "scorer": "Observation: My friend is a hunter. The elk was nowhere to be found.\nQuestion: Did My friend who is a hunter found lots of elk. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend is a hunter. The elk was nowhere to be found.", "hypotheses": ["She set up a hunting blind in the woods.", "My friend who is a hunter found lots of elk."]}
{"example_id": "21", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I didn't study for the test. happen?", "a": "yes", "prior_probs": [0.4435230981074238, 0.5564769018925761], "posterior_probs": [0.3642835334420612, 0.6357164665579388], "prior_entropy": 0.6867542648829669, "posterior_entropy": 0.6558430356005807, "delta_entropy": 0.03091122928238621, "eig_estimate": 0.030711854345329283, "pred": 1, "gold": 1, "confidence": 0.6357164665579388, "accuracy": 1, "tokens_in": 122, "tokens_out": 335, "tokens_total": 457, "latency_total": 0.20495033699990017, "latency_per_module": {"eig": [0.098170835000019, 0.10062102899973979], "scorer": 0.0061584730001413845}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 78, "tokens_out": 16, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I saw the string by the door. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I didn't study for the test. happen?\nAnswer:"], "scorer": "Observation: I walked into my math class. I ended up failing.\nQuestion: Did I didn't study for the test. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I walked into my math class. I ended up failing.", "hypotheses": ["I saw the string by the door.", "I didn't study for the test."]}
{"example_id": "22", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did we bought the owners grandmother a new pc. happen?", "a": "no", "prior_probs": [0.5059038040919378, 0.4940961959080622], "posterior_probs": [0.6598127430352833, 0.3401872569647167], "prior_entropy": 0.693077469132524, "posterior_entropy": 0.6411596062170226, "delta_entropy": 0.05191786291550138, "eig_estimate": 0.05085224037323732, "pred": 0, "gold": 1, "confidence": 0.6598127430352833, "accuracy": 0, "tokens_in": 151, "tokens_out": 334, "tokens_total": 485, "latency_total": 0.20472089199938637, "latency_per_module": {"eig": [0.09948801799964713, 0.09932515399941622], "scorer": 0.005907720000323025}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 22, "tokens_out": 157, "tokens_total": 179}], "scorer": {"tokens_in": 106, "tokens_out": 17, "tokens_total": 123}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did we bought the owners grandmother a new pc. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Our founder Rachel only uses the PC. happen?\nAnswer:"], "scorer": "Observation: In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.\nQuestion: Did we bought the owners grandmother a new pc. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "In 1989, our company started using personal computers. When she got her pc later she did not know how to use it.", "hypotheses": ["we bought the owners grandmother a new pc.", "Our founder Rachel only uses the PC."]}
{"example_id": "23", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did It seemed that the cold weather stopped for two months. happen?", "a": "yes", "prior_probs": [0.432870459419034, 0.5671295405809661], "posterior_probs": [0.32125783913224776, 0.6787421608677522], "prior_entropy": 0.6841071564298835, "posterior_entropy": 0.6278139482787215, "delta_entropy": 0.05629320815116201, "eig_estimate": 0.05686684458334024, "pred": 1, "gold": 1, "confidence": 0.6787421608677522, "accuracy": 1, "tokens_in": 142, "tokens_out": 336, "tokens_total": 478, "latency_total": 0.2062961469991933, "latency_per_module": {"eig": [0.10295799899995473, 0.09733709699958126], "scorer": 0.006001050999657309}, "tokens_per_module": {"eig": [{"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 98, "tokens_out": 16, "tokens_total": 114}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Mary wears two jackets. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did It seemed that the cold weather stopped for two months. happen?\nAnswer:"], "scorer": "Observation: Mary doesn't like cold weather. At least until she can afford to move to warmer state.\nQuestion: Did It seemed that the cold weather stopped for two months. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary doesn't like cold weather. At least until she can afford to move to warmer state.", "hypotheses": ["Mary wears two jackets.", "It seemed that the cold weather stopped for two months."]}
{"example_id": "24", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers. happen?", "a": "yes", "prior_probs": [0.5128077854715616, 0.48719221452843847], "posterior_probs": [0.2547552668331887, 0.7452447331668112], "prior_entropy": 0.6928190659326015, "posterior_entropy": 0.5674992911138368, "delta_entropy": 0.12531977481876466, "eig_estimate": 0.125035012167166, "pred": 1, "gold": 1, "confidence": 0.7452447331668112, "accuracy": 1, "tokens_in": 217, "tokens_out": 364, "tokens_total": 581, "latency_total": 0.21487101299862843, "latency_per_module": {"eig": [0.10210909599936713, 0.10498756800006959], "scorer": 0.007774348999191716}, "tokens_per_module": {"eig": [{"tokens_in": 30, "tokens_out": 160, "tokens_total": 190}, {"tokens_in": 43, "tokens_out": 159, "tokens_total": 202}], "scorer": {"tokens_in": 144, "tokens_out": 45, "tokens_total": 189}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did They started getting followed by a policeman, ran, and hid behind a building. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers. happen?\nAnswer:"], "scorer": "Observation: Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.\nQuestion: Did The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy and her friends were out at 3 AM. They stayed there breathing hard, and praying they hadn't been seen.", "hypotheses": ["They started getting followed by a policeman, ran, and hid behind a building.", "The decided to break into the football field. When suddenly they saw a flashlight comming towards them. They all started running for the bleachers."]}
{"example_id": "25", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Bob got away with sneaking out. happen?", "a": "yes", "prior_probs": [0.47422526177721863, 0.5257747382227814], "posterior_probs": [0.4284412900887107, 0.5715587099112893], "prior_entropy": 0.6918179172122214, "posterior_entropy": 0.6828706316114652, "delta_entropy": 0.008947285600756194, "eig_estimate": 0.008313681808189656, "pred": 1, "gold": 1, "confidence": 0.5715587099112893, "accuracy": 1, "tokens_in": 133, "tokens_out": 331, "tokens_total": 464, "latency_total": 0.2113198220013146, "latency_per_module": {"eig": [0.10360557700005302, 0.10152167800060852], "scorer": 0.006192567000653071}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 158, "tokens_total": 178}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 92, "tokens_out": 13, "tokens_total": 105}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Bob got caught sneaking out. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Bob got away with sneaking out. happen?\nAnswer:"], "scorer": "Observation: Bob's parents grounded him. He came back home but his parents didn't even know he left.\nQuestion: Did Bob got away with sneaking out. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob's parents grounded him. He came back home but his parents didn't even know he left.", "hypotheses": ["Bob got caught sneaking out.", "Bob got away with sneaking out."]}
{"example_id": "26", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Amy won an award for how much work she accomplished and was given the same quota. happen?", "a": "no", "prior_probs": [0.48297515281453285, 0.5170248471854672], "posterior_probs": [0.6638150680427155, 0.3361849319572845], "prior_entropy": 0.6925673776487646, "posterior_entropy": 0.6384724756259421, "delta_entropy": 0.05409490202282252, "eig_estimate": 0.05424854269417777, "pred": 0, "gold": 1, "confidence": 0.6638150680427155, "accuracy": 0, "tokens_in": 171, "tokens_out": 346, "tokens_total": 517, "latency_total": 0.20827951399951417, "latency_per_module": {"eig": [0.10186611599965545, 0.09940756200012402], "scorer": 0.0070058359997347}, "tokens_per_module": {"eig": [{"tokens_in": 31, "tokens_out": 159, "tokens_total": 190}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 116, "tokens_out": 27, "tokens_total": 143}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Amy won an award for how much work she accomplished and was given the same quota. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Amy's boss said she needed to do more. happen?\nAnswer:"], "scorer": "Observation: Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.\nQuestion: Did Amy won an award for how much work she accomplished and was given the same quota. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy worked in the stockroom at Marshall's. Amy decided she would push herself to meet her new quota.", "hypotheses": ["Amy won an award for how much work she accomplished and was given the same quota.", "Amy's boss said she needed to do more."]}
{"example_id": "27", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He didn't let his inspiration go to waste, he trained and trained. happen?", "a": "no", "prior_probs": [0.6126871071764336, 0.3873128928235664], "posterior_probs": [0.7139302465013939, 0.28606975349860614], "prior_entropy": 0.6675309227274598, "posterior_entropy": 0.5985949909209831, "delta_entropy": 0.06893593180647672, "eig_estimate": 0.06865839553296574, "pred": 0, "gold": 1, "confidence": 0.7139302465013939, "accuracy": 0, "tokens_in": 162, "tokens_out": 340, "tokens_total": 502, "latency_total": 0.21316748299977917, "latency_per_module": {"eig": [0.10490902699984872, 0.1020303920004153], "scorer": 0.006228063999515143}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}], "scorer": {"tokens_in": 114, "tokens_out": 20, "tokens_total": 134}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He didn't let his inspiration go to waste, he trained and trained. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jason learned to knit. happen?\nAnswer:"], "scorer": "Observation: Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.\nQuestion: Did He didn't let his inspiration go to waste, he trained and trained. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jason had always admired the fast movements of boxers he saw on TV. He won his first fight by knockout.", "hypotheses": ["He didn't let his inspiration go to waste, he trained and trained.", "Jason learned to knit."]}
{"example_id": "28", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Erin, practiced drawing at home and became recognized for her talent. happen?", "a": "yes", "prior_probs": [0.4758078557354764, 0.5241921442645237], "posterior_probs": [0.3772105452749935, 0.6227894547250066], "prior_entropy": 0.6919762037360642, "posterior_entropy": 0.6626820249170335, "delta_entropy": 0.029294178819030625, "eig_estimate": 0.028895239785691064, "pred": 1, "gold": 1, "confidence": 0.6227894547250066, "accuracy": 1, "tokens_in": 145, "tokens_out": 344, "tokens_total": 489, "latency_total": 0.20617485200000374, "latency_per_module": {"eig": [0.10061982200022612, 0.09949627300011343], "scorer": 0.006058756999664183}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 94, "tokens_out": 25, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Erin, practiced drawing at home with no luck. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Erin, practiced drawing at home and became recognized for her talent. happen?\nAnswer:"], "scorer": "Observation: Erin tried to learn how to draw. So she joined a drawing class.\nQuestion: Did Erin, practiced drawing at home and became recognized for her talent. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erin tried to learn how to draw. So she joined a drawing class.", "hypotheses": ["Erin, practiced drawing at home with no luck.", "Erin, practiced drawing at home and became recognized for her talent."]}
{"example_id": "29", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jon crashed the police car into a telephone poll. happen?", "a": "no", "prior_probs": [0.47143509427231317, 0.5285649057276869], "posterior_probs": [0.6258173277287377, 0.37418267227126234], "prior_entropy": 0.6915143840109692, "posterior_entropy": 0.6611443006749416, "delta_entropy": 0.030370083336027642, "eig_estimate": 0.030831928927268007, "pred": 0, "gold": 1, "confidence": 0.6258173277287377, "accuracy": 0, "tokens_in": 133, "tokens_out": 335, "tokens_total": 468, "latency_total": 0.20558380100010254, "latency_per_module": {"eig": [0.09938278299978265, 0.09998848600025667], "scorer": 0.00621253200006322}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}], "scorer": {"tokens_in": 90, "tokens_out": 15, "tokens_total": 105}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jon crashed the police car into a telephone poll. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jon wasn't caught. happen?\nAnswer:"], "scorer": "Observation: Jon decided to steal a police car. Jon went to prison for three years.\nQuestion: Did Jon crashed the police car into a telephone poll. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jon decided to steal a police car. Jon went to prison for three years.", "hypotheses": ["Jon crashed the police car into a telephone poll.", "Jon wasn't caught."]}
{"example_id": "30", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did After getting a good grade, I learned an easy lesson. happen?", "a": "yes", "prior_probs": [0.47574630971646853, 0.5242536902835314], "posterior_probs": [0.34275030904397885, 0.6572496909560211], "prior_entropy": 0.6919702357676847, "posterior_entropy": 0.642842909583263, "delta_entropy": 0.04912732618442173, "eig_estimate": 0.048950270903588326, "pred": 1, "gold": 1, "confidence": 0.6572496909560211, "accuracy": 1, "tokens_in": 144, "tokens_out": 337, "tokens_total": 481, "latency_total": 0.20852716299941676, "latency_per_module": {"eig": [0.10077445200022339, 0.10172253499968065], "scorer": 0.006030175999512721}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 98, "tokens_out": 18, "tokens_total": 116}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I failed a big test. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did After getting a good grade, I learned an easy lesson. happen?\nAnswer:"], "scorer": "Observation: I used to procrastinate about studying. Now, I never procrastinate studying.\nQuestion: Did After getting a good grade, I learned an easy lesson. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used to procrastinate about studying. Now, I never procrastinate studying.", "hypotheses": ["I failed a big test.", "After getting a good grade, I learned an easy lesson."]}
{"example_id": "31", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jacob decided to buy himself a car. happen?", "a": "no", "prior_probs": [0.499290943621067, 0.500709056378933], "posterior_probs": [0.539850527857254, 0.46014947214274604], "prior_entropy": 0.6931461750357113, "posterior_entropy": 0.6899676802449393, "delta_entropy": 0.003178494790772035, "eig_estimate": 0.0031862491964111507, "pred": 0, "gold": 1, "confidence": 0.539850527857254, "accuracy": 0, "tokens_in": 139, "tokens_out": 335, "tokens_total": 474, "latency_total": 0.20544900500044605, "latency_per_module": {"eig": [0.10014501500063488, 0.09900095800003328], "scorer": 0.006303031999777886}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 96, "tokens_out": 15, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jacob decided to buy himself a car. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jacob couldn't afford a car. happen?\nAnswer:"], "scorer": "Observation: Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.\nQuestion: Did Jacob decided to buy himself a car. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jacob hated walking to school. Jacob was able to buy himself a used truck after 5 months of saving.", "hypotheses": ["Jacob decided to buy himself a car.", "Jacob couldn't afford a car."]}
{"example_id": "32", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He yelled at the players for every home run. happen?", "a": "no", "prior_probs": [0.5412382362692145, 0.4587617637307855], "posterior_probs": [0.6658260573473478, 0.3341739426526522], "prior_entropy": 0.6897421297482087, "posterior_entropy": 0.6370952450353706, "delta_entropy": 0.05264688471283807, "eig_estimate": 0.05228383720703938, "pred": 0, "gold": 1, "confidence": 0.6658260573473478, "accuracy": 0, "tokens_in": 145, "tokens_out": 334, "tokens_total": 479, "latency_total": 0.2056406630017591, "latency_per_module": {"eig": [0.10033958300027734, 0.09924204400067538], "scorer": 0.006059036000806373}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 158, "tokens_total": 182}, {"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}], "scorer": {"tokens_in": 100, "tokens_out": 17, "tokens_total": 117}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He yelled at the players for every home run. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Roy made the other team uncomfortable. happen?\nAnswer:"], "scorer": "Observation: Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.\nQuestion: Did He yelled at the players for every home run. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roy went to the park to watch Little League baseball. The visiting team developed performance issues as a result.", "hypotheses": ["He yelled at the players for every home run.", "Roy made the other team uncomfortable."]}
{"example_id": "33", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Stan was out of school for a week with the stomach ache. happen?", "a": "no", "prior_probs": [0.5939922725940271, 0.40600772740597296], "posterior_probs": [0.6555961817365425, 0.3444038182634574], "prior_entropy": 0.6753725208367414, "posterior_entropy": 0.6439133824685718, "delta_entropy": 0.031459138368169604, "eig_estimate": 0.03180169053258146, "pred": 0, "gold": 1, "confidence": 0.6555961817365425, "accuracy": 0, "tokens_in": 165, "tokens_out": 343, "tokens_total": 508, "latency_total": 0.2118341679988589, "latency_per_module": {"eig": [0.10239277799973934, 0.10264252199976909], "scorer": 0.0067988679993504775}, "tokens_per_module": {"eig": [{"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 114, "tokens_out": 23, "tokens_total": 137}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Stan was out of school for a week with the stomach ache. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The school nurse sent Stan home from school. happen?\nAnswer:"], "scorer": "Observation: Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.\nQuestion: Did Stan was out of school for a week with the stomach ache. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Stan started to feel sick at school one day. Stan finally recovered but said he wanted a flu shot from now on.", "hypotheses": ["Stan was out of school for a week with the stomach ache.", "The school nurse sent Stan home from school."]}
{"example_id": "34", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Lisa and Tim went to a fertility clinic to get pregnant. happen?", "a": "no", "prior_probs": [0.6792737606353848, 0.32072623936461514], "posterior_probs": [0.7102502042842097, 0.28974979571579024], "prior_entropy": 0.6274156648989593, "posterior_entropy": 0.6019275016193724, "delta_entropy": 0.025488163279586917, "eig_estimate": 0.025706350643871948, "pred": 0, "gold": 1, "confidence": 0.7102502042842097, "accuracy": 0, "tokens_in": 144, "tokens_out": 345, "tokens_total": 489, "latency_total": 0.20875531699948624, "latency_per_module": {"eig": [0.10249781899983645, 0.09993179600041913], "scorer": 0.00632570199923066}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 28, "tokens_out": 159, "tokens_total": 187}], "scorer": {"tokens_in": 90, "tokens_out": 26, "tokens_total": 116}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Lisa and Tim went to a fertility clinic to get pregnant. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did They decided to try the advice given in a book about guitar playing. happen?\nAnswer:"], "scorer": "Observation: Lisa and Tim had been married for a long time. It worked.\nQuestion: Did Lisa and Tim went to a fertility clinic to get pregnant. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa and Tim had been married for a long time. It worked.", "hypotheses": ["Lisa and Tim went to a fertility clinic to get pregnant.", "They decided to try the advice given in a book about guitar playing."]}
{"example_id": "35", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?", "a": "no", "prior_probs": [0.6086536919959997, 0.3913463080040002], "posterior_probs": [0.7024292513921555, 0.2975707486078445], "prior_entropy": 0.6693464989756049, "posterior_entropy": 0.6087919301335103, "delta_entropy": 0.06055456884209465, "eig_estimate": 0.06075937948426617, "pred": 0, "gold": 1, "confidence": 0.7024292513921555, "accuracy": 0, "tokens_in": 173, "tokens_out": 347, "tokens_total": 520, "latency_total": 0.20824257899948861, "latency_per_module": {"eig": [0.10108154299996386, 0.09987106599965045], "scorer": 0.007289969999874302}, "tokens_per_module": {"eig": [{"tokens_in": 31, "tokens_out": 160, "tokens_total": 191}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 118, "tokens_out": 27, "tokens_total": 145}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Adam made himself a pb&j sandwich. happen?\nAnswer:"], "scorer": "Observation: Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.\nQuestion: Did Adam made himself a sandwich using bread, turkey, and a slice of American cheese. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Adam loves to eat sandwiches when he drinks. Adam choked to death because he forgot to take off the cheese wrapper.", "hypotheses": ["Adam made himself a sandwich using bread, turkey, and a slice of American cheese.", "Adam made himself a pb&j sandwich."]}
{"example_id": "36", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tom got tired of painting after he finished. happen?", "a": "no", "prior_probs": [0.5647430126639348, 0.4352569873360652], "posterior_probs": [0.6830631411099708, 0.31693685889002915], "prior_entropy": 0.6847402799808633, "posterior_entropy": 0.6245389516875033, "delta_entropy": 0.06020132829336, "eig_estimate": 0.060928193777312026, "pred": 0, "gold": 1, "confidence": 0.6830631411099708, "accuracy": 0, "tokens_in": 128, "tokens_out": 338, "tokens_total": 466, "latency_total": 0.21069683699988673, "latency_per_module": {"eig": [0.10476313900016976, 0.09975530500014429], "scorer": 0.006178392999572679}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 82, "tokens_out": 18, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tom got tired of painting after he finished. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tom heard a game was on and left. happen?\nAnswer:"], "scorer": "Observation: Tom was painting his fence. Tom left his fence half painted.\nQuestion: Did Tom got tired of painting after he finished. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was painting his fence. Tom left his fence half painted.", "hypotheses": ["Tom got tired of painting after he finished.", "Tom heard a game was on and left."]}
{"example_id": "37", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She would be with her friends out there. happen?", "a": "no", "prior_probs": [0.6175754766923246, 0.3824245233076754], "posterior_probs": [0.6861092164085281, 0.31389078359147204], "prior_entropy": 0.6652385826527232, "posterior_entropy": 0.62217845014932, "delta_entropy": 0.043060132503403126, "eig_estimate": 0.04273960722026826, "pred": 0, "gold": 1, "confidence": 0.6861092164085281, "accuracy": 0, "tokens_in": 140, "tokens_out": 337, "tokens_total": 477, "latency_total": 0.2056112630007192, "latency_per_module": {"eig": [0.09992424300071434, 0.09880209699986153], "scorer": 0.006884923000143317}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She would be with her friends out there. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Amy wanted to live by the beadch. happen?\nAnswer:"], "scorer": "Observation: Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.\nQuestion: Did She would be with her friends out there. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy decided to move from Wisconsin to Florida. However the experience was no fun without her friends.", "hypotheses": ["She would be with her friends out there.", "Amy wanted to live by the beadch."]}
{"example_id": "38", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Roger tried but he wasn't as good as his idol. happen?", "a": "yes", "prior_probs": [0.40261986142381756, 0.5973801385761824], "posterior_probs": [0.31948490138913493, 0.6805150986108651], "prior_entropy": 0.6740596404089629, "posterior_entropy": 0.6264805809228802, "delta_entropy": 0.047579059486082764, "eig_estimate": 0.047325392193231244, "pred": 1, "gold": 1, "confidence": 0.6805150986108651, "accuracy": 1, "tokens_in": 166, "tokens_out": 344, "tokens_total": 510, "latency_total": 0.21037848600008147, "latency_per_module": {"eig": [0.10050867199970526, 0.10234453700013546], "scorer": 0.007525277000240749}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 114, "tokens_out": 24, "tokens_total": 138}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Roger overslept and lounged most the day. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Roger tried but he wasn't as good as his idol. happen?\nAnswer:"], "scorer": "Observation: Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.\nQuestion: Did Roger tried but he wasn't as good as his idol. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Roger wanted to play tennis as well as Roger Federer. No one ever told Roger all his balls never landed in the court.", "hypotheses": ["Roger overslept and lounged most the day.", "Roger tried but he wasn't as good as his idol."]}
{"example_id": "39", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Barry did not tell anyone that Julie farted. happen?", "a": "no", "prior_probs": [0.5156470688694731, 0.484352931130527], "posterior_probs": [0.6214058532731264, 0.37859414672687375], "prior_entropy": 0.6926574390754046, "posterior_entropy": 0.6633716971305815, "delta_entropy": 0.02928574194482303, "eig_estimate": 0.029125540454409002, "pred": 0, "gold": 1, "confidence": 0.6214058532731264, "accuracy": 0, "tokens_in": 146, "tokens_out": 342, "tokens_total": 488, "latency_total": 0.21173345199986215, "latency_per_module": {"eig": [0.10619564700027695, 0.09935548700013896], "scorer": 0.006182317999446241}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Barry did not tell anyone that Julie farted. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Barry laughed at Julie's unzipped pants. happen?\nAnswer:"], "scorer": "Observation: Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.\nQuestion: Did Barry did not tell anyone that Julie farted. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Julie had a coworker named Barry who loved to make trouble for others. Julie was incredibly embarrassed.", "hypotheses": ["Barry did not tell anyone that Julie farted.", "Barry laughed at Julie's unzipped pants."]}
{"example_id": "40", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Bob stopped in the middle of his hike to tie his shoes. happen?", "a": "yes", "prior_probs": [0.44667890952075856, 0.5533210904792414], "posterior_probs": [0.3929538976400134, 0.6070461023599866], "prior_entropy": 0.6874500759315252, "posterior_entropy": 0.670051079307378, "delta_entropy": 0.017398996624147234, "eig_estimate": 0.017059891585784203, "pred": 1, "gold": 1, "confidence": 0.6070461023599866, "accuracy": 1, "tokens_in": 156, "tokens_out": 348, "tokens_total": 504, "latency_total": 0.20571435799865867, "latency_per_module": {"eig": [0.10037468100017577, 0.09948049599915976], "scorer": 0.005859180999323144}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 100, "tokens_out": 28, "tokens_total": 128}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Bob stopped in the middle of the hike because he had no bug spray. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Bob stopped in the middle of his hike to tie his shoes. happen?\nAnswer:"], "scorer": "Observation: Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.\nQuestion: Did Bob stopped in the middle of his hike to tie his shoes. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob decided to hike in the jungles of Indonesia. Bob had also forgotten his shoes.", "hypotheses": ["Bob stopped in the middle of the hike because he had no bug spray.", "Bob stopped in the middle of his hike to tie his shoes."]}
{"example_id": "41", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Lucy decided to make the pizzas at home. happen?", "a": "no", "prior_probs": [0.5715470914982475, 0.42845290850175255], "posterior_probs": [0.6069365033988582, 0.3930634966011417], "prior_entropy": 0.6828739799192332, "posterior_entropy": 0.6700987200822498, "delta_entropy": 0.012775259836983355, "eig_estimate": 0.012617405308151475, "pred": 0, "gold": 1, "confidence": 0.6069365033988582, "accuracy": 0, "tokens_in": 154, "tokens_out": 335, "tokens_total": 489, "latency_total": 0.20661399600066943, "latency_per_module": {"eig": [0.10154542600048444, 0.09923574599997664], "scorer": 0.005832824000208348}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 20, "tokens_out": 157, "tokens_total": 177}], "scorer": {"tokens_in": 110, "tokens_out": 18, "tokens_total": 128}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Lucy decided to make the pizzas at home. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Lucy started ordering the pizza. happen?\nAnswer:"], "scorer": "Observation: Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.\nQuestion: Did Lucy decided to make the pizzas at home. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy got all her friends together for a pizza party. When the pizzas were done, they had a taste and were delicious.", "hypotheses": ["Lucy decided to make the pizzas at home.", "Lucy started ordering the pizza."]}
{"example_id": "42", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jim's hat blew away in the wind. happen?", "a": "yes", "prior_probs": [0.49518695439491683, 0.5048130456050831], "posterior_probs": [0.4130054198613961, 0.5869945801386038], "prior_entropy": 0.6931008490264108, "posterior_entropy": 0.6779337592873937, "delta_entropy": 0.015167089739017037, "eig_estimate": 0.015109204640076722, "pred": 1, "gold": 1, "confidence": 0.5869945801386038, "accuracy": 1, "tokens_in": 142, "tokens_out": 337, "tokens_total": 479, "latency_total": 0.20776105999993888, "latency_per_module": {"eig": [0.10295916900031443, 0.09852066699932038], "scorer": 0.006281224000304064}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jim found his new hat in a storm. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jim's hat blew away in the wind. happen?\nAnswer:"], "scorer": "Observation: It was a very windy day. Jim wished he hadn't gone out in his new hat.\nQuestion: Did Jim's hat blew away in the wind. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was a very windy day. Jim wished he hadn't gone out in his new hat.", "hypotheses": ["Jim found his new hat in a storm.", "Jim's hat blew away in the wind."]}
{"example_id": "43", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The water was perfect for all levels of fishing. happen?", "a": "no", "prior_probs": [0.5889699375363152, 0.4110300624636848], "posterior_probs": [0.664761432879087, 0.335238567120913], "prior_entropy": 0.6772312612503355, "posterior_entropy": 0.6378266161575247, "delta_entropy": 0.03940464509281083, "eig_estimate": 0.0400399064587339, "pred": 0, "gold": 1, "confidence": 0.664761432879087, "accuracy": 0, "tokens_in": 149, "tokens_out": 336, "tokens_total": 485, "latency_total": 0.2053948210004819, "latency_per_module": {"eig": [0.10045429399997374, 0.09890605700002197], "scorer": 0.006034470000486181}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}], "scorer": {"tokens_in": 104, "tokens_out": 17, "tokens_total": 121}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The water was perfect for all levels of fishing. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The water was spitting up poles. happen?\nAnswer:"], "scorer": "Observation: Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.\nQuestion: Did The water was perfect for all levels of fishing. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Trevor went to the lake one day to fish. Trevor was forced to go home after he lost his fishing pole.", "hypotheses": ["The water was perfect for all levels of fishing.", "The water was spitting up poles."]}
{"example_id": "44", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?", "a": "no", "prior_probs": [0.5439580116016066, 0.4560419883983934], "posterior_probs": [0.6560598541048825, 0.34394014589511757], "prior_entropy": 0.6892775731215821, "posterior_entropy": 0.6436144263886251, "delta_entropy": 0.045663146732957016, "eig_estimate": 0.04583400914405995, "pred": 0, "gold": 1, "confidence": 0.6560598541048825, "accuracy": 0, "tokens_in": 186, "tokens_out": 349, "tokens_total": 535, "latency_total": 0.21011248500053625, "latency_per_module": {"eig": [0.101295339999524, 0.10179833200072608], "scorer": 0.007018813000286173}, "tokens_per_module": {"eig": [{"tokens_in": 33, "tokens_out": 159, "tokens_total": 192}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 128, "tokens_out": 30, "tokens_total": 158}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I looked down from her balcony to see the clouds. happen?\nAnswer:"], "scorer": "Observation: My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.\nQuestion: Did I went to visit her and stepped out onto the balcony of her apartment with a great view. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My sister lived at the top of a highrise apartment. I realized I was afraid of such massive heights and I fainted.", "hypotheses": ["I went to visit her and stepped out onto the balcony of her apartment with a great view.", "I looked down from her balcony to see the clouds."]}
{"example_id": "45", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Allister was still a novice at the bow. happen?", "a": "no", "prior_probs": [0.5420201553935746, 0.45797984460642543], "posterior_probs": [0.5608728440586146, 0.43912715594138535], "prior_entropy": 0.689611624953167, "posterior_entropy": 0.6857177571864664, "delta_entropy": 0.0038938677667005317, "eig_estimate": 0.004052026207084914, "pred": 0, "gold": 1, "confidence": 0.5608728440586146, "accuracy": 0, "tokens_in": 151, "tokens_out": 338, "tokens_total": 489, "latency_total": 0.2098455319992354, "latency_per_module": {"eig": [0.10448920799990447, 0.09952745699956722], "scorer": 0.005828866999763704}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Allister was still a novice at the bow. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Allister was a pro at the bow. happen?\nAnswer:"], "scorer": "Observation: Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.\nQuestion: Did Allister was still a novice at the bow. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Allister was practicing her with her bow. It ended up coming straight down and landing on the instructor's foot.", "hypotheses": ["Allister was still a novice at the bow.", "Allister was a pro at the bow."]}
{"example_id": "46", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Billy got home from work early. happen?", "a": "yes", "prior_probs": [0.36029998877311387, 0.6397000112268861], "posterior_probs": [0.35586779600101964, 0.6441322039989804], "prior_entropy": 0.6535906023014595, "posterior_entropy": 0.6510035547666551, "delta_entropy": 0.002587047534804432, "eig_estimate": 0.0019180063413269278, "pred": 1, "gold": 1, "confidence": 0.6441322039989804, "accuracy": 1, "tokens_in": 142, "tokens_out": 338, "tokens_total": 480, "latency_total": 0.2062017129992455, "latency_per_module": {"eig": [0.10187458399923344, 0.09802434399989579], "scorer": 0.006302785000116273}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Billy played games and forgot about cleaning until 5PM. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Billy got home from work early. happen?\nAnswer:"], "scorer": "Observation: Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.\nQuestion: Did Billy got home from work early. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Billy was going to have his boss over for dinner. He was still vacuuming when his boss arrived.", "hypotheses": ["Billy played games and forgot about cleaning until 5PM.", "Billy got home from work early."]}
{"example_id": "47", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She ended up falling into the river. happen?", "a": "no", "prior_probs": [0.5429731534759819, 0.4570268465240181], "posterior_probs": [0.5858130486295954, 0.4141869513704046], "prior_entropy": 0.6894492362019962, "posterior_entropy": 0.6783462540618717, "delta_entropy": 0.011102982140124507, "eig_estimate": 0.011365003824833585, "pred": 0, "gold": 1, "confidence": 0.5858130486295954, "accuracy": 0, "tokens_in": 150, "tokens_out": 338, "tokens_total": 488, "latency_total": 0.20348515499972564, "latency_per_module": {"eig": [0.09868634799931897, 0.09870953300014662], "scorer": 0.006089274000260048}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 104, "tokens_out": 19, "tokens_total": 123}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She ended up falling into the river. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Maya slipped on some rocks and broke her back. happen?\nAnswer:"], "scorer": "Observation: Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.\nQuestion: Did She ended up falling into the river. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Maya was walking alongside a river, looking for frogs. Luckily, she was able to get back up and walk home safely.", "hypotheses": ["She ended up falling into the river.", "Maya slipped on some rocks and broke her back."]}
{"example_id": "48", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She had to put in some chicken. happen?", "a": "yes", "prior_probs": [0.4976706673336924, 0.5023293326663075], "posterior_probs": [0.46362169740111325, 0.5363783025988867], "prior_entropy": 0.6931363289373521, "posterior_entropy": 0.6904980786835201, "delta_entropy": 0.0026382502538320196, "eig_estimate": 0.002574385399263601, "pred": 1, "gold": 1, "confidence": 0.5363783025988867, "accuracy": 1, "tokens_in": 132, "tokens_out": 336, "tokens_total": 468, "latency_total": 0.20306918500045867, "latency_per_module": {"eig": [0.0994740450005338, 0.09739982799965219], "scorer": 0.006195312000272679}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She had to put in some broth. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She had to put in some chicken. happen?\nAnswer:"], "scorer": "Observation: Susan was making a soup. She did her best to cut away the bad parts.\nQuestion: Did She had to put in some chicken. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Susan was making a soup. She did her best to cut away the bad parts.", "hypotheses": ["She had to put in some broth.", "She had to put in some chicken."]}
{"example_id": "49", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Lulu's daughter was going to go to school for the first time. happen?", "a": "no", "prior_probs": [0.5997206215358223, 0.40027937846417777], "posterior_probs": [0.629891948814377, 0.3701080511856229], "prior_entropy": 0.6731247826300334, "posterior_entropy": 0.6590131622967765, "delta_entropy": 0.014111620333256858, "eig_estimate": 0.014624322871517513, "pred": 0, "gold": 1, "confidence": 0.629891948814377, "accuracy": 0, "tokens_in": 170, "tokens_out": 352, "tokens_total": 522, "latency_total": 0.21086914199895546, "latency_per_module": {"eig": [0.10172332299953268, 0.1021481939997102], "scorer": 0.006997624999712571}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 31, "tokens_out": 160, "tokens_total": 191}], "scorer": {"tokens_in": 110, "tokens_out": 32, "tokens_total": 142}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Lulu's daughter was going to go to school for the first time. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Lulu's mom was thinking of sending her to a new house despite her objections. happen?\nAnswer:"], "scorer": "Observation: Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.\nQuestion: Did Lulu's daughter was going to go to school for the first time. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lulu felt so preoccupied lately. But she was prepared to send her because the school was excellent.", "hypotheses": ["Lulu's daughter was going to go to school for the first time.", "Lulu's mom was thinking of sending her to a new house despite her objections."]}
{"example_id": "50", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I programmed with Java for robot game because it was easy. happen?", "a": "yes", "prior_probs": [0.4704830929730231, 0.529516907026977], "posterior_probs": [0.4368249688091804, 0.5631750311908196], "prior_entropy": 0.6914036714465945, "posterior_entropy": 0.6851436362907115, "delta_entropy": 0.006260035155883026, "eig_estimate": 0.0064451858545810925, "pred": 1, "gold": 1, "confidence": 0.5631750311908196, "accuracy": 1, "tokens_in": 146, "tokens_out": 340, "tokens_total": 486, "latency_total": 0.21150934100023733, "latency_per_module": {"eig": [0.10422744900006364, 0.10124409000036394], "scorer": 0.006037801999809744}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I programmed with Java for robot game. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I programmed with Java for robot game because it was easy. happen?\nAnswer:"], "scorer": "Observation: I wanted to create a video game. Indeed, Java was terrible for programming video games.\nQuestion: Did I programmed with Java for robot game because it was easy. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I wanted to create a video game. Indeed, Java was terrible for programming video games.", "hypotheses": ["I programmed with Java for robot game.", "I programmed with Java for robot game because it was easy."]}
{"example_id": "51", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My best friend visited me on a vacation. happen?", "a": "no", "prior_probs": [0.5536741528365954, 0.4463258471634046], "posterior_probs": [0.6009604072669581, 0.3990395927330419], "prior_entropy": 0.6873742336400734, "posterior_entropy": 0.6726203332274289, "delta_entropy": 0.014753900412644572, "eig_estimate": 0.015121142002490396, "pred": 0, "gold": 1, "confidence": 0.6009604072669581, "accuracy": 0, "tokens_in": 152, "tokens_out": 335, "tokens_total": 487, "latency_total": 0.20407244999933027, "latency_per_module": {"eig": [0.10027027999967686, 0.09798834800039913], "scorer": 0.005813821999254287}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 108, "tokens_out": 16, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did My best friend visited me on a vacation. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I went with her to celebrate. happen?\nAnswer:"], "scorer": "Observation: My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.\nQuestion: Did My best friend visited me on a vacation. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My best friend got married and moved to Hawaii. We celebrated for days and I left for home at the end of the week.", "hypotheses": ["My best friend visited me on a vacation.", "I went with her to celebrate."]}
{"example_id": "52", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tom was an elitist. happen?", "a": "no", "prior_probs": [0.6080432574413293, 0.39195674255867086], "posterior_probs": [0.6445600116953581, 0.3554399883046418], "prior_entropy": 0.6696153193660608, "posterior_entropy": 0.6507493180587725, "delta_entropy": 0.018866001307288327, "eig_estimate": 0.020228837366724095, "pred": 0, "gold": 1, "confidence": 0.6445600116953581, "accuracy": 0, "tokens_in": 139, "tokens_out": 336, "tokens_total": 475, "latency_total": 0.20278471000074205, "latency_per_module": {"eig": [0.09833130500010157, 0.0981940890005717], "scorer": 0.006259316000068793}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 94, "tokens_out": 17, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tom was an elitist. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tom bought costly ones but they broke right away. happen?\nAnswer:"], "scorer": "Observation: Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.\nQuestion: Did Tom was an elitist. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom went to an electronics store to buy earphones. Tom decided to always buy more expensive ear phones.", "hypotheses": ["Tom was an elitist.", "Tom bought costly ones but they broke right away."]}
{"example_id": "53", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did At the store, Kaya saw a very beautiful vase. happen?", "a": "no", "prior_probs": [0.531998472862934, 0.46800152713706594], "posterior_probs": [0.6159191484167477, 0.38408085158325234], "prior_entropy": 0.691097975897329, "posterior_entropy": 0.6660266062473641, "delta_entropy": 0.025071369649964925, "eig_estimate": 0.02637652717969119, "pred": 0, "gold": 1, "confidence": 0.6159191484167477, "accuracy": 0, "tokens_in": 171, "tokens_out": 345, "tokens_total": 516, "latency_total": 0.21002656700056832, "latency_per_module": {"eig": [0.10116321599980438, 0.10159118600040529], "scorer": 0.007272165000358655}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 118, "tokens_out": 25, "tokens_total": 143}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did At the store, Kaya saw a very beautiful vase. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Kaya could not find a single thing at the store,. happen?\nAnswer:"], "scorer": "Observation: Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.\nQuestion: Did At the store, Kaya saw a very beautiful vase. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kaya needed to buy a new dress for the upcoming Spring Formal. She couldn't resist so she bought the antique vase.", "hypotheses": ["At the store, Kaya saw a very beautiful vase.", "Kaya could not find a single thing at the store,."]}
{"example_id": "54", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Sam put a towel under the leaky fridge. happen?", "a": "no", "prior_probs": [0.5717724395920868, 0.4282275604079132], "posterior_probs": [0.6390454428289989, 0.360954557171001], "prior_entropy": 0.6828089385588626, "posterior_entropy": 0.6539654361392004, "delta_entropy": 0.02884350241966216, "eig_estimate": 0.02884043649381971, "pred": 0, "gold": 1, "confidence": 0.6390454428289989, "accuracy": 0, "tokens_in": 152, "tokens_out": 338, "tokens_total": 490, "latency_total": 0.20949586900133, "latency_per_module": {"eig": [0.10389754200059542, 0.09991407000052277], "scorer": 0.005684257000211801}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Sam put a towel under the leaky fridge. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Sam dishwasher broke and was leaking. happen?\nAnswer:"], "scorer": "Observation: Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.\nQuestion: Did Sam put a towel under the leaky fridge. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam came home to find water all over his kitchen floor. Sam restocked his new fridge with all his favorite food.", "hypotheses": ["Sam put a towel under the leaky fridge.", "Sam dishwasher broke and was leaking."]}
{"example_id": "55", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Anna took a few asprins and laid down and took a nap. happen?", "a": "no", "prior_probs": [0.46141858598069874, 0.5385814140193013], "posterior_probs": [0.672859242674844, 0.32714075732515596], "prior_entropy": 0.6901671682063506, "posterior_entropy": 0.6321352477832707, "delta_entropy": 0.05803192042307992, "eig_estimate": 0.05932410222177099, "pred": 0, "gold": 1, "confidence": 0.672859242674844, "accuracy": 0, "tokens_in": 156, "tokens_out": 346, "tokens_total": 502, "latency_total": 0.21112667199849966, "latency_per_module": {"eig": [0.1020618509992346, 0.10277972599942586], "scorer": 0.0062850949998392025}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 102, "tokens_out": 26, "tokens_total": 128}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Anna took a few asprins and laid down and took a nap. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She went to a concert to get rid of it. happen?\nAnswer:"], "scorer": "Observation: Anna had a bad headache. Thankfully, when she awoke, the headache was gone.\nQuestion: Did Anna took a few asprins and laid down and took a nap. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Anna had a bad headache. Thankfully, when she awoke, the headache was gone.", "hypotheses": ["Anna took a few asprins and laid down and took a nap.", "She went to a concert to get rid of it."]}
{"example_id": "56", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did my dream was very real and I was on a fun bike tour. happen?", "a": "no", "prior_probs": [0.5567255632063237, 0.44327443679367623], "posterior_probs": [0.6731230080832663, 0.32687699191673364], "prior_entropy": 0.6866977243851222, "posterior_entropy": 0.6319448764571933, "delta_entropy": 0.054752847927928894, "eig_estimate": 0.053502986075783285, "pred": 0, "gold": 1, "confidence": 0.6731230080832663, "accuracy": 0, "tokens_in": 146, "tokens_out": 338, "tokens_total": 484, "latency_total": 0.2072811180005374, "latency_per_module": {"eig": [0.10143560699998488, 0.09981598400008806], "scorer": 0.006029527000464441}, "tokens_per_module": {"eig": [{"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}, {"tokens_in": 20, "tokens_out": 158, "tokens_total": 178}], "scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did my dream was very real and I was on a fun bike tour. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I actually have a bike. happen?\nAnswer:"], "scorer": "Observation: Last night I had a dream about biking. I woke up, bitterly disappointed.\nQuestion: Did my dream was very real and I was on a fun bike tour. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Last night I had a dream about biking. I woke up, bitterly disappointed.", "hypotheses": ["my dream was very real and I was on a fun bike tour.", "I actually have a bike."]}
{"example_id": "57", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She was supposed to be home an hour after she arrived. happen?", "a": "yes", "prior_probs": [0.4345182363492293, 0.5654817636507706], "posterior_probs": [0.370441827913764, 0.6295581720862359], "prior_entropy": 0.6845467737315923, "posterior_entropy": 0.6591904102643054, "delta_entropy": 0.02535636346728698, "eig_estimate": 0.025259695078640145, "pred": 1, "gold": 1, "confidence": 0.6295581720862359, "accuracy": 1, "tokens_in": 151, "tokens_out": 343, "tokens_total": 494, "latency_total": 0.20401963199947204, "latency_per_module": {"eig": [0.09793294700011757, 0.10029121399929863], "scorer": 0.005795471000055841}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 100, "tokens_out": 23, "tokens_total": 123}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She forgot what time it was and was home late. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She was supposed to be home an hour after she arrived. happen?\nAnswer:"], "scorer": "Observation: It was starting to get late outside. Her parents grounded her for a week for being late.\nQuestion: Did She was supposed to be home an hour after she arrived. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "It was starting to get late outside. Her parents grounded her for a week for being late.", "hypotheses": ["She forgot what time it was and was home late.", "She was supposed to be home an hour after she arrived."]}
{"example_id": "58", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Kim needed more money than she could get. happen?", "a": "no", "prior_probs": [0.574938188784644, 0.425061811215356], "posterior_probs": [0.5984736563405317, 0.40152634365946827], "prior_entropy": 0.6818732852598134, "posterior_entropy": 0.6736256945591717, "delta_entropy": 0.008247590700641738, "eig_estimate": 0.008591172098102695, "pred": 0, "gold": 1, "confidence": 0.5984736563405317, "accuracy": 0, "tokens_in": 143, "tokens_out": 336, "tokens_total": 479, "latency_total": 0.20648423200054822, "latency_per_module": {"eig": [0.10168465000060678, 0.09851727200020832], "scorer": 0.0062823099997331155}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 98, "tokens_out": 17, "tokens_total": 115}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Kim needed more money than she could get. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Kim applied for jobs to make money. happen?\nAnswer:"], "scorer": "Observation: Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.\nQuestion: Did Kim needed more money than she could get. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kim wanted to buy a car. Kim was hired to a job and saved enough money for her car.", "hypotheses": ["Kim needed more money than she could get.", "Kim applied for jobs to make money."]}
{"example_id": "59", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Joe didn't have time to cook something. happen?", "a": "yes", "prior_probs": [0.49453006156178386, 0.5054699384382161], "posterior_probs": [0.4724009112590442, 0.5275990887409558], "prior_entropy": 0.6930873389112259, "posterior_entropy": 0.6916229866127033, "delta_entropy": 0.0014643522985225976, "eig_estimate": 0.0014560301306165923, "pred": 1, "gold": 1, "confidence": 0.5275990887409558, "accuracy": 1, "tokens_in": 140, "tokens_out": 338, "tokens_total": 478, "latency_total": 0.20536083800107008, "latency_per_module": {"eig": [0.0985254810002516, 0.10055162200023915], "scorer": 0.006283735000579327}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Joe had plenty of time to cook something. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Joe didn't have time to cook something. happen?\nAnswer:"], "scorer": "Observation: Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!\nQuestion: Did Joe didn't have time to cook something. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe was running late for school. Joe had a tasty, healthy breakfast of apples and bananas!", "hypotheses": ["Joe had plenty of time to cook something.", "Joe didn't have time to cook something."]}
{"example_id": "60", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tim looked for a long time in the messy fridge. happen?", "a": "yes", "prior_probs": [0.4444638647440158, 0.5555361352559842], "posterior_probs": [0.30262409795309964, 0.6973759020469004], "prior_entropy": 0.6869659093462148, "posterior_entropy": 0.6130713268160947, "delta_entropy": 0.07389458253012016, "eig_estimate": 0.07389443130899187, "pred": 1, "gold": 1, "confidence": 0.6973759020469004, "accuracy": 1, "tokens_in": 142, "tokens_out": 340, "tokens_total": 482, "latency_total": 0.2055579759999091, "latency_per_module": {"eig": [0.10060549700028787, 0.09867773999940255], "scorer": 0.006274739000218688}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 94, "tokens_out": 20, "tokens_total": 114}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He went to the near by super market. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tim looked for a long time in the messy fridge. happen?\nAnswer:"], "scorer": "Observation: Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.\nQuestion: Did Tim looked for a long time in the messy fridge. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim needed a fruit to eat. Finally, he found some fresh grapes to eat.", "hypotheses": ["He went to the near by super market.", "Tim looked for a long time in the messy fridge."]}
{"example_id": "61", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The rap tape was mean and rude. happen?", "a": "no", "prior_probs": [0.6524055812220552, 0.3475944187779449], "posterior_probs": [0.7030050701302726, 0.2969949298697275], "prior_entropy": 0.6459447581254353, "posterior_entropy": 0.6082965702015342, "delta_entropy": 0.03764818792390112, "eig_estimate": 0.038780807886258484, "pred": 0, "gold": 1, "confidence": 0.7030050701302726, "accuracy": 0, "tokens_in": 139, "tokens_out": 336, "tokens_total": 475, "latency_total": 0.20407089899981656, "latency_per_module": {"eig": [0.09922879900022963, 0.09844966000036948], "scorer": 0.0063924399992174585}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 96, "tokens_out": 16, "tokens_total": 112}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The rap tape was mean and rude. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Gina  take their new tape. happen?\nAnswer:"], "scorer": "Observation: Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.\nQuestion: Did The rap tape was mean and rude. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Gina's mom had taken a rap tape from her kids. He stood smiling as his tape was smashed.", "hypotheses": ["The rap tape was mean and rude.", "Gina  take their new tape."]}
{"example_id": "62", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did We ordered a dessert everyone would like. happen?", "a": "no", "prior_probs": [0.5428062460011205, 0.45719375399887957], "posterior_probs": [0.5853486189041158, 0.4146513810958843], "prior_entropy": 0.6894779411916492, "posterior_entropy": 0.6785068196410964, "delta_entropy": 0.010971121550552798, "eig_estimate": 0.011209157158969509, "pred": 0, "gold": 1, "confidence": 0.5853486189041158, "accuracy": 0, "tokens_in": 134, "tokens_out": 336, "tokens_total": 470, "latency_total": 0.20665596600065328, "latency_per_module": {"eig": [0.09859063499970944, 0.10166014800051926], "scorer": 0.006405183000424586}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 90, "tokens_out": 16, "tokens_total": 106}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did We ordered a dessert everyone would like. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did We ordered appetizers everyone would like. happen?\nAnswer:"], "scorer": "Observation: Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.\nQuestion: Did We ordered a dessert everyone would like. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly loves to eat sweet desserts. We all ate the brownie sundae for dessert.", "hypotheses": ["We ordered a dessert everyone would like.", "We ordered appetizers everyone would like."]}
{"example_id": "63", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ora decided to eat healthy for a month. happen?", "a": "no", "prior_probs": [0.5766461630064038, 0.4233538369935963], "posterior_probs": [0.6339982773888124, 0.36600172261118746], "prior_entropy": 0.6813514588057896, "posterior_entropy": 0.6567933882651372, "delta_entropy": 0.0245580705406524, "eig_estimate": 0.02497782052081916, "pred": 0, "gold": 1, "confidence": 0.6339982773888124, "accuracy": 0, "tokens_in": 143, "tokens_out": 341, "tokens_total": 484, "latency_total": 0.20458880399928603, "latency_per_module": {"eig": [0.1000501409998833, 0.09824087699962547], "scorer": 0.006297785999777261}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 94, "tokens_out": 21, "tokens_total": 115}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ora decided to eat healthy for a month. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ora decided she wanted to maintainer her weight. happen?\nAnswer:"], "scorer": "Observation: Ora had always been overweight. With their help, Ora lost over twenty pounds!\nQuestion: Did Ora decided to eat healthy for a month. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora had always been overweight. With their help, Ora lost over twenty pounds!", "hypotheses": ["Ora decided to eat healthy for a month.", "Ora decided she wanted to maintainer her weight."]}
{"example_id": "64", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The food that Priya ordered was microwaved and precooked. happen?", "a": "yes", "prior_probs": [0.4374002057690711, 0.562599794230929], "posterior_probs": [0.30954852399197325, 0.6904514760080268], "prior_entropy": 0.6852891073100924, "posterior_entropy": 0.618738953166184, "delta_entropy": 0.06655015414390841, "eig_estimate": 0.06624909446889551, "pred": 1, "gold": 1, "confidence": 0.6904514760080268, "accuracy": 1, "tokens_in": 145, "tokens_out": 338, "tokens_total": 483, "latency_total": 0.20429548799893382, "latency_per_module": {"eig": [0.0995471890000772, 0.09878489999937301], "scorer": 0.005963398999483616}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 98, "tokens_out": 19, "tokens_total": 117}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She ordered two shrimp dishes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The food that Priya ordered was microwaved and precooked. happen?\nAnswer:"], "scorer": "Observation: Priya decided to try a new restaurant. Priya thought her food was delicious.\nQuestion: Did The food that Priya ordered was microwaved and precooked. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Priya decided to try a new restaurant. Priya thought her food was delicious.", "hypotheses": ["She ordered two shrimp dishes.", "The food that Priya ordered was microwaved and precooked."]}
{"example_id": "65", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jamie and Candice couldn't decide what to do. happen?", "a": "yes", "prior_probs": [0.43257818279028354, 0.5674218172097164], "posterior_probs": [0.40822417282041834, 0.5917758271795815], "prior_entropy": 0.6840280241266522, "posterior_entropy": 0.6762056859186427, "delta_entropy": 0.007822338208009483, "eig_estimate": 0.007648422452990533, "pred": 1, "gold": 1, "confidence": 0.5917758271795815, "accuracy": 1, "tokens_in": 147, "tokens_out": 342, "tokens_total": 489, "latency_total": 0.20820603999982268, "latency_per_module": {"eig": [0.10303030399973068, 0.0989188120001927], "scorer": 0.0062569239998993}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 25, "tokens_out": 159, "tokens_total": 184}], "scorer": {"tokens_in": 96, "tokens_out": 23, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jamie and Candice did not know what movie to see. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jamie and Candice couldn't decide what to do. happen?\nAnswer:"], "scorer": "Observation: Jamie and Candice were going on a date. Finally, they settled on ice cream!\nQuestion: Did Jamie and Candice couldn't decide what to do. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jamie and Candice were going on a date. Finally, they settled on ice cream!", "hypotheses": ["Jamie and Candice did not know what movie to see.", "Jamie and Candice couldn't decide what to do."]}
{"example_id": "66", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living. happen?", "a": "yes", "prior_probs": [0.6046598668084326, 0.39534013319156736], "posterior_probs": [0.3172329950977277, 0.6827670049022723], "prior_entropy": 0.6710769560047052, "posterior_entropy": 0.6247661476495492, "delta_entropy": 0.046310808355156, "eig_estimate": 0.04639715273606515, "pred": 1, "gold": 1, "confidence": 0.6827670049022723, "accuracy": 1, "tokens_in": 187, "tokens_out": 346, "tokens_total": 533, "latency_total": 0.20768420700005663, "latency_per_module": {"eig": [0.09960310799942818, 0.10044924400062882], "scorer": 0.007631854999999632}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 33, "tokens_out": 159, "tokens_total": 192}], "scorer": {"tokens_in": 132, "tokens_out": 27, "tokens_total": 159}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Scott found a job in New York. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living. happen?\nAnswer:"], "scorer": "Observation: Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.\nQuestion: Did The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Scott has felt increasingly unhappy in his last few Year's in New York. Driving out of New York, Scott feels both relieved and nostalgic.", "hypotheses": ["Scott found a job in New York.", "The daily grind, extreme traffic and rude city dwellers left Scott longing for small town living."]}
{"example_id": "67", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I bought jeans thought they were a bit expensive. happen?", "a": "yes", "prior_probs": [0.42338107670920266, 0.5766189232907972], "posterior_probs": [0.33595767133238874, 0.6640423286676113], "prior_entropy": 0.6813598749171441, "posterior_entropy": 0.6383177449042357, "delta_entropy": 0.043042130012908464, "eig_estimate": 0.04337607421587823, "pred": 1, "gold": 1, "confidence": 0.6640423286676113, "accuracy": 1, "tokens_in": 143, "tokens_out": 335, "tokens_total": 478, "latency_total": 0.20359635099976003, "latency_per_module": {"eig": [0.10006407099990611, 0.09756159399967146], "scorer": 0.005970686000182468}, "tokens_per_module": {"eig": [{"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 100, "tokens_out": 15, "tokens_total": 115}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I got larger sizes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I bought jeans thought they were a bit expensive. happen?\nAnswer:"], "scorer": "Observation: I went to the store one day to buy clothes. I went home and the jeans fit much better.\nQuestion: Did I bought jeans thought they were a bit expensive. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I went to the store one day to buy clothes. I went home and the jeans fit much better.", "hypotheses": ["I got larger sizes.", "I bought jeans thought they were a bit expensive."]}
{"example_id": "68", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did They ran into a cute friend of Sam's on the way to dinner. happen?", "a": "no", "prior_probs": [0.5093751289457198, 0.49062487105428026], "posterior_probs": [0.6646352315268922, 0.3353647684731078], "prior_entropy": 0.6929713841707495, "posterior_entropy": 0.6379129760809052, "delta_entropy": 0.0550584080898443, "eig_estimate": 0.0558579741095601, "pred": 0, "gold": 1, "confidence": 0.6646352315268922, "accuracy": 0, "tokens_in": 173, "tokens_out": 346, "tokens_total": 519, "latency_total": 0.21962681700006215, "latency_per_module": {"eig": [0.10368208099953335, 0.10847765800008347], "scorer": 0.007467078000445326}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}], "scorer": {"tokens_in": 118, "tokens_out": 27, "tokens_total": 145}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did They ran into a cute friend of Sam's on the way to dinner. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The date went bad, they went home on good terms. happen?\nAnswer:"], "scorer": "Observation: Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.\nQuestion: Did They ran into a cute friend of Sam's on the way to dinner. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was on a date with her boyfriend. When her boyfriend asked if she wanted to hang out, she yelled at him.", "hypotheses": ["They ran into a cute friend of Sam's on the way to dinner.", "The date went bad, they went home on good terms."]}
{"example_id": "69", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Pam tried her best to be as honest as possible. happen?", "a": "no", "prior_probs": [0.5997088313230708, 0.4002911686769291], "posterior_probs": [0.6315549156136061, 0.368445084386394], "prior_entropy": 0.6731295491372684, "posterior_entropy": 0.6581229393318856, "delta_entropy": 0.01500660980538282, "eig_estimate": 0.015025175292159942, "pred": 0, "gold": 1, "confidence": 0.6315549156136061, "accuracy": 0, "tokens_in": 159, "tokens_out": 341, "tokens_total": 500, "latency_total": 0.20854209099979926, "latency_per_module": {"eig": [0.10363836099986656, 0.09872444000029645], "scorer": 0.006179289999636239}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 112, "tokens_out": 21, "tokens_total": 133}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Pam tried her best to be as honest as possible. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Pam wasted time with doing her surveys. happen?\nAnswer:"], "scorer": "Observation: Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.\nQuestion: Did Pam tried her best to be as honest as possible. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Pam does surveys daily to try to win prizes on Survey Monkey. Pam did not seem to think there was anything wrong with that.", "hypotheses": ["Pam tried her best to be as honest as possible.", "Pam wasted time with doing her surveys."]}
{"example_id": "70", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did In Lydia's dream, she was rich and famous. happen?", "a": "yes", "prior_probs": [0.4946844792302691, 0.505315520769731], "posterior_probs": [0.450184402868635, 0.5498155971313651], "prior_entropy": 0.6930906699713483, "posterior_entropy": 0.6881757492693933, "delta_entropy": 0.004914920701954983, "eig_estimate": 0.004686199035337357, "pred": 1, "gold": 1, "confidence": 0.5498155971313651, "accuracy": 1, "tokens_in": 140, "tokens_out": 341, "tokens_total": 481, "latency_total": 0.20751834000020608, "latency_per_module": {"eig": [0.10205701399991085, 0.09919799599992984], "scorer": 0.006263330000365386}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 159, "tokens_total": 184}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 90, "tokens_out": 22, "tokens_total": 112}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did In Lydia's dream, she was poor and lonely. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did In Lydia's dream, she was rich and famous. happen?\nAnswer:"], "scorer": "Observation: Lydia had a strange dream last night. Lydia wished the dream were real.\nQuestion: Did In Lydia's dream, she was rich and famous. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lydia had a strange dream last night. Lydia wished the dream were real.", "hypotheses": ["In Lydia's dream, she was poor and lonely.", "In Lydia's dream, she was rich and famous."]}
{"example_id": "71", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Her friend asked Dana to teach her how to ride. happen?", "a": "yes", "prior_probs": [0.486309255929223, 0.5136907440707771], "posterior_probs": [0.3776516131269616, 0.6223483868730385], "prior_entropy": 0.6927722607542264, "posterior_entropy": 0.6629027645544058, "delta_entropy": 0.0298694961998206, "eig_estimate": 0.029491309329808656, "pred": 1, "gold": 1, "confidence": 0.6223483868730385, "accuracy": 1, "tokens_in": 140, "tokens_out": 341, "tokens_total": 481, "latency_total": 0.2117016779993719, "latency_per_module": {"eig": [0.10835035099989909, 0.09703548000015871], "scorer": 0.00631584699931409}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}, {"tokens_in": 25, "tokens_out": 161, "tokens_total": 186}], "scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Dana asked a neighbor to ride with her. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Her friend asked Dana to teach her how to ride. happen?\nAnswer:"], "scorer": "Observation: Dana always wanted to ride a bike. They were riding around together within minutes.\nQuestion: Did Her friend asked Dana to teach her how to ride. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dana always wanted to ride a bike. They were riding around together within minutes.", "hypotheses": ["Dana asked a neighbor to ride with her.", "Her friend asked Dana to teach her how to ride."]}
{"example_id": "72", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Kelly tried out for the soccer team but was cut. happen?", "a": "no", "prior_probs": [0.5442938108983753, 0.4557061891016247], "posterior_probs": [0.624728277636263, 0.37527172236373696], "prior_entropy": 0.6892181487243876, "posterior_entropy": 0.6617018834066446, "delta_entropy": 0.027516265317742983, "eig_estimate": 0.027840368779414204, "pred": 0, "gold": 1, "confidence": 0.624728277636263, "accuracy": 0, "tokens_in": 138, "tokens_out": 338, "tokens_total": 476, "latency_total": 0.21399877200110495, "latency_per_module": {"eig": [0.10826051100048062, 0.0994721600000048], "scorer": 0.006266101000619528}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Kelly tried out for the soccer team but was cut. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Kelly made it onto the team. happen?\nAnswer:"], "scorer": "Observation: Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.\nQuestion: Did Kelly tried out for the soccer team but was cut. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly wanted to try out for soccer this year. Kelly celebrated by getting pizza.", "hypotheses": ["Kelly tried out for the soccer team but was cut.", "Kelly made it onto the team."]}
{"example_id": "73", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tom didn't check the grill's safety. happen?", "a": "yes", "prior_probs": [0.4592465255863792, 0.5407534744136208], "posterior_probs": [0.4000717522970673, 0.5999282477029327], "prior_entropy": 0.6898218015182185, "posterior_entropy": 0.6730407493345406, "delta_entropy": 0.01678105218367787, "eig_estimate": 0.016622832354974748, "pred": 1, "gold": 1, "confidence": 0.5999282477029327, "accuracy": 1, "tokens_in": 140, "tokens_out": 337, "tokens_total": 477, "latency_total": 0.20582519300023705, "latency_per_module": {"eig": [0.1002884540002924, 0.0991472519999661], "scorer": 0.006389486999978544}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 94, "tokens_out": 18, "tokens_total": 112}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tom didn't check the grills gas. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tom didn't check the grill's safety. happen?\nAnswer:"], "scorer": "Observation: Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.\nQuestion: Did Tom didn't check the grill's safety. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was excited to use his new gas grill. Tom's garage and grill were both destroyed.", "hypotheses": ["Tom didn't check the grills gas.", "Tom didn't check the grill's safety."]}
{"example_id": "74", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He was the best fisherman of the group. happen?", "a": "yes", "prior_probs": [0.33492383062893677, 0.6650761693710633], "posterior_probs": [0.30369741687667073, 0.6963025831233293], "prior_entropy": 0.6376109297354657, "posterior_entropy": 0.6139646415141717, "delta_entropy": 0.023646288221294, "eig_estimate": 0.023391958256338642, "pred": 1, "gold": 1, "confidence": 0.6963025831233293, "accuracy": 1, "tokens_in": 138, "tokens_out": 337, "tokens_total": 475, "latency_total": 0.20935265200023423, "latency_per_module": {"eig": [0.10350589399968158, 0.09950870200009376], "scorer": 0.0063380560004588915}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 92, "tokens_out": 18, "tokens_total": 110}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Martins friends went fishing without martin. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He was the best fisherman of the group. happen?\nAnswer:"], "scorer": "Observation: Martin went to camp with his friends. Martin caught many fish so that everyone could eat.\nQuestion: Did He was the best fisherman of the group. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martin went to camp with his friends. Martin caught many fish so that everyone could eat.", "hypotheses": ["Martins friends went fishing without martin.", "He was the best fisherman of the group."]}
{"example_id": "75", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He rolled up all his windows and began daydreaming of how well his car would look after the wash. happen?", "a": "yes", "prior_probs": [0.44405487857142134, 0.5559451214285787], "posterior_probs": [0.27827771844126614, 0.7217222815587339], "prior_entropy": 0.6868743401477769, "posterior_entropy": 0.5913193226000542, "delta_entropy": 0.09555501754772278, "eig_estimate": 0.09572224079042534, "pred": 1, "gold": 1, "confidence": 0.7217222815587339, "accuracy": 1, "tokens_in": 188, "tokens_out": 355, "tokens_total": 543, "latency_total": 0.2130487840013302, "latency_per_module": {"eig": [0.10505990700039547, 0.10096420100035175], "scorer": 0.007024676000582986}, "tokens_per_module": {"eig": [{"tokens_in": 28, "tokens_out": 159, "tokens_total": 187}, {"tokens_in": 36, "tokens_out": 160, "tokens_total": 196}], "scorer": {"tokens_in": 124, "tokens_out": 36, "tokens_total": 160}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did After the car wash Sam noticed his car seats were all soaking wet. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He rolled up all his windows and began daydreaming of how well his car would look after the wash. happen?\nAnswer:"], "scorer": "Observation: Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.\nQuestion: Did He rolled up all his windows and began daydreaming of how well his car would look after the wash. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam was excited to get his car washed. He had forgotten to close the window in all the excitement.", "hypotheses": ["After the car wash Sam noticed his car seats were all soaking wet.", "He rolled up all his windows and began daydreaming of how well his car would look after the wash."]}
{"example_id": "76", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did May sent out tweets looking for tickets. happen?", "a": "no", "prior_probs": [0.4737475754119093, 0.5262524245880907], "posterior_probs": [0.6188438713921466, 0.3811561286078534], "prior_entropy": 0.6917681669549507, "posterior_entropy": 0.6646272717223558, "delta_entropy": 0.027140895232594886, "eig_estimate": 0.027139859549505818, "pred": 0, "gold": 1, "confidence": 0.6188438713921466, "accuracy": 0, "tokens_in": 155, "tokens_out": 335, "tokens_total": 490, "latency_total": 0.20551733700085606, "latency_per_module": {"eig": [0.10172776200033695, 0.09778324800026894], "scorer": 0.006006327000250167}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 112, "tokens_out": 15, "tokens_total": 127}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did May sent out tweets looking for tickets. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did So she bought 2 tickets online. happen?\nAnswer:"], "scorer": "Observation: May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!\nQuestion: Did May sent out tweets looking for tickets. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "May really wanted to go to the concert that was playing next week. A kind reader wrote back to offer her two free tickets to the concert!", "hypotheses": ["May sent out tweets looking for tickets.", "So she bought 2 tickets online."]}
{"example_id": "77", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did When Mindy walked in the door, Jake was naked. happen?", "a": "no", "prior_probs": [0.486044932696252, 0.5139550673037481], "posterior_probs": [0.5926495053176007, 0.4073504946823992], "prior_entropy": 0.692757642168361, "posterior_entropy": 0.6758796990385001, "delta_entropy": 0.016877943129860906, "eig_estimate": 0.01702077089052869, "pred": 0, "gold": 1, "confidence": 0.5926495053176007, "accuracy": 0, "tokens_in": 155, "tokens_out": 342, "tokens_total": 497, "latency_total": 0.20457148000059533, "latency_per_module": {"eig": [0.10059544500018092, 0.09828630999982124], "scorer": 0.005689725000593171}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 25, "tokens_out": 159, "tokens_total": 184}], "scorer": {"tokens_in": 104, "tokens_out": 23, "tokens_total": 127}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did When Mindy walked in the door, Jake was naked. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Mindy scared Jake when she came into the house. happen?\nAnswer:"], "scorer": "Observation: Mindy decided to go over jake's house. She panicked and ran screaming out of his house.\nQuestion: Did When Mindy walked in the door, Jake was naked. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mindy decided to go over jake's house. She panicked and ran screaming out of his house.", "hypotheses": ["When Mindy walked in the door, Jake was naked.", "Mindy scared Jake when she came into the house."]}
{"example_id": "78", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My sister stole my slippers. happen?", "a": "yes", "prior_probs": [0.4790031487503871, 0.5209968512496128], "posterior_probs": [0.4218260211495296, 0.5781739788504705], "prior_entropy": 0.6922651856976607, "posterior_entropy": 0.6808745500392104, "delta_entropy": 0.011390635658450332, "eig_estimate": 0.011295740167959289, "pred": 1, "gold": 1, "confidence": 0.5781739788504705, "accuracy": 1, "tokens_in": 132, "tokens_out": 334, "tokens_total": 466, "latency_total": 0.2042384120004499, "latency_per_module": {"eig": [0.10046495999995386, 0.09754447100021935], "scorer": 0.00622898100027669}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 90, "tokens_out": 14, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The dog stole the slippers. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did My sister stole my slippers. happen?\nAnswer:"], "scorer": "Observation: I was having trouble finding my comfortable slippers. Finders keepers, she told me.\nQuestion: Did My sister stole my slippers. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was having trouble finding my comfortable slippers. Finders keepers, she told me.", "hypotheses": ["The dog stole the slippers.", "My sister stole my slippers."]}
{"example_id": "79", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I wanted to have normal lips. I painted them red and people liked it. happen?", "a": "yes", "prior_probs": [0.44394140244086727, 0.5560585975591328], "posterior_probs": [0.3419649756170469, 0.6580350243829531], "prior_entropy": 0.6868488135486308, "posterior_entropy": 0.6423302396141802, "delta_entropy": 0.04451857393445058, "eig_estimate": 0.04438976540739934, "pred": 1, "gold": 1, "confidence": 0.6580350243829531, "accuracy": 1, "tokens_in": 176, "tokens_out": 346, "tokens_total": 522, "latency_total": 0.21311070299998391, "latency_per_module": {"eig": [0.1030409499999223, 0.10322475900011341], "scorer": 0.006844993999948201}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 30, "tokens_out": 160, "tokens_total": 190}], "scorer": {"tokens_in": 122, "tokens_out": 26, "tokens_total": 148}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I used a marker to paint them bright pink. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I wanted to have normal lips. I painted them red and people liked it. happen?\nAnswer:"], "scorer": "Observation: I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.\nQuestion: Did I wanted to have normal lips. I painted them red and people liked it. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I did not like the color of my lips. Over time, people at school accepted me for the person I wished to be.", "hypotheses": ["I used a marker to paint them bright pink.", "I wanted to have normal lips. I painted them red and people liked it."]}
{"example_id": "80", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The boy needed a new cell phone. happen?", "a": "yes", "prior_probs": [0.4492307602125571, 0.5507692397874429], "posterior_probs": [0.3883124042080154, 0.6116875957919847], "prior_entropy": 0.6879832542902835, "posterior_entropy": 0.6679872158394434, "delta_entropy": 0.01999603845084008, "eig_estimate": 0.019564959832235274, "pred": 1, "gold": 1, "confidence": 0.6116875957919847, "accuracy": 1, "tokens_in": 127, "tokens_out": 334, "tokens_total": 461, "latency_total": 0.20899465500042425, "latency_per_module": {"eig": [0.1030780520004555, 0.09974504399997386], "scorer": 0.006171558999994886}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 84, "tokens_out": 15, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Their son was very well behaved. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The boy needed a new cell phone. happen?\nAnswer:"], "scorer": "Observation: A family went shopping together. The father bought the boy a new computer.\nQuestion: Did The boy needed a new cell phone. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A family went shopping together. The father bought the boy a new computer.", "hypotheses": ["Their son was very well behaved.", "The boy needed a new cell phone."]}
{"example_id": "81", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?", "a": "no", "prior_probs": [0.47648300690444706, 0.5235169930955529], "posterior_probs": [0.6916962383653161, 0.3083037616346839], "prior_entropy": 0.6920406744505931, "posterior_entropy": 0.6177367389586781, "delta_entropy": 0.07430393549191505, "eig_estimate": 0.07396897834505782, "pred": 0, "gold": 1, "confidence": 0.6916962383653161, "accuracy": 0, "tokens_in": 170, "tokens_out": 348, "tokens_total": 518, "latency_total": 0.20868975699977454, "latency_per_module": {"eig": [0.10000861600019562, 0.10147056299956603], "scorer": 0.007210578000012902}, "tokens_per_module": {"eig": [{"tokens_in": 32, "tokens_out": 160, "tokens_total": 192}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 114, "tokens_out": 28, "tokens_total": 142}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The internet was very slow and then stopped completely. happen?\nAnswer:"], "scorer": "Observation: I was working on my laptop one day. After paying the bill, I no longer experienced issues.\nQuestion: Did i got an e-mail saying my cable bill was current and service would be upgraded. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was working on my laptop one day. After paying the bill, I no longer experienced issues.", "hypotheses": ["i got an e-mail saying my cable bill was current and service would be upgraded.", "The internet was very slow and then stopped completely."]}
{"example_id": "82", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Isa went to the country site and found some flowers. happen?", "a": "no", "prior_probs": [0.4668909333315746, 0.5331090666684254], "posterior_probs": [0.607676300037499, 0.39232369996250105], "prior_entropy": 0.6909531549137811, "posterior_entropy": 0.6697761658843993, "delta_entropy": 0.02117698902938181, "eig_estimate": 0.02164787182509547, "pred": 0, "gold": 1, "confidence": 0.607676300037499, "accuracy": 0, "tokens_in": 152, "tokens_out": 338, "tokens_total": 490, "latency_total": 0.2097493420005776, "latency_per_module": {"eig": [0.1033830800006399, 0.1006912229995578], "scorer": 0.005675039000379911}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Isa went to the country site and found some flowers. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Isa forgot about the bouquet. happen?\nAnswer:"], "scorer": "Observation: Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!\nQuestion: Did Isa went to the country site and found some flowers. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Isa was getting married but couldn't afford a bouquet. Isa ended up with a beautiful hand picked bouquet!", "hypotheses": ["Isa went to the country site and found some flowers.", "Isa forgot about the bouquet."]}
{"example_id": "83", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Everyone was all over the hotel, trying to see him. happen?", "a": "yes", "prior_probs": [0.4949569028203426, 0.5050430971796575], "posterior_probs": [0.34867117851201407, 0.6513288214879859], "prior_entropy": 0.6930963140371449, "posterior_entropy": 0.6466201633512116, "delta_entropy": 0.04647615068593325, "eig_estimate": 0.04649024154297514, "pred": 1, "gold": 1, "confidence": 0.6513288214879859, "accuracy": 1, "tokens_in": 170, "tokens_out": 342, "tokens_total": 512, "latency_total": 0.22057415200015384, "latency_per_module": {"eig": [0.11249541200049862, 0.101312427999801], "scorer": 0.006766311999854224}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 120, "tokens_out": 22, "tokens_total": 142}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did People destroyed the dam the beaver was building. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Everyone was all over the hotel, trying to see him. happen?\nAnswer:"], "scorer": "Observation: There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.\nQuestion: Did Everyone was all over the hotel, trying to see him. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There is a beaver that has been building a dam by our school. The beaver decided to leave the area and went further down stream.", "hypotheses": ["People destroyed the dam the beaver was building.", "Everyone was all over the hotel, trying to see him."]}
{"example_id": "84", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did We took a photo next to the coliseum in Rome. happen?", "a": "yes", "prior_probs": [0.48274284883258106, 0.5172571511674189], "posterior_probs": [0.3594977012231183, 0.6405022987768816], "prior_entropy": 0.6925514437149196, "posterior_entropy": 0.653128642439055, "delta_entropy": 0.039422801275864616, "eig_estimate": 0.039417760700676305, "pred": 1, "gold": 1, "confidence": 0.6405022987768816, "accuracy": 1, "tokens_in": 144, "tokens_out": 339, "tokens_total": 483, "latency_total": 0.21308266399955755, "latency_per_module": {"eig": [0.10699180400024488, 0.10006644699933531], "scorer": 0.006024412999977358}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}], "scorer": {"tokens_in": 96, "tokens_out": 20, "tokens_total": 116}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did We took a beautiful picture of Spain. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did We took a photo next to the coliseum in Rome. happen?\nAnswer:"], "scorer": "Observation: My family was on vacation in Italy. It was our favorite picture of the vacation.\nQuestion: Did We took a photo next to the coliseum in Rome. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My family was on vacation in Italy. It was our favorite picture of the vacation.", "hypotheses": ["We took a beautiful picture of Spain.", "We took a photo next to the coliseum in Rome."]}
{"example_id": "85", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did bill said he would be fine and left. happen?", "a": "no", "prior_probs": [0.5220601301779403, 0.47793986982205966], "posterior_probs": [0.6299223516894003, 0.3700776483105997], "prior_entropy": 0.6921735658547711, "posterior_entropy": 0.6589969934851203, "delta_entropy": 0.03317657236965077, "eig_estimate": 0.032747182111887096, "pred": 0, "gold": 1, "confidence": 0.6299223516894003, "accuracy": 0, "tokens_in": 147, "tokens_out": 338, "tokens_total": 485, "latency_total": 0.20548790099928738, "latency_per_module": {"eig": [0.0976151499999105, 0.10182728499967197], "scorer": 0.006045465999704902}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 22, "tokens_out": 161, "tokens_total": 183}], "scorer": {"tokens_in": 102, "tokens_out": 17, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did bill said he would be fine and left. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Bill went to Lansing via air anyways. happen?\nAnswer:"], "scorer": "Observation: I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.\nQuestion: Did bill said he would be fine and left. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I told Bill not to go to Lansing. He got in an accident on the highway, he should of listened.", "hypotheses": ["bill said he would be fine and left.", "Bill went to Lansing via air anyways."]}
{"example_id": "86", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She was not very talented. happen?", "a": "yes", "prior_probs": [0.4965328533869331, 0.5034671466130668], "posterior_probs": [0.40596506435865604, 0.5940349356413439], "prior_entropy": 0.6931231381539927, "posterior_entropy": 0.6753562840245265, "delta_entropy": 0.017766854129466214, "eig_estimate": 0.01702790655072324, "pred": 1, "gold": 1, "confidence": 0.5940349356413439, "accuracy": 1, "tokens_in": 121, "tokens_out": 334, "tokens_total": 455, "latency_total": 0.2023835029986003, "latency_per_module": {"eig": [0.09823872800006939, 0.09806947999913973], "scorer": 0.00607529499939119}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}], "scorer": {"tokens_in": 80, "tokens_out": 14, "tokens_total": 94}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Liv's mother signed her up. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She was not very talented. happen?\nAnswer:"], "scorer": "Observation: Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!\nQuestion: Did She was not very talented. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Liv wanted to dance ballet. Liv's classmates her so jealous and shocked!", "hypotheses": ["Liv's mother signed her up.", "She was not very talented."]}
{"example_id": "87", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did One day a nurse said they should go to the coffee shop for a treat. happen?", "a": "no", "prior_probs": [0.48536694808139563, 0.5146330519186044], "posterior_probs": [0.6529475111829282, 0.34705248881707185], "prior_entropy": 0.6927188669867086, "posterior_entropy": 0.6456028950197156, "delta_entropy": 0.04711597196699302, "eig_estimate": 0.047062443197236625, "pred": 0, "gold": 1, "confidence": 0.6529475111829282, "accuracy": 0, "tokens_in": 178, "tokens_out": 347, "tokens_total": 525, "latency_total": 0.2107815519993892, "latency_per_module": {"eig": [0.10190808799961815, 0.10207827799968072], "scorer": 0.006795186000090325}, "tokens_per_module": {"eig": [{"tokens_in": 30, "tokens_out": 159, "tokens_total": 189}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 122, "tokens_out": 28, "tokens_total": 150}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did One day a nurse said they should go to the coffee shop for a treat. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The baseball player looked out the window at a coffee shop. happen?\nAnswer:"], "scorer": "Observation: A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!\nQuestion: Did One day a nurse said they should go to the coffee shop for a treat. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A little boy sick in a hospital worshiped a famous basketball player. Standing in front of the coffee shop was the ball player!", "hypotheses": ["One day a nurse said they should go to the coffee shop for a treat.", "The baseball player looked out the window at a coffee shop."]}
{"example_id": "88", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jake ended up getting free from the mud. happen?", "a": "no", "prior_probs": [0.500141024585799, 0.49985897541420105], "posterior_probs": [0.6109633844449505, 0.38903661555504954], "prior_entropy": 0.6931471407820773, "posterior_entropy": 0.6683152020064588, "delta_entropy": 0.02483193877561851, "eig_estimate": 0.024956066039149728, "pred": 0, "gold": 1, "confidence": 0.6109633844449505, "accuracy": 0, "tokens_in": 130, "tokens_out": 333, "tokens_total": 463, "latency_total": 0.2125969509988863, "latency_per_module": {"eig": [0.10762454399991839, 0.0987561519996234], "scorer": 0.006216254999344528}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 157, "tokens_total": 180}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 86, "tokens_out": 16, "tokens_total": 102}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jake ended up getting free from the mud. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jake got stuff in the mud. happen?\nAnswer:"], "scorer": "Observation: Jake was off roading. He had to get help to get out.\nQuestion: Did Jake ended up getting free from the mud. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake was off roading. He had to get help to get out.", "hypotheses": ["Jake ended up getting free from the mud.", "Jake got stuff in the mud."]}
{"example_id": "89", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I sat down to test out the game. happen?", "a": "yes", "prior_probs": [0.4483684781669959, 0.551631521833004], "posterior_probs": [0.3757050124781799, 0.6242949875218201], "prior_entropy": 0.6878060363970406, "posterior_entropy": 0.6619223165027031, "delta_entropy": 0.025883719894337576, "eig_estimate": 0.02605336219983001, "pred": 1, "gold": 1, "confidence": 0.6242949875218201, "accuracy": 1, "tokens_in": 135, "tokens_out": 339, "tokens_total": 474, "latency_total": 0.20481599500089942, "latency_per_module": {"eig": [0.10046746500029258, 0.09808962100032659], "scorer": 0.006258909000280255}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 88, "tokens_out": 19, "tokens_total": 107}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did It was so fun, I was a clown. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I sat down to test out the game. happen?\nAnswer:"], "scorer": "Observation: I got a new racing game yesterday. Finally after hours of playing I stopped.\nQuestion: Did I sat down to test out the game. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I got a new racing game yesterday. Finally after hours of playing I stopped.", "hypotheses": ["It was so fun, I was a clown.", "I sat down to test out the game."]}
{"example_id": "90", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She doesn't ask nicely to get things done. happen?", "a": "yes", "prior_probs": [0.3680791136694201, 0.63192088633058], "posterior_probs": [0.38931325276770257, 0.6106867472322975], "prior_entropy": 0.6579254323609798, "posterior_entropy": 0.6684399049933705, "delta_entropy": -0.010514472632390692, "eig_estimate": -0.010571477320040136, "pred": 1, "gold": 1, "confidence": 0.6106867472322975, "accuracy": 1, "tokens_in": 150, "tokens_out": 339, "tokens_total": 489, "latency_total": 0.2032205800005613, "latency_per_module": {"eig": [0.09808572700057994, 0.09946926300017367], "scorer": 0.0056655899998077075}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She doesn't ask nicely to get things indefinite. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She doesn't ask nicely to get things done. happen?\nAnswer:"], "scorer": "Observation: Freda is the boss of her office. Freda can't understand why people have a problem with her!\nQuestion: Did She doesn't ask nicely to get things done. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Freda is the boss of her office. Freda can't understand why people have a problem with her!", "hypotheses": ["She doesn't ask nicely to get things indefinite.", "She doesn't ask nicely to get things done."]}
{"example_id": "91", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did One of them had a bad cough. happen?", "a": "no", "prior_probs": [0.5296516170952371, 0.4703483829047629], "posterior_probs": [0.6103227199271737, 0.38967728007282637], "prior_entropy": 0.6913877116125613, "posterior_entropy": 0.668603511370976, "delta_entropy": 0.02278420024158534, "eig_estimate": 0.02289098172012194, "pred": 0, "gold": 1, "confidence": 0.6103227199271737, "accuracy": 0, "tokens_in": 157, "tokens_out": 340, "tokens_total": 497, "latency_total": 0.20842517399978533, "latency_per_module": {"eig": [0.10240313699978287, 0.09939075900001626], "scorer": 0.006631277999986196}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 110, "tokens_out": 20, "tokens_total": 130}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did One of them had a bad cough. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Carly, noticed her daughter had gotten into Poison Ivy. happen?\nAnswer:"], "scorer": "Observation: Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.\nQuestion: Did One of them had a bad cough. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carly had just called her kids in from playing outside. Then, she put some medicated lotion on her and hoped it would pass.", "hypotheses": ["One of them had a bad cough.", "Carly, noticed her daughter had gotten into Poison Ivy."]}
{"example_id": "92", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Alexis made sure the tree wasn't under 20 feet tall. happen?", "a": "no", "prior_probs": [0.48982945320701626, 0.5101705467929837], "posterior_probs": [0.5861605045654602, 0.41383949543453974], "prior_entropy": 0.6929402862449963, "posterior_entropy": 0.6782255481044289, "delta_entropy": 0.01471473814056734, "eig_estimate": 0.014869717407572102, "pred": 0, "gold": 1, "confidence": 0.5861605045654602, "accuracy": 0, "tokens_in": 163, "tokens_out": 343, "tokens_total": 506, "latency_total": 0.21015444000022399, "latency_per_module": {"eig": [0.10362673800045741, 0.09963551500004542], "scorer": 0.0068921869997211616}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 114, "tokens_out": 23, "tokens_total": 137}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Alexis made sure the tree wasn't under 20 feet tall. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Alexis was worried it would be too big. happen?\nAnswer:"], "scorer": "Observation: Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.\nQuestion: Did Alexis made sure the tree wasn't under 20 feet tall. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alexis had cut down a Christmas tree in the woods. When she put it up, it was the right size for her ceiling.", "hypotheses": ["Alexis made sure the tree wasn't under 20 feet tall.", "Alexis was worried it would be too big."]}
{"example_id": "93", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Francine decided to go to school to pursue her dreams. happen?", "a": "no", "prior_probs": [0.6592424657664189, 0.3407575342335811], "posterior_probs": [0.6563665991731485, 0.34363340082685134], "prior_entropy": 0.6415366677654246, "posterior_entropy": 0.6434161266894474, "delta_entropy": -0.0018794589240227477, "eig_estimate": -0.001857741051695472, "pred": 0, "gold": 1, "confidence": 0.6563665991731485, "accuracy": 0, "tokens_in": 151, "tokens_out": 340, "tokens_total": 491, "latency_total": 0.20810523200088937, "latency_per_module": {"eig": [0.10457820999999967, 0.09754874300051597], "scorer": 0.005978279000373732}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Francine decided to go to school to pursue her dreams. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did francine applied to business school. happen?\nAnswer:"], "scorer": "Observation: Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.\nQuestion: Did Francine decided to go to school to pursue her dreams. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Francine had always dreamed of being a fashion designer. She was so proud that she followed her dreams.", "hypotheses": ["Francine decided to go to school to pursue her dreams.", "francine applied to business school."]}
{"example_id": "94", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jack caught a ball that bounced over the fence. happen?", "a": "no", "prior_probs": [0.5440713882680684, 0.45592861173193144], "posterior_probs": [0.6289758792893719, 0.37102412071062807], "prior_entropy": 0.6892575603602893, "posterior_entropy": 0.6594984860689397, "delta_entropy": 0.029759074291349608, "eig_estimate": 0.029833877857470315, "pred": 0, "gold": 1, "confidence": 0.6289758792893719, "accuracy": 0, "tokens_in": 157, "tokens_out": 337, "tokens_total": 494, "latency_total": 0.2024614659994768, "latency_per_module": {"eig": [0.09930441699998482, 0.0974253710000994], "scorer": 0.005731677999392559}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 110, "tokens_out": 19, "tokens_total": 129}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jack caught a ball that bounced over the fence. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jack saw how wild the crowd was getting. happen?\nAnswer:"], "scorer": "Observation: Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.\nQuestion: Did Jack caught a ball that bounced over the fence. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jack went to his first basketball game with his dad. As the crowd cheered, Jack knew they'd never forget his first game.", "hypotheses": ["Jack caught a ball that bounced over the fence.", "Jack saw how wild the crowd was getting."]}
{"example_id": "95", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Nathan never got into a fight. happen?", "a": "no", "prior_probs": [0.5983544817378198, 0.40164551826218015], "posterior_probs": [0.6266960741546074, 0.3733039258453927], "prior_entropy": 0.6736732287085543, "posterior_entropy": 0.660690699410478, "delta_entropy": 0.012982529298076306, "eig_estimate": 0.012680969361671356, "pred": 0, "gold": 1, "confidence": 0.6266960741546074, "accuracy": 0, "tokens_in": 135, "tokens_out": 335, "tokens_total": 470, "latency_total": 0.20570885700090003, "latency_per_module": {"eig": [0.09962227100004384, 0.0997938240006988], "scorer": 0.006292762000157381}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 161, "tokens_total": 182}, {"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}], "scorer": {"tokens_in": 94, "tokens_out": 15, "tokens_total": 109}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Nathan never got into a fight. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Nathan got detention in school. happen?\nAnswer:"], "scorer": "Observation: Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.\nQuestion: Did Nathan never got into a fight. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nathan bullied a lot of other boys at school. Nathan knew his violence would never prevent his own abuse.", "hypotheses": ["Nathan never got into a fight.", "Nathan got detention in school."]}
{"example_id": "96", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Mike found pasta hard to make. happen?", "a": "no", "prior_probs": [0.6835728746419494, 0.3164271253580506], "posterior_probs": [0.7071798178427685, 0.29282018215723155], "prior_entropy": 0.624146934819759, "posterior_entropy": 0.6046575516987129, "delta_entropy": 0.01948938312104609, "eig_estimate": 0.018876379435246227, "pred": 0, "gold": 1, "confidence": 0.7071798178427685, "accuracy": 0, "tokens_in": 131, "tokens_out": 331, "tokens_total": 462, "latency_total": 0.20511254300072324, "latency_per_module": {"eig": [0.099600546000147, 0.0991902020005], "scorer": 0.006321795000076236}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 20, "tokens_out": 157, "tokens_total": 177}], "scorer": {"tokens_in": 90, "tokens_out": 14, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Mike found pasta hard to make. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Mile loves italian food. happen?\nAnswer:"], "scorer": "Observation: Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.\nQuestion: Did Mike found pasta hard to make. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike made homemade pasta one day. Pasta is now a regular staple of Mike's diet.", "hypotheses": ["Mike found pasta hard to make.", "Mile loves italian food."]}
{"example_id": "97", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He didn't know that part of town very good. happen?", "a": "yes", "prior_probs": [0.5251973619984914, 0.47480263800150857], "posterior_probs": [0.3719844503562799, 0.62801554964372], "prior_entropy": 0.6918768284318874, "posterior_entropy": 0.6600033976174073, "delta_entropy": 0.03187343081448002, "eig_estimate": 0.031951453539142374, "pred": 1, "gold": 1, "confidence": 0.62801554964372, "accuracy": 1, "tokens_in": 149, "tokens_out": 335, "tokens_total": 484, "latency_total": 0.2053493499997785, "latency_per_module": {"eig": [0.10118212900033541, 0.0982008049995784], "scorer": 0.005966415999864694}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}, {"tokens_in": 25, "tokens_out": 158, "tokens_total": 183}], "scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Randy knew the area well. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He didn't know that part of town very good. happen?\nAnswer:"], "scorer": "Observation: Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.\nQuestion: Did He didn't know that part of town very good. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy told his friend how relieved he was he found it.", "hypotheses": ["Randy knew the area well.", "He didn't know that part of town very good."]}
{"example_id": "98", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She told him she did not like him. happen?", "a": "yes", "prior_probs": [0.4126552520352513, 0.5873447479647488], "posterior_probs": [0.38064967289631346, 0.6193503271036865], "prior_entropy": 0.6778104031708004, "posterior_entropy": 0.664381277228171, "delta_entropy": 0.01342912594262946, "eig_estimate": 0.013524366969335478, "pred": 1, "gold": 1, "confidence": 0.6193503271036865, "accuracy": 1, "tokens_in": 148, "tokens_out": 338, "tokens_total": 486, "latency_total": 0.20335357800104248, "latency_per_module": {"eig": [0.10021194599994487, 0.09751005100042676], "scorer": 0.005631581000670849}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 102, "tokens_out": 18, "tokens_total": 120}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Cat sent a love note to the boy. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She told him she did not like him. happen?\nAnswer:"], "scorer": "Observation: Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!\nQuestion: Did She told him she did not like him. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Cay had a crush on a boy in her class. He smiled at her after and said he liked her too!", "hypotheses": ["Cat sent a love note to the boy.", "She told him she did not like him."]}
{"example_id": "99", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Karen missed Lacy so much, she couldn't bear not talking to her friend. happen?", "a": "no", "prior_probs": [0.5060554166199094, 0.49394458338009056], "posterior_probs": [0.5292023292396653, 0.4707976707603348], "prior_entropy": 0.693073842624229, "posterior_entropy": 0.6914406575305173, "delta_entropy": 0.0016331850937116865, "eig_estimate": 0.0015844191145689913, "pred": 0, "gold": 1, "confidence": 0.5292023292396653, "accuracy": 0, "tokens_in": 172, "tokens_out": 352, "tokens_total": 524, "latency_total": 0.21411639000052674, "latency_per_module": {"eig": [0.10439429899997776, 0.10272496700054035], "scorer": 0.0069971240000086254}, "tokens_per_module": {"eig": [{"tokens_in": 31, "tokens_out": 159, "tokens_total": 190}, {"tokens_in": 31, "tokens_out": 158, "tokens_total": 189}], "scorer": {"tokens_in": 110, "tokens_out": 35, "tokens_total": 145}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Karen missed Lacy so much, she couldn't bear not talking to her friend. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Lacy missed Karen so much, she couldn't bear not talking to her friend. happen?\nAnswer:"], "scorer": "Observation: Lacy and Karen got in a fight. Karen apologized too so they could be friends again.\nQuestion: Did Karen missed Lacy so much, she couldn't bear not talking to her friend. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lacy and Karen got in a fight. Karen apologized too so they could be friends again.", "hypotheses": ["Karen missed Lacy so much, she couldn't bear not talking to her friend.", "Lacy missed Karen so much, she couldn't bear not talking to her friend."]}
{"example_id": "100", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My friend told us he had cancer and was expected to die in a week. happen?", "a": "no", "prior_probs": [0.6252029901029774, 0.3747970098970227], "posterior_probs": [0.6939831178540657, 0.30601688214593425], "prior_entropy": 0.6614594576997013, "posterior_entropy": 0.6158765210874091, "delta_entropy": 0.04558293661229218, "eig_estimate": 0.046678918521766824, "pred": 0, "gold": 1, "confidence": 0.6939831178540657, "accuracy": 0, "tokens_in": 169, "tokens_out": 345, "tokens_total": 514, "latency_total": 0.21351319100085675, "latency_per_module": {"eig": [0.10430210300000908, 0.10229315100059466], "scorer": 0.006917937000253005}, "tokens_per_module": {"eig": [{"tokens_in": 30, "tokens_out": 160, "tokens_total": 190}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 116, "tokens_out": 25, "tokens_total": 141}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did My friend told us he had cancer and was expected to die in a week. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did my friend usually talks about some business deal. happen?\nAnswer:"], "scorer": "Observation: My friend had an announcement to make. So, I put on a smile and wished him the best of luck.\nQuestion: Did My friend told us he had cancer and was expected to die in a week. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My friend had an announcement to make. So, I put on a smile and wished him the best of luck.", "hypotheses": ["My friend told us he had cancer and was expected to die in a week.", "my friend usually talks about some business deal."]}
{"example_id": "101", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did George was the underdog, but he had been training months to compete in this event. happen?", "a": "yes", "prior_probs": [0.5037615781635802, 0.49623842183641964], "posterior_probs": [0.33047910829379884, 0.6695208917062011], "prior_entropy": 0.6931188813504345, "posterior_entropy": 0.6345174140752035, "delta_entropy": 0.05860146727523108, "eig_estimate": 0.058296229087462195, "pred": 1, "gold": 1, "confidence": 0.6695208917062011, "accuracy": 1, "tokens_in": 169, "tokens_out": 347, "tokens_total": 516, "latency_total": 0.21254686699921876, "latency_per_module": {"eig": [0.10412471699964954, 0.10106982900015282], "scorer": 0.007352320999416406}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 31, "tokens_out": 160, "tokens_total": 191}], "scorer": {"tokens_in": 114, "tokens_out": 27, "tokens_total": 141}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did George trained hard for two days before the fight. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did George was the underdog, but he had been training months to compete in this event. happen?\nAnswer:"], "scorer": "Observation: George was about to participate in his first professional fight. George proved his skills and won his first match.\nQuestion: Did George was the underdog, but he had been training months to compete in this event. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "George was about to participate in his first professional fight. George proved his skills and won his first match.", "hypotheses": ["George trained hard for two days before the fight.", "George was the underdog, but he had been training months to compete in this event."]}
{"example_id": "102", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The team that Bob and his kids like won. happen?", "a": "no", "prior_probs": [0.6479163582241244, 0.3520836417758756], "posterior_probs": [0.7038117543767948, 0.29618824562320534], "prior_entropy": 0.6487269617884174, "posterior_entropy": 0.607599932314811, "delta_entropy": 0.04112702947360636, "eig_estimate": 0.04112702947360636, "pred": 0, "gold": 1, "confidence": 0.7038117543767948, "accuracy": 0, "tokens_in": 142, "tokens_out": 338, "tokens_total": 480, "latency_total": 0.2130152089994226, "latency_per_module": {"eig": [0.10632896700008132, 0.10028453999984777], "scorer": 0.006401701999493525}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 96, "tokens_out": 18, "tokens_total": 114}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The team that Bob and his kids like won. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did They played basketball out back all afternoon. happen?\nAnswer:"], "scorer": "Observation: Bob and his kids love football. Bob and his kids share a hug to celebrate the win.\nQuestion: Did The team that Bob and his kids like won. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Bob and his kids love football. Bob and his kids share a hug to celebrate the win.", "hypotheses": ["The team that Bob and his kids like won.", "They played basketball out back all afternoon."]}
{"example_id": "103", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He didn't see the toy he really wanted in any of the aisles. happen?", "a": "no", "prior_probs": [0.5873619102187421, 0.412638089781258], "posterior_probs": [0.688925036893191, 0.311074963106809], "prior_entropy": 0.67780434429697, "posterior_entropy": 0.6199580637323454, "delta_entropy": 0.0578462805646246, "eig_estimate": 0.057551641753587936, "pred": 0, "gold": 1, "confidence": 0.688925036893191, "accuracy": 0, "tokens_in": 165, "tokens_out": 343, "tokens_total": 508, "latency_total": 0.21302600299986807, "latency_per_module": {"eig": [0.10241323899936106, 0.10402364899982786], "scorer": 0.006589115000679158}, "tokens_per_module": {"eig": [{"tokens_in": 31, "tokens_out": 159, "tokens_total": 190}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 112, "tokens_out": 25, "tokens_total": 137}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He didn't see the toy he really wanted in any of the aisles. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Alex saw a game he really wanted. happen?\nAnswer:"], "scorer": "Observation: Alex was at target with his mom. He begged his mother to buy it until she gave in.\nQuestion: Did He didn't see the toy he really wanted in any of the aisles. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Alex was at target with his mom. He begged his mother to buy it until she gave in.", "hypotheses": ["He didn't see the toy he really wanted in any of the aisles.", "Alex saw a game he really wanted."]}
{"example_id": "104", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Nova applied to a few dance schools but was denied by her first choice. happen?", "a": "no", "prior_probs": [0.6404063862747398, 0.3595936137252602], "posterior_probs": [0.6551086188003145, 0.34489138119968565], "prior_entropy": 0.6531840162431645, "posterior_entropy": 0.6442267151415673, "delta_entropy": 0.008957301101597204, "eig_estimate": 0.00914613239666362, "pred": 0, "gold": 1, "confidence": 0.6551086188003145, "accuracy": 0, "tokens_in": 148, "tokens_out": 343, "tokens_total": 491, "latency_total": 0.20814971099935065, "latency_per_module": {"eig": [0.10218072399948142, 0.0998445189998165], "scorer": 0.00612446800005273}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 159, "tokens_total": 188}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 98, "tokens_out": 24, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Nova applied to a few dance schools but was denied by her first choice. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Nova applied to one dance school. happen?\nAnswer:"], "scorer": "Observation: Nova dreamed of being a professional dancer. Nova's second choice accepted her.\nQuestion: Did Nova applied to a few dance schools but was denied by her first choice. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nova dreamed of being a professional dancer. Nova's second choice accepted her.", "hypotheses": ["Nova applied to a few dance schools but was denied by her first choice.", "Nova applied to one dance school."]}
{"example_id": "105", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did An unexpected event happened when the tide didn't come in that day. happen?", "a": "no", "prior_probs": [0.45470671903037124, 0.5452932809696288], "posterior_probs": [0.6297319693761745, 0.3702680306238256], "prior_entropy": 0.6890385880217316, "posterior_entropy": 0.6590981770031632, "delta_entropy": 0.02994041101856837, "eig_estimate": 0.03009630931102547, "pred": 0, "gold": 1, "confidence": 0.6297319693761745, "accuracy": 0, "tokens_in": 160, "tokens_out": 344, "tokens_total": 504, "latency_total": 0.2127132290006557, "latency_per_module": {"eig": [0.10278161899987026, 0.10365604600065126], "scorer": 0.006275564000134182}, "tokens_per_module": {"eig": [{"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 108, "tokens_out": 24, "tokens_total": 132}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did An unexpected event happened when the tide didn't come in that day. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The sand castle was built right on the shore. happen?\nAnswer:"], "scorer": "Observation: Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.\nQuestion: Did An unexpected event happened when the tide didn't come in that day. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Carla and Sam spent the morning building a sand castle. The tide had rolled in, smashing it.", "hypotheses": ["An unexpected event happened when the tide didn't come in that day.", "The sand castle was built right on the shore."]}
{"example_id": "106", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tami was tall for her age. happen?", "a": "yes", "prior_probs": [0.43206894712477, 0.5679310528752299], "posterior_probs": [0.3553356120242836, 0.6446643879757163], "prior_entropy": 0.6838893198791889, "posterior_entropy": 0.6506871683184323, "delta_entropy": 0.033202151560756565, "eig_estimate": 0.032698790509057794, "pred": 1, "gold": 1, "confidence": 0.6446643879757163, "accuracy": 1, "tokens_in": 154, "tokens_out": 336, "tokens_total": 490, "latency_total": 0.2694820880005864, "latency_per_module": {"eig": [0.15473526599998877, 0.10841776100005518], "scorer": 0.006329061000542424}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 158, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 108, "tokens_out": 18, "tokens_total": 126}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She wanted to be less involved and lose friends. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tami was tall for her age. happen?\nAnswer:"], "scorer": "Observation: Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.\nQuestion: Did Tami was tall for her age. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tami was in the 8th grade. Tami made up her mind to try out for the JV Volleyball team.", "hypotheses": ["She wanted to be less involved and lose friends.", "Tami was tall for her age."]}
{"example_id": "107", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Donald is not the candidate I want for president. happen?", "a": "yes", "prior_probs": [0.4372740685760933, 0.5627259314239066], "posterior_probs": [0.3843425855515875, 0.6156574144484125], "prior_entropy": 0.6852573237333645, "posterior_entropy": 0.6661500686378553, "delta_entropy": 0.019107255095509146, "eig_estimate": 0.019108111017940035, "pred": 1, "gold": 1, "confidence": 0.6156574144484125, "accuracy": 1, "tokens_in": 129, "tokens_out": 339, "tokens_total": 468, "latency_total": 0.2777600090003034, "latency_per_module": {"eig": [0.12446498300050735, 0.14274424199993518], "scorer": 0.010550783999860869}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 82, "tokens_out": 19, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Donald is a selfless, wonderful person. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Donald is not the candidate I want for president. happen?\nAnswer:"], "scorer": "Observation: Donald is running for president. Hopefully he loses the election.\nQuestion: Did Donald is not the candidate I want for president. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Donald is running for president. Hopefully he loses the election.", "hypotheses": ["Donald is a selfless, wonderful person.", "Donald is not the candidate I want for president."]}
{"example_id": "108", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I had rope that I used for jumping at home. happen?", "a": "no", "prior_probs": [0.5228436593353636, 0.4771563406646364], "posterior_probs": [0.6700584353828868, 0.32994156461711316], "prior_entropy": 0.6921031516315433, "posterior_entropy": 0.6341372449229019, "delta_entropy": 0.057965906708641435, "eig_estimate": 0.058160575838237194, "pred": 0, "gold": 1, "confidence": 0.6700584353828868, "accuracy": 0, "tokens_in": 147, "tokens_out": 340, "tokens_total": 487, "latency_total": 0.3747473080002237, "latency_per_module": {"eig": [0.17397671100025036, 0.19013847699989128], "scorer": 0.010632120000082068}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}], "scorer": {"tokens_in": 98, "tokens_out": 21, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I had rope that I used for jumping at home. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I committed to exercise every month by jumping rope. happen?\nAnswer:"], "scorer": "Observation: I was very out of shape. After weeks of jumping rope, I began to feel excellent.\nQuestion: Did I had rope that I used for jumping at home. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was very out of shape. After weeks of jumping rope, I began to feel excellent.", "hypotheses": ["I had rope that I used for jumping at home.", "I committed to exercise every month by jumping rope."]}
{"example_id": "109", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ora's mom kicked her out of the house so their gas bill would reduce. happen?", "a": "yes", "prior_probs": [0.47234653950590905, 0.527653460494091], "posterior_probs": [0.3551095925255607, 0.6448904074744394], "prior_entropy": 0.6916169721313477, "posterior_entropy": 0.6505524244193581, "delta_entropy": 0.04106454771198953, "eig_estimate": 0.04055252779753796, "pred": 1, "gold": 1, "confidence": 0.6448904074744394, "accuracy": 1, "tokens_in": 177, "tokens_out": 347, "tokens_total": 524, "latency_total": 0.34334296100041684, "latency_per_module": {"eig": [0.20211903400013398, 0.13089128299998265], "scorer": 0.010332644000300206}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 31, "tokens_out": 160, "tokens_total": 191}], "scorer": {"tokens_in": 122, "tokens_out": 27, "tokens_total": 149}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ora's family needed to use more gas. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ora's mom kicked her out of the house so their gas bill would reduce. happen?\nAnswer:"], "scorer": "Observation: Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.\nQuestion: Did Ora's mom kicked her out of the house so their gas bill would reduce. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ora's mom told her that the family had a very high gas bill. Ora was unhappy, but she understood.", "hypotheses": ["Ora's family needed to use more gas.", "Ora's mom kicked her out of the house so their gas bill would reduce."]}
{"example_id": "110", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I got up and tried to find out why they kept barking. happen?", "a": "yes", "prior_probs": [0.5043134215278807, 0.4956865784721193], "posterior_probs": [0.3765506401523359, 0.6234493598476641], "prior_entropy": 0.6931099688858192, "posterior_entropy": 0.6623502181283265, "delta_entropy": 0.03075975075749271, "eig_estimate": 0.03086467749412998, "pred": 1, "gold": 1, "confidence": 0.6234493598476641, "accuracy": 1, "tokens_in": 178, "tokens_out": 344, "tokens_total": 522, "latency_total": 0.2659656380001252, "latency_per_module": {"eig": [0.12958044199967844, 0.12787561799996183], "scorer": 0.008509578000484908}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 126, "tokens_out": 24, "tokens_total": 150}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The puppy was given to me by a stork. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I got up and tried to find out why they kept barking. happen?\nAnswer:"], "scorer": "Observation: Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.\nQuestion: Did I got up and tried to find out why they kept barking. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Some dog kept me awake till I could not take it anymore. I gave the puppy the food and I went back to bed, both of us happy.", "hypotheses": ["The puppy was given to me by a stork.", "I got up and tried to find out why they kept barking."]}
{"example_id": "111", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Dan ate a lot of food very slowly, hoping to win. happen?", "a": "no", "prior_probs": [0.5925861323348321, 0.4074138676651678], "posterior_probs": [0.6746455779099958, 0.3253544220900042], "prior_entropy": 0.6759034511018422, "posterior_entropy": 0.6308397848142369, "delta_entropy": 0.045063666287605386, "eig_estimate": 0.04509796286449976, "pred": 0, "gold": 1, "confidence": 0.6746455779099958, "accuracy": 0, "tokens_in": 156, "tokens_out": 343, "tokens_total": 499, "latency_total": 0.28191470999991, "latency_per_module": {"eig": [0.12899474400001054, 0.1419315650000499], "scorer": 0.010988400999849546}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 161, "tokens_total": 188}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 106, "tokens_out": 22, "tokens_total": 128}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Dan ate a lot of food very slowly, hoping to win. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Dan tried to eat 30 cold hot dogs. happen?\nAnswer:"], "scorer": "Observation: Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.\nQuestion: Did Dan ate a lot of food very slowly, hoping to win. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Dan entered an eating contest at his local fair. Dan lost the contest, and was sick for days.", "hypotheses": ["Dan ate a lot of food very slowly, hoping to win.", "Dan tried to eat 30 cold hot dogs."]}
{"example_id": "112", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Melissa's friend insisted they go out to eat somewhere Melissa hated. happen?", "a": "yes", "prior_probs": [0.5464544162805514, 0.4535455837194486], "posterior_probs": [0.3533866836012695, 0.6466133163987305], "prior_entropy": 0.688824924075583, "posterior_entropy": 0.6495179582310981, "delta_entropy": 0.0393069658444849, "eig_estimate": 0.039499772411275225, "pred": 1, "gold": 1, "confidence": 0.6466133163987305, "accuracy": 1, "tokens_in": 152, "tokens_out": 340, "tokens_total": 492, "latency_total": 0.4194070069997906, "latency_per_module": {"eig": [0.18847873599952436, 0.22041360500043083], "scorer": 0.010514665999835415}, "tokens_per_module": {"eig": [{"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 106, "tokens_out": 20, "tokens_total": 126}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Melissa was being rude. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Melissa's friend insisted they go out to eat somewhere Melissa hated. happen?\nAnswer:"], "scorer": "Observation: Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.\nQuestion: Did Melissa's friend insisted they go out to eat somewhere Melissa hated. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Melissa's friend was coming for a visit. Luckily, when Melissa pointed this out, her friend apologized.", "hypotheses": ["Melissa was being rude.", "Melissa's friend insisted they go out to eat somewhere Melissa hated."]}
{"example_id": "113", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Nell was told to do something unexpected. happen?", "a": "yes", "prior_probs": [0.4783050226856938, 0.5216949773143061], "posterior_probs": [0.3966425942713651, 0.6033574057286349], "prior_entropy": 0.6922055408777172, "posterior_entropy": 0.6716268503199789, "delta_entropy": 0.02057869055773831, "eig_estimate": 0.020571742679951235, "pred": 1, "gold": 1, "confidence": 0.6033574057286349, "accuracy": 1, "tokens_in": 152, "tokens_out": 338, "tokens_total": 490, "latency_total": 0.5121156020004491, "latency_per_module": {"eig": [0.31925032199978887, 0.18604123300065112], "scorer": 0.006824047000009159}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Nell studied biology to draw an animal. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Nell was told to do something unexpected. happen?\nAnswer:"], "scorer": "Observation: Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.\nQuestion: Did Nell was told to do something unexpected. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nell's teacher admired her artistic student. The teacher was impressed that Nell drew an X-ray of an animal.", "hypotheses": ["Nell studied biology to draw an animal.", "Nell was told to do something unexpected."]}
{"example_id": "114", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did His dad asked Eli how to tie his shoes. happen?", "a": "no", "prior_probs": [0.527262563582004, 0.47273743641799604], "posterior_probs": [0.5579518583675479, 0.4420481416324521], "prior_entropy": 0.69165994837897, "posterior_entropy": 0.6864152247577697, "delta_entropy": 0.00524472362120032, "eig_estimate": 0.005455680005394292, "pred": 0, "gold": 1, "confidence": 0.5579518583675479, "accuracy": 0, "tokens_in": 145, "tokens_out": 340, "tokens_total": 485, "latency_total": 0.22840433700002905, "latency_per_module": {"eig": [0.11100738699951762, 0.110522855000454], "scorer": 0.006874095000057423}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 96, "tokens_out": 20, "tokens_total": 116}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did His dad asked Eli how to tie his shoes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did eli asked his dad how to tie his shoes. happen?\nAnswer:"], "scorer": "Observation: Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.\nQuestion: Did His dad asked Eli how to tie his shoes. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Eli didn't know how to tie his shoes. Eli learned and now ties like a pro.", "hypotheses": ["His dad asked Eli how to tie his shoes.", "eli asked his dad how to tie his shoes."]}
{"example_id": "115", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Francine saw bananas for sale at the fair. happen?", "a": "yes", "prior_probs": [0.48110675756754917, 0.5188932424324508], "posterior_probs": [0.35823196518572076, 0.6417680348142794], "prior_entropy": 0.6924331013528291, "posterior_entropy": 0.6523941423340074, "delta_entropy": 0.04003895901882171, "eig_estimate": 0.04003895901882171, "pred": 1, "gold": 1, "confidence": 0.6417680348142794, "accuracy": 1, "tokens_in": 154, "tokens_out": 338, "tokens_total": 492, "latency_total": 0.2163406230001783, "latency_per_module": {"eig": [0.10634359800042148, 0.10352440299993759], "scorer": 0.006472621999819239}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 108, "tokens_out": 18, "tokens_total": 126}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She was anxious to buy some today. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Francine saw bananas for sale at the fair. happen?\nAnswer:"], "scorer": "Observation: When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.\nQuestion: Did Francine saw bananas for sale at the fair. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Francine was a little girl, her favorite treat was cotton candy. She purchased a bag to share with her friends.", "hypotheses": ["She was anxious to buy some today.", "Francine saw bananas for sale at the fair."]}
{"example_id": "116", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ben pitched a large tent on the sand to block out the sun. happen?", "a": "yes", "prior_probs": [0.47096512638183935, 0.5290348736181606], "posterior_probs": [0.3482738071706149, 0.6517261928293852], "prior_entropy": 0.6914601839197501, "posterior_entropy": 0.6463715041180824, "delta_entropy": 0.04508867980166764, "eig_estimate": 0.04493085715834204, "pred": 1, "gold": 1, "confidence": 0.6517261928293852, "accuracy": 1, "tokens_in": 154, "tokens_out": 342, "tokens_total": 496, "latency_total": 0.2170444999992469, "latency_per_module": {"eig": [0.10436201899938169, 0.10504537999986496], "scorer": 0.007637101000000257}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}], "scorer": {"tokens_in": 104, "tokens_out": 22, "tokens_total": 126}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ben spent hours sitting in the sun. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ben pitched a large tent on the sand to block out the sun. happen?\nAnswer:"], "scorer": "Observation: Ben went to the beach on a sunny day. Ben crawled into his tent and napped.\nQuestion: Did Ben pitched a large tent on the sand to block out the sun. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ben went to the beach on a sunny day. Ben crawled into his tent and napped.", "hypotheses": ["Ben spent hours sitting in the sun.", "Ben pitched a large tent on the sand to block out the sun."]}
{"example_id": "117", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tili ran for the prison gate one night. happen?", "a": "no", "prior_probs": [0.6605205014879388, 0.33947949851206116], "posterior_probs": [0.7161064630653952, 0.2838935369346048], "prior_entropy": 0.6406896284443233, "posterior_entropy": 0.5965931208986868, "delta_entropy": 0.04409650754563654, "eig_estimate": 0.04369887232598559, "pred": 0, "gold": 1, "confidence": 0.7161064630653952, "accuracy": 0, "tokens_in": 137, "tokens_out": 335, "tokens_total": 472, "latency_total": 0.30550998899980186, "latency_per_module": {"eig": [0.10311296100007894, 0.19096743399950356], "scorer": 0.011429594000219367}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}], "scorer": {"tokens_in": 94, "tokens_out": 15, "tokens_total": 109}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tili ran for the prison gate one night. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Doug formulated a plan. happen?\nAnswer:"], "scorer": "Observation: Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.\nQuestion: Did Tili ran for the prison gate one night. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tili wanted to escape from prison. Tili's escape attempt was stopped and punished.", "hypotheses": ["Tili ran for the prison gate one night.", "Doug formulated a plan."]}
{"example_id": "118", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Sam decided to work two jobs to pay off his debt. happen?", "a": "yes", "prior_probs": [0.47853895165243243, 0.5214610483475676], "posterior_probs": [0.4087280655967913, 0.5912719344032088], "prior_entropy": 0.6922257443158863, "posterior_entropy": 0.6763922616097241, "delta_entropy": 0.01583348270616225, "eig_estimate": 0.015799900308989014, "pred": 1, "gold": 1, "confidence": 0.5912719344032088, "accuracy": 1, "tokens_in": 157, "tokens_out": 342, "tokens_total": 499, "latency_total": 0.23694709099981992, "latency_per_module": {"eig": [0.12231339000027219, 0.10843531699993036], "scorer": 0.006198383999617363}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}], "scorer": {"tokens_in": 106, "tokens_out": 23, "tokens_total": 129}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Sam lost his job and could not make the payment. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Sam decided to work two jobs to pay off his debt. happen?\nAnswer:"], "scorer": "Observation: Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.\nQuestion: Did Sam decided to work two jobs to pay off his debt. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam had to pay off his credit card debt. Sam eventually made enough to pay off his credit card debt.", "hypotheses": ["Sam lost his job and could not make the payment.", "Sam decided to work two jobs to pay off his debt."]}
{"example_id": "119", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Katie needed to hurry and get it clean. happen?", "a": "yes", "prior_probs": [0.36083121223098397, 0.639168787769016], "posterior_probs": [0.31434060061653146, 0.6856593993834684], "prior_entropy": 0.6538949456312322, "posterior_entropy": 0.622529733706699, "delta_entropy": 0.03136521192453323, "eig_estimate": 0.03137671424661659, "pred": 1, "gold": 1, "confidence": 0.6856593993834684, "accuracy": 1, "tokens_in": 146, "tokens_out": 339, "tokens_total": 485, "latency_total": 0.22456130499995197, "latency_per_module": {"eig": [0.10649199599993153, 0.11163738699997339], "scorer": 0.006431922000047052}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 100, "tokens_out": 19, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The kitchen got so bad mold might grow. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Katie needed to hurry and get it clean. happen?\nAnswer:"], "scorer": "Observation: Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.\nQuestion: Did Katie needed to hurry and get it clean. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Katie had let her kitchen get pretty messy. She used bleach to be sure that it was clean and safe.", "hypotheses": ["The kitchen got so bad mold might grow.", "Katie needed to hurry and get it clean."]}
{"example_id": "120", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The house had boarded windows and looked bad. happen?", "a": "yes", "prior_probs": [0.47581760802240475, 0.5241823919775953], "posterior_probs": [0.3382764283609428, 0.6617235716390572], "prior_entropy": 0.6919771479977935, "posterior_entropy": 0.6398856181283568, "delta_entropy": 0.05209152986943677, "eig_estimate": 0.05173853559775907, "pred": 1, "gold": 1, "confidence": 0.6617235716390572, "accuracy": 1, "tokens_in": 152, "tokens_out": 340, "tokens_total": 492, "latency_total": 0.21721971399983886, "latency_per_module": {"eig": [0.1062591979998615, 0.10482722300002933], "scorer": 0.006133292999948026}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 104, "tokens_out": 20, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The directions took Randy thru a great part of town. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The house had boarded windows and looked bad. happen?\nAnswer:"], "scorer": "Observation: Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.\nQuestion: Did The house had boarded windows and looked bad. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Randy's friend gave him directions to his house. Randy saw the house but kept on driving and didn't come back.", "hypotheses": ["The directions took Randy thru a great part of town.", "The house had boarded windows and looked bad."]}
{"example_id": "121", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I called my insurance company to see if I could get assistance. happen?", "a": "yes", "prior_probs": [0.41262664839655694, 0.5873733516034432], "posterior_probs": [0.3666908317429713, 0.6333091682570287], "prior_entropy": 0.6778003044142535, "posterior_entropy": 0.6571709675084099, "delta_entropy": 0.020629336905843676, "eig_estimate": 0.02067055411880321, "pred": 1, "gold": 1, "confidence": 0.6333091682570287, "accuracy": 1, "tokens_in": 167, "tokens_out": 340, "tokens_total": 507, "latency_total": 0.23294284500025242, "latency_per_module": {"eig": [0.1094641710005817, 0.1161002719991302], "scorer": 0.007378402000540518}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 118, "tokens_out": 21, "tokens_total": 139}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I was worried about how to tow. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I called my insurance company to see if I could get assistance. happen?\nAnswer:"], "scorer": "Observation: I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!\nQuestion: Did I called my insurance company to see if I could get assistance. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was driving on the highway when my car suddenly broke down. Turns out my insurance covered the tow truck and the rest was cheap!", "hypotheses": ["I was worried about how to tow.", "I called my insurance company to see if I could get assistance."]}
{"example_id": "122", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tia had dinner with her parents. happen?", "a": "no", "prior_probs": [0.5599508886754255, 0.4400491113245745], "posterior_probs": [0.5905579070366755, 0.40944209296332457], "prior_entropy": 0.6859416391441366, "posterior_entropy": 0.67665484443315, "delta_entropy": 0.00928679471098659, "eig_estimate": 0.009350233389150997, "pred": 0, "gold": 1, "confidence": 0.5905579070366755, "accuracy": 0, "tokens_in": 152, "tokens_out": 337, "tokens_total": 489, "latency_total": 0.33984686300027533, "latency_per_module": {"eig": [0.1919253200003368, 0.14115557199966133], "scorer": 0.0067659710002772044}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}], "scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tia had dinner with her parents. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tia thought something seemed good between her parents. happen?\nAnswer:"], "scorer": "Observation: Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.\nQuestion: Did Tia had dinner with her parents. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tia was in college and went home for the holidays. Before she left her father told her that they were getting a divorce.", "hypotheses": ["Tia had dinner with her parents.", "Tia thought something seemed good between her parents."]}
{"example_id": "123", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Joe tripped down the stairs with his shoes untied. happen?", "a": "no", "prior_probs": [0.554069221489842, 0.44593077851015805], "posterior_probs": [0.616268266930562, 0.3837317330694381], "prior_entropy": 0.6872887698399445, "posterior_entropy": 0.6658614729796433, "delta_entropy": 0.021427296860301204, "eig_estimate": 0.022004649947309107, "pred": 0, "gold": 1, "confidence": 0.616268266930562, "accuracy": 0, "tokens_in": 169, "tokens_out": 341, "tokens_total": 510, "latency_total": 0.24022887000046467, "latency_per_module": {"eig": [0.11286845300037385, 0.11819005100005597], "scorer": 0.009170366000034846}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 120, "tokens_out": 21, "tokens_total": 141}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Joe tripped down the stairs with his shoes untied. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Joe tied them and fell down the stairs. happen?\nAnswer:"], "scorer": "Observation: Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.\nQuestion: Did Joe tripped down the stairs with his shoes untied. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe's mother bugged him constantly to tie his shoelaces. As he lay at the bottom of the stairs he wished he'd listened.", "hypotheses": ["Joe tripped down the stairs with his shoes untied.", "Joe tied them and fell down the stairs."]}
{"example_id": "124", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Nita was never able to beat her Dad. happen?", "a": "yes", "prior_probs": [0.4648569698295527, 0.5351430301704473], "posterior_probs": [0.4163043480885633, 0.5836956519114367], "prior_entropy": 0.6906750776487804, "posterior_entropy": 0.679071085773872, "delta_entropy": 0.01160399187490846, "eig_estimate": 0.011635371667147254, "pred": 1, "gold": 1, "confidence": 0.5836956519114367, "accuracy": 1, "tokens_in": 154, "tokens_out": 339, "tokens_total": 493, "latency_total": 0.279073038000206, "latency_per_module": {"eig": [0.1380913929997405, 0.13443418099996052], "scorer": 0.006547464000504988}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}], "scorer": {"tokens_in": 106, "tokens_out": 20, "tokens_total": 126}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Nita practiced playing rummy with a dog. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Nita was never able to beat her Dad. happen?\nAnswer:"], "scorer": "Observation: Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.\nQuestion: Did Nita was never able to beat her Dad. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nita was playing rummy with her dad. Nita was so happy she finally beat her dad at rummy.", "hypotheses": ["Nita practiced playing rummy with a dog.", "Nita was never able to beat her Dad."]}
{"example_id": "125", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Fred saw a child come over with a pin. happen?", "a": "no", "prior_probs": [0.5531757724693236, 0.44682422753067647], "posterior_probs": [0.6776444487220556, 0.32235555127794435], "prior_entropy": 0.6874811455731205, "posterior_entropy": 0.6286322725880162, "delta_entropy": 0.05884887298510422, "eig_estimate": 0.05993922621137267, "pred": 0, "gold": 1, "confidence": 0.6776444487220556, "accuracy": 0, "tokens_in": 141, "tokens_out": 336, "tokens_total": 477, "latency_total": 0.2456661249998433, "latency_per_module": {"eig": [0.1098988769999778, 0.12687422300041362], "scorer": 0.008893024999451882}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Fred saw a child come over with a pin. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Fred fill one balloon too small. happen?\nAnswer:"], "scorer": "Observation: Fred had a job at the fair to fill the balloons. The balloon popped in his face!\nQuestion: Did Fred saw a child come over with a pin. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Fred had a job at the fair to fill the balloons. The balloon popped in his face!", "hypotheses": ["Fred saw a child come over with a pin.", "Fred fill one balloon too small."]}
{"example_id": "126", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Samantha found some friends who gave her a ride. happen?", "a": "no", "prior_probs": [0.5468633476618596, 0.4531366523381404], "posterior_probs": [0.6306783119430895, 0.3693216880569105], "prior_entropy": 0.6887483802422558, "posterior_entropy": 0.6585936844799656, "delta_entropy": 0.03015469576229024, "eig_estimate": 0.03043867729739551, "pred": 0, "gold": 1, "confidence": 0.6306783119430895, "accuracy": 0, "tokens_in": 155, "tokens_out": 340, "tokens_total": 495, "latency_total": 0.2744694070006517, "latency_per_module": {"eig": [0.12720856100077071, 0.13539056199988408], "scorer": 0.0118702839999969}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 108, "tokens_out": 21, "tokens_total": 129}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Samantha found some friends who gave her a ride. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She had a flat tire and changed it. happen?\nAnswer:"], "scorer": "Observation: Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.\nQuestion: Did Samantha found some friends who gave her a ride. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Samantha's dad always taught her how to be self-sufficient. Samantha was very grateful to be able to get home safely.", "hypotheses": ["Samantha found some friends who gave her a ride.", "She had a flat tire and changed it."]}
{"example_id": "127", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did It was the right size. happen?", "a": "yes", "prior_probs": [0.4549968142922656, 0.5450031857077344], "posterior_probs": [0.40016588926648955, 0.5998341107335105], "prior_entropy": 0.6890911202623089, "posterior_entropy": 0.6730788719875, "delta_entropy": 0.01601224827480885, "eig_estimate": 0.015803663312116467, "pred": 1, "gold": 1, "confidence": 0.5998341107335105, "accuracy": 1, "tokens_in": 139, "tokens_out": 332, "tokens_total": 471, "latency_total": 0.2681711820005148, "latency_per_module": {"eig": [0.1467642440002237, 0.1150352370004839], "scorer": 0.006371700999807217}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}], "scorer": {"tokens_in": 98, "tokens_out": 13, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The tree fell on my fort. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did It was the right size. happen?\nAnswer:"], "scorer": "Observation: I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.\nQuestion: Did It was the right size. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I used my ruined treehouse to build a fort. The new fort is twice as good as the old treehouse.", "hypotheses": ["The tree fell on my fort.", "It was the right size."]}
{"example_id": "128", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Mike was having blood drawn because he needed a routine checkup. happen?", "a": "no", "prior_probs": [0.5035711515654095, 0.49642884843459045], "posterior_probs": [0.6479735181300184, 0.3520264818699817], "prior_entropy": 0.6931216740940784, "posterior_entropy": 0.648692093209366, "delta_entropy": 0.04442958088471238, "eig_estimate": 0.04455269629902832, "pred": 0, "gold": 1, "confidence": 0.6479735181300184, "accuracy": 0, "tokens_in": 154, "tokens_out": 342, "tokens_total": 496, "latency_total": 0.22861654900043504, "latency_per_module": {"eig": [0.10877737600003456, 0.11358446000031108], "scorer": 0.006254713000089396}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 104, "tokens_out": 22, "tokens_total": 126}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Mike was having blood drawn because he needed a routine checkup. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Mike complained of soreness in his kidneys. happen?\nAnswer:"], "scorer": "Observation: Mike had to go to the doctor. All the blood work came back clear and he was relieved.\nQuestion: Did Mike was having blood drawn because he needed a routine checkup. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike had to go to the doctor. All the blood work came back clear and he was relieved.", "hypotheses": ["Mike was having blood drawn because he needed a routine checkup.", "Mike complained of soreness in his kidneys."]}
{"example_id": "129", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Rocky was a rocket scientist, and he hated rockets. happen?", "a": "yes", "prior_probs": [0.5061941554108962, 0.49380584458910376], "posterior_probs": [0.39638095735405476, 0.6036190426459452], "prior_entropy": 0.6930704434725565, "posterior_entropy": 0.6715169573731603, "delta_entropy": 0.021553486099396224, "eig_estimate": 0.021026978971682084, "pred": 1, "gold": 1, "confidence": 0.6036190426459452, "accuracy": 1, "tokens_in": 145, "tokens_out": 341, "tokens_total": 486, "latency_total": 0.22504736300015793, "latency_per_module": {"eig": [0.11118247600006725, 0.10699536199990689], "scorer": 0.006869525000183785}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 98, "tokens_out": 21, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Rocky was very good at playing hockey. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Rocky was a rocket scientist, and he hated rockets. happen?\nAnswer:"], "scorer": "Observation: Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.\nQuestion: Did Rocky was a rocket scientist, and he hated rockets. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone thought it was funny that Rocky played hockey. Well, Rocket prefers the term Aerospace Engineer.", "hypotheses": ["Rocky was very good at playing hockey.", "Rocky was a rocket scientist, and he hated rockets."]}
{"example_id": "130", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did A girl came up to hug him one day. happen?", "a": "yes", "prior_probs": [0.4433643422955545, 0.5566356577044455], "posterior_probs": [0.32777346259434126, 0.6722265374056587], "prior_entropy": 0.686718195974517, "posterior_entropy": 0.632590611409606, "delta_entropy": 0.05412758456491096, "eig_estimate": 0.05412758456491096, "pred": 1, "gold": 1, "confidence": 0.6722265374056587, "accuracy": 1, "tokens_in": 123, "tokens_out": 336, "tokens_total": 459, "latency_total": 0.2154104820010616, "latency_per_module": {"eig": [0.10476989300059358, 0.10382399599984637], "scorer": 0.006816593000621651}, "tokens_per_module": {"eig": [{"tokens_in": 19, "tokens_out": 159, "tokens_total": 178}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 80, "tokens_out": 17, "tokens_total": 97}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Arnold saw a boy. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did A girl came up to hug him one day. happen?\nAnswer:"], "scorer": "Observation: Arnold was scared of girls. He nearly fainted.\nQuestion: Did A girl came up to hug him one day. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Arnold was scared of girls. He nearly fainted.", "hypotheses": ["Arnold saw a boy.", "A girl came up to hug him one day."]}
{"example_id": "131", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Bobby found a cat in the garden. happen?", "a": "no", "prior_probs": [0.5233937045763193, 0.4766062954236807], "posterior_probs": [0.5847974362098407, 0.41520256379015924], "prior_entropy": 0.6920522500476836, "posterior_entropy": 0.678696224851346, "delta_entropy": 0.013356025196337606, "eig_estimate": 0.013402084293778182, "pred": 0, "gold": 1, "confidence": 0.5847974362098407, "accuracy": 0, "tokens_in": 147, "tokens_out": 339, "tokens_total": 486, "latency_total": 0.22701973100083706, "latency_per_module": {"eig": [0.11024377000012464, 0.1101975660003518], "scorer": 0.006578395000360615}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Bobby found a cat in the garden. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Bobby begged his mom for a feral cat. happen?\nAnswer:"], "scorer": "Observation: Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.\nQuestion: Did Bobby found a cat in the garden. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Six year old Bobby wanted a cat very badly. Bobby was overjoyed when his mother said the cat could stay.", "hypotheses": ["Bobby found a cat in the garden.", "Bobby begged his mom for a feral cat."]}
{"example_id": "132", "dataset": "art", "method": "eig_ia", "asked": false, "q": "", "a": "", "prior_probs": [0.2999824166984342, 0.7000175833015658], "posterior_probs": [0.2999824166984342, 0.7000175833015658], "prior_entropy": 0.610849403022965, "posterior_entropy": 0.610849403022965, "delta_entropy": 0.0, "eig_estimate": 0.010075000714013993, "pred": 1, "gold": 1, "confidence": 0.7000175833015658, "accuracy": 1, "tokens_in": 122, "tokens_out": 344, "tokens_total": 466, "latency_total": 0.22494530900075915, "latency_per_module": {"eig": [0.11212027700003091, 0.10588425000059942], "scorer": 0.00694078200012882}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}], "scorer": {"tokens_in": 72, "tokens_out": 24, "tokens_total": 96}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Lucky looked for them in one store. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Lucy looked for the sandals everywhere, but could never find them. happen?\nAnswer:"], "scorer": "Observation: There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "There was a unique pair of louboutin sandals Lucy had to have. Finally by chance, she stumbled across a pair.", "hypotheses": ["Lucky looked for them in one store.", "Lucy looked for the sandals everywhere, but could never find them."]}
{"example_id": "133", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did We found one of them in the backyard. happen?", "a": "yes", "prior_probs": [0.4819782992719845, 0.5180217007280155], "posterior_probs": [0.39587947183927336, 0.6041205281607266], "prior_entropy": 0.6924974764463473, "posterior_entropy": 0.6713055232350339, "delta_entropy": 0.021191953211313397, "eig_estimate": 0.02069045479976255, "pred": 1, "gold": 1, "confidence": 0.6041205281607266, "accuracy": 1, "tokens_in": 151, "tokens_out": 335, "tokens_total": 486, "latency_total": 0.22769606899964856, "latency_per_module": {"eig": [0.1093771679998099, 0.11197543599973869], "scorer": 0.006343465000099968}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 108, "tokens_out": 15, "tokens_total": 123}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did We love to go fishing. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did We found one of them in the backyard. happen?\nAnswer:"], "scorer": "Observation: My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.\nQuestion: Did We found one of them in the backyard. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My dad and I like to hunt for worms. My dad and I decided to sell some worms and use the rest for fishing.", "hypotheses": ["We love to go fishing.", "We found one of them in the backyard."]}
{"example_id": "134", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did We got sick of this town. happen?", "a": "yes", "prior_probs": [0.49344076594076103, 0.5065592340592391], "posterior_probs": [0.4156647183729342, 0.5843352816270658], "prior_entropy": 0.6930611309868531, "posterior_entropy": 0.6788540725651089, "delta_entropy": 0.01420705842174419, "eig_estimate": 0.013867074798552537, "pred": 1, "gold": 1, "confidence": 0.5843352816270658, "accuracy": 1, "tokens_in": 130, "tokens_out": 333, "tokens_total": 463, "latency_total": 0.2147883860006914, "latency_per_module": {"eig": [0.10469983400071214, 0.10380612300014036], "scorer": 0.006282428999838885}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 88, "tokens_out": 14, "tokens_total": 102}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did We grew fond of this town. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did We got sick of this town. happen?\nAnswer:"], "scorer": "Observation: We decided to move to a new town next year. It will be a fun adventure.\nQuestion: Did We got sick of this town. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "We decided to move to a new town next year. It will be a fun adventure.", "hypotheses": ["We grew fond of this town.", "We got sick of this town."]}
{"example_id": "135", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ryan was mad at Kim for stealing his phone. happen?", "a": "no", "prior_probs": [0.4610257841579939, 0.5389742158420062], "posterior_probs": [0.602280342496119, 0.39771965750388105], "prior_entropy": 0.6901061176167884, "posterior_entropy": 0.672076227528758, "delta_entropy": 0.018029890088030465, "eig_estimate": 0.01797852062647152, "pred": 0, "gold": 1, "confidence": 0.602280342496119, "accuracy": 0, "tokens_in": 138, "tokens_out": 339, "tokens_total": 477, "latency_total": 0.2141153940001459, "latency_per_module": {"eig": [0.10293997900043905, 0.10470120599984512], "scorer": 0.00647420899986173}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 92, "tokens_out": 19, "tokens_total": 111}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ryan was mad at Kim for stealing his phone. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Heather kept the phone away from Ryan. happen?\nAnswer:"], "scorer": "Observation: Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.\nQuestion: Did Ryan was mad at Kim for stealing his phone. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Heather stole Ryan's phone. Ryan snatched his phone back and reported Heather to administration.", "hypotheses": ["Ryan was mad at Kim for stealing his phone.", "Heather kept the phone away from Ryan."]}
{"example_id": "136", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tracy spends his days scrubbing and cleaning. happen?", "a": "no", "prior_probs": [0.5327502600876581, 0.46724973991234187], "posterior_probs": [0.6044295915605569, 0.3955704084394431], "prior_entropy": 0.6910004849454078, "posterior_entropy": 0.6711746935665772, "delta_entropy": 0.01982579137883056, "eig_estimate": 0.020215129108348395, "pred": 0, "gold": 1, "confidence": 0.6044295915605569, "accuracy": 0, "tokens_in": 158, "tokens_out": 339, "tokens_total": 497, "latency_total": 0.255974499000331, "latency_per_module": {"eig": [0.12450619399987772, 0.12489527500019904], "scorer": 0.006573030000254221}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 112, "tokens_out": 19, "tokens_total": 131}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tracy spends his days scrubbing and cleaning. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I asked him how it is so clean. happen?\nAnswer:"], "scorer": "Observation: Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.\nQuestion: Did Tracy spends his days scrubbing and cleaning. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tracy E Warren keeps his house cleaner than any woman. Finally I just asked him and he said he has a maid so now I know.", "hypotheses": ["Tracy spends his days scrubbing and cleaning.", "I asked him how it is so clean."]}
{"example_id": "137", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did At first it was hard, but she persevered. happen?", "a": "yes", "prior_probs": [0.3580635427488348, 0.6419364572511652], "posterior_probs": [0.28964902593398345, 0.7103509740660164], "prior_entropy": 0.6522958825764628, "posterior_entropy": 0.6018371268070214, "delta_entropy": 0.050458755769441366, "eig_estimate": 0.050793891221464785, "pred": 1, "gold": 1, "confidence": 0.7103509740660164, "accuracy": 1, "tokens_in": 144, "tokens_out": 339, "tokens_total": 483, "latency_total": 0.23253982699952758, "latency_per_module": {"eig": [0.11522936399978789, 0.11053476400047657], "scorer": 0.006775698999263113}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 96, "tokens_out": 20, "tokens_total": 116}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Kya learned many new steak recipes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did At first it was hard, but she persevered. happen?\nAnswer:"], "scorer": "Observation: Kya was trying to be vegan. Before long, being vegan was effortless.\nQuestion: Did At first it was hard, but she persevered. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kya was trying to be vegan. Before long, being vegan was effortless.", "hypotheses": ["Kya learned many new steak recipes.", "At first it was hard, but she persevered."]}
{"example_id": "138", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The system was very expensive to have installed. happen?", "a": "no", "prior_probs": [0.5818840765131251, 0.4181159234868748], "posterior_probs": [0.6456029362016825, 0.3543970637983174], "prior_entropy": 0.6796765813872114, "posterior_entropy": 0.6501261825380267, "delta_entropy": 0.029550398849184756, "eig_estimate": 0.029810070814169354, "pred": 0, "gold": 1, "confidence": 0.6456029362016825, "accuracy": 0, "tokens_in": 152, "tokens_out": 341, "tokens_total": 493, "latency_total": 0.2223527380001542, "latency_per_module": {"eig": [0.1057475859997794, 0.10935322600016661], "scorer": 0.0072519260002081865}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}, {"tokens_in": 29, "tokens_out": 158, "tokens_total": 187}], "scorer": {"tokens_in": 100, "tokens_out": 24, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The system was very expensive to have installed. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did This month, my electric bill was double  what is used to be. happen?\nAnswer:"], "scorer": "Observation: Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.\nQuestion: Did The system was very expensive to have installed. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Today I bought a solar panel system for my house. I hope the system pays for itself within ten years.", "hypotheses": ["The system was very expensive to have installed.", "This month, my electric bill was double  what is used to be."]}
{"example_id": "139", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did We all had dinner at my tiny house. happen?", "a": "yes", "prior_probs": [0.5041482688845201, 0.4958517311154799], "posterior_probs": [0.4594548880022557, 0.5405451119977444], "prior_entropy": 0.6931127638936314, "posterior_entropy": 0.6898557555968071, "delta_entropy": 0.003257008296824293, "eig_estimate": 0.003144805266552386, "pred": 1, "gold": 1, "confidence": 0.5405451119977444, "accuracy": 1, "tokens_in": 146, "tokens_out": 338, "tokens_total": 484, "latency_total": 0.26454517999991367, "latency_per_module": {"eig": [0.12982657500015193, 0.12756460800028435], "scorer": 0.0071539969994773855}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 100, "tokens_out": 18, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did We all had dinner at my big house. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did We all had dinner at my tiny house. happen?\nAnswer:"], "scorer": "Observation: My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.\nQuestion: Did We all had dinner at my tiny house. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My parents live a few blocks from me. Somehow there was enough room and it was a lot of fun.", "hypotheses": ["We all had dinner at my big house.", "We all had dinner at my tiny house."]}
{"example_id": "140", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did there was a piece of steak on a plate. happen?", "a": "yes", "prior_probs": [0.52242454564882, 0.47757545435118004], "posterior_probs": [0.43497869131203637, 0.5650213086879636], "prior_entropy": 0.6921411226333666, "posterior_entropy": 0.684667644645748, "delta_entropy": 0.007473477987618549, "eig_estimate": 0.007734148470453773, "pred": 1, "gold": 1, "confidence": 0.5650213086879636, "accuracy": 1, "tokens_in": 149, "tokens_out": 339, "tokens_total": 488, "latency_total": 0.2682578370004194, "latency_per_module": {"eig": [0.11701587500010646, 0.1424750309997762], "scorer": 0.008766931000536715}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did There wasn't any food on a plate. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did there was a piece of steak on a plate. happen?\nAnswer:"], "scorer": "Observation: Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.\nQuestion: Did there was a piece of steak on a plate. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lilly sent a lovely table for her dinner party. She didn't tell anyone the cat had been licking it.", "hypotheses": ["There wasn't any food on a plate.", "there was a piece of steak on a plate."]}
{"example_id": "141", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Helen went to the store. happen?", "a": "no", "prior_probs": [0.5214472455295223, 0.47855275447047774], "posterior_probs": [0.570148339473299, 0.42985166052670093], "prior_entropy": 0.6922269295543793, "posterior_entropy": 0.6832730590304166, "delta_entropy": 0.008953870523962681, "eig_estimate": 0.009174676837454099, "pred": 0, "gold": 1, "confidence": 0.570148339473299, "accuracy": 0, "tokens_in": 125, "tokens_out": 332, "tokens_total": 457, "latency_total": 0.23273103300016373, "latency_per_module": {"eig": [0.11683207499936543, 0.10773999400043976], "scorer": 0.008158964000358537}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}, {"tokens_in": 19, "tokens_out": 159, "tokens_total": 178}], "scorer": {"tokens_in": 86, "tokens_out": 13, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Helen went to the store. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Helen went to sleep. happen?\nAnswer:"], "scorer": "Observation: Helen hung up the stocking on the railing. And someone had put presents in her stocking!\nQuestion: Did Helen went to the store. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Helen hung up the stocking on the railing. And someone had put presents in her stocking!", "hypotheses": ["Helen went to the store.", "Helen went to sleep."]}
{"example_id": "142", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tim realized it would barely make it in time for her birthday. happen?", "a": "yes", "prior_probs": [0.4818382331647717, 0.5181617668352283], "posterior_probs": [0.38699690053434277, 0.6130030994656572], "prior_entropy": 0.6924873358643442, "posterior_entropy": 0.6673857914594791, "delta_entropy": 0.025101544404865117, "eig_estimate": 0.02502134762489421, "pred": 1, "gold": 1, "confidence": 0.6130030994656572, "accuracy": 1, "tokens_in": 175, "tokens_out": 345, "tokens_total": 520, "latency_total": 0.25420955600020534, "latency_per_module": {"eig": [0.11999554300018644, 0.12003103200004261], "scorer": 0.01418298099997628}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 122, "tokens_out": 25, "tokens_total": 147}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tim forgot the gift in his car until after her birthday. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tim realized it would barely make it in time for her birthday. happen?\nAnswer:"], "scorer": "Observation: Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.\nQuestion: Did Tim realized it would barely make it in time for her birthday. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim wanted to mail a gift to his mom in a different state. Tim left an apology in her voicemail and promised to call her later.", "hypotheses": ["Tim forgot the gift in his car until after her birthday.", "Tim realized it would barely make it in time for her birthday."]}
{"example_id": "143", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did greg had not called the police right away. happen?", "a": "no", "prior_probs": [0.47161095172450157, 0.5283890482754985], "posterior_probs": [0.6602976390354764, 0.3397023609645235], "prior_entropy": 0.6915344372670527, "posterior_entropy": 0.6408378582092928, "delta_entropy": 0.050696579057759816, "eig_estimate": 0.050411541795961815, "pred": 0, "gold": 1, "confidence": 0.6602976390354764, "accuracy": 0, "tokens_in": 127, "tokens_out": 337, "tokens_total": 464, "latency_total": 0.27125777199944423, "latency_per_module": {"eig": [0.15262247800001205, 0.1116106489998856], "scorer": 0.007024644999546581}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 80, "tokens_out": 18, "tokens_total": 98}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did greg had not called the police right away. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Greg's lawyer made a compelling defense case. happen?\nAnswer:"], "scorer": "Observation: Greg was arrested for manslaughter. So Greg was convicted.\nQuestion: Did greg had not called the police right away. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Greg was arrested for manslaughter. So Greg was convicted.", "hypotheses": ["greg had not called the police right away.", "Greg's lawyer made a compelling defense case."]}
{"example_id": "144", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He made up a a rhyme that included his phone number. happen?", "a": "yes", "prior_probs": [0.44015820877619033, 0.5598417912238096], "posterior_probs": [0.30788121055288925, 0.6921187894471107], "prior_entropy": 0.6859679034141187, "posterior_entropy": 0.6173948730164754, "delta_entropy": 0.06857303039764329, "eig_estimate": 0.06922587966064228, "pred": 1, "gold": 1, "confidence": 0.6921187894471107, "accuracy": 1, "tokens_in": 141, "tokens_out": 339, "tokens_total": 480, "latency_total": 0.25791523899988533, "latency_per_module": {"eig": [0.11570860699976038, 0.12717305499973008], "scorer": 0.015033577000394871}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 158, "tokens_total": 180}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 92, "tokens_out": 21, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Joe could not have a pizza delivered. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He made up a a rhyme that included his phone number. happen?\nAnswer:"], "scorer": "Observation: Joe could not remember his address. After that he always remembered it.\nQuestion: Did He made up a a rhyme that included his phone number. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Joe could not remember his address. After that he always remembered it.", "hypotheses": ["Joe could not have a pizza delivered.", "He made up a a rhyme that included his phone number."]}
{"example_id": "145", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Nadia's friend gifted her the money for new shoes. happen?", "a": "no", "prior_probs": [0.5366545194477929, 0.46334548055220715], "posterior_probs": [0.6292983031262465, 0.37070169687375343], "prior_entropy": 0.6904576609280619, "posterior_entropy": 0.6593280796775458, "delta_entropy": 0.031129581250516125, "eig_estimate": 0.031374497430399884, "pred": 0, "gold": 1, "confidence": 0.6292983031262465, "accuracy": 0, "tokens_in": 146, "tokens_out": 338, "tokens_total": 484, "latency_total": 0.43580597399977705, "latency_per_module": {"eig": [0.16586761700000352, 0.2548732209997979], "scorer": 0.01506513599997561}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 22, "tokens_out": 158, "tokens_total": 180}], "scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Nadia's friend gifted her the money for new shoes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did A friend bought Nadia a pair. happen?\nAnswer:"], "scorer": "Observation: Nadia needed new ballet shoes. After her first dance show, she paid him back.\nQuestion: Did Nadia's friend gifted her the money for new shoes. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Nadia needed new ballet shoes. After her first dance show, she paid him back.", "hypotheses": ["Nadia's friend gifted her the money for new shoes.", "A friend bought Nadia a pair."]}
{"example_id": "146", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Matthew read the basketball rules and practiced the game. happen?", "a": "no", "prior_probs": [0.5012941331574995, 0.49870586684250057], "posterior_probs": [0.6492430463533558, 0.3507569536466442], "prior_entropy": 0.6931438309929469, "posterior_entropy": 0.6479139641429985, "delta_entropy": 0.04522986684994834, "eig_estimate": 0.04637852894015844, "pred": 0, "gold": 1, "confidence": 0.6492430463533558, "accuracy": 0, "tokens_in": 153, "tokens_out": 339, "tokens_total": 492, "latency_total": 0.28956031300003815, "latency_per_module": {"eig": [0.17317398300019704, 0.10922315900006652], "scorer": 0.007163170999774593}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 106, "tokens_out": 19, "tokens_total": 125}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Matthew read the basketball rules and practiced the game. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He practiced and tried out for a role. happen?\nAnswer:"], "scorer": "Observation: Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.\nQuestion: Did Matthew read the basketball rules and practiced the game. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Michael loved to play basketball, but wasn't very good. He made the team and went on to be a star.", "hypotheses": ["Matthew read the basketball rules and practiced the game.", "He practiced and tried out for a role."]}
{"example_id": "147", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Joseph could tell Mike was playing too over-confidently. happen?", "a": "yes", "prior_probs": [0.5013264386531003, 0.4986735613468997], "posterior_probs": [0.2982723504724022, 0.7017276495275979], "prior_entropy": 0.693143661674817, "posterior_entropy": 0.6093933538774118, "delta_entropy": 0.08375030779740522, "eig_estimate": 0.08413598699485594, "pred": 1, "gold": 1, "confidence": 0.7017276495275979, "accuracy": 1, "tokens_in": 146, "tokens_out": 339, "tokens_total": 485, "latency_total": 0.2986969399989903, "latency_per_module": {"eig": [0.17525403599938727, 0.11644952000006015], "scorer": 0.006993383999542857}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Mike entered a contest partnering with Joseph. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Joseph could tell Mike was playing too over-confidently. happen?\nAnswer:"], "scorer": "Observation: Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.\nQuestion: Did Joseph could tell Mike was playing too over-confidently. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mike was an incredible Chess player. Joseph won the chess game and Mike was humbled.", "hypotheses": ["Mike entered a contest partnering with Joseph.", "Joseph could tell Mike was playing too over-confidently."]}
{"example_id": "148", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?", "a": "no", "prior_probs": [0.5719734763411052, 0.42802652365889493], "posterior_probs": [0.7629772617452668, 0.2370227382547333], "prior_entropy": 0.6827507390451305, "posterior_entropy": 0.547623731750972, "delta_entropy": 0.1351270072941585, "eig_estimate": 0.1349147042710807, "pred": 0, "gold": 1, "confidence": 0.7629772617452668, "accuracy": 0, "tokens_in": 157, "tokens_out": 348, "tokens_total": 505, "latency_total": 0.3215658600001916, "latency_per_module": {"eig": [0.10958846900030039, 0.19912646299962944], "scorer": 0.012850928000261774}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 102, "tokens_out": 28, "tokens_total": 130}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Lucy shared supplies in art class with Lisa, they bonded. happen?\nAnswer:"], "scorer": "Observation: Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.\nQuestion: Did All flights were grounded to Lisa couldn't leave for a couple of days. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lisa was going home. The next day Lucy missed Lisa when she saw their drawings.", "hypotheses": ["All flights were grounded to Lisa couldn't leave for a couple of days.", "Lucy shared supplies in art class with Lisa, they bonded."]}
{"example_id": "149", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Martha worked hard to learn some recipies. happen?", "a": "yes", "prior_probs": [0.5042689001377829, 0.4957310998622171], "posterior_probs": [0.3598190824602085, 0.6401809175397916], "prior_entropy": 0.6931107330983648, "posterior_entropy": 0.6533140302897587, "delta_entropy": 0.039796702808606144, "eig_estimate": 0.039796702808606144, "pred": 1, "gold": 1, "confidence": 0.6401809175397916, "accuracy": 1, "tokens_in": 144, "tokens_out": 340, "tokens_total": 484, "latency_total": 0.28997010700004466, "latency_per_module": {"eig": [0.1337028999996619, 0.14941108000039094], "scorer": 0.0068561269999918295}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 98, "tokens_out": 20, "tokens_total": 118}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Martha's boyfriend enrolled himself in cooking classes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Martha worked hard to learn some recipies. happen?\nAnswer:"], "scorer": "Observation: Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.\nQuestion: Did Martha worked hard to learn some recipies. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Martha was a terrible cook and her boyfriend knew it. After a while, everything Martha cooked was delicious.", "hypotheses": ["Martha's boyfriend enrolled himself in cooking classes.", "Martha worked hard to learn some recipies."]}
{"example_id": "150", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Her husband did not expect Lucy to be home yet. happen?", "a": "no", "prior_probs": [0.5310802956494481, 0.4689197043505518], "posterior_probs": [0.6117678949259431, 0.388232105074057], "prior_entropy": 0.6912139649068965, "posterior_entropy": 0.6679507134155024, "delta_entropy": 0.023263251491394077, "eig_estimate": 0.024811594352415357, "pred": 0, "gold": 1, "confidence": 0.6117678949259431, "accuracy": 0, "tokens_in": 145, "tokens_out": 342, "tokens_total": 487, "latency_total": 0.2350319489996764, "latency_per_module": {"eig": [0.10936539300018921, 0.11907468000026711], "scorer": 0.006591875999220065}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 159, "tokens_total": 184}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 94, "tokens_out": 23, "tokens_total": 117}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Her husband did not expect Lucy to be home yet. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did she couldn't wait to tell him she wanted a divorce. happen?\nAnswer:"], "scorer": "Observation: Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.\nQuestion: Did Her husband did not expect Lucy to be home yet. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lucy was at home waiting for her husband. Lucy's husband was excited and thrill.", "hypotheses": ["Her husband did not expect Lucy to be home yet.", "she couldn't wait to tell him she wanted a divorce."]}
{"example_id": "151", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Kelsi saw Lucy was reading the same book she was. happen?", "a": "yes", "prior_probs": [0.5187758093381991, 0.4812241906618008], "posterior_probs": [0.38211380344323176, 0.6178861965567684], "prior_entropy": 0.692441952727644, "posterior_entropy": 0.6650894595320398, "delta_entropy": 0.027352493195604177, "eig_estimate": 0.02717840718228595, "pred": 1, "gold": 1, "confidence": 0.6178861965567684, "accuracy": 1, "tokens_in": 168, "tokens_out": 342, "tokens_total": 510, "latency_total": 0.27321298099923297, "latency_per_module": {"eig": [0.1110666869999477, 0.1549453909992735], "scorer": 0.007200903000011749}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 118, "tokens_out": 22, "tokens_total": 140}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Kelsi and Thomas met at school. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Kelsi saw Lucy was reading the same book she was. happen?\nAnswer:"], "scorer": "Observation: Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.\nQuestion: Did Kelsi saw Lucy was reading the same book she was. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelsi was new at school and wanted to make new friends. Lucy was happy she was able to have a best friend finally.", "hypotheses": ["Kelsi and Thomas met at school.", "Kelsi saw Lucy was reading the same book she was."]}
{"example_id": "152", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did i had enough money to spend on food and extra stuff. happen?", "a": "no", "prior_probs": [0.5489156641466169, 0.45108433585338314], "posterior_probs": [0.6618535661213454, 0.33814643387865456], "prior_entropy": 0.6883540331646313, "posterior_entropy": 0.6397983560963837, "delta_entropy": 0.048555677068247594, "eig_estimate": 0.04802161823897964, "pred": 0, "gold": 1, "confidence": 0.6618535661213454, "accuracy": 0, "tokens_in": 150, "tokens_out": 337, "tokens_total": 487, "latency_total": 0.22319086300103663, "latency_per_module": {"eig": [0.11109804200077633, 0.10542674200041802], "scorer": 0.006666078999842284}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}, {"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}], "scorer": {"tokens_in": 104, "tokens_out": 18, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did i had enough money to spend on food and extra stuff. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I bought my dream cat. happen?\nAnswer:"], "scorer": "Observation: I saved up money for a long time. I took the boat out on the lake and felt happy.\nQuestion: Did i had enough money to spend on food and extra stuff. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I saved up money for a long time. I took the boat out on the lake and felt happy.", "hypotheses": ["i had enough money to spend on food and extra stuff.", "I bought my dream cat."]}
{"example_id": "153", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jake immediately threw his new toy in the trash. happen?", "a": "no", "prior_probs": [0.5717033204360721, 0.428296679563928], "posterior_probs": [0.6419857770755857, 0.35801422292441437], "prior_entropy": 0.6828289102070211, "posterior_entropy": 0.6522670854142874, "delta_entropy": 0.030561824792733727, "eig_estimate": 0.03072021987224698, "pred": 0, "gold": 1, "confidence": 0.6419857770755857, "accuracy": 0, "tokens_in": 158, "tokens_out": 339, "tokens_total": 497, "latency_total": 0.22541621300115366, "latency_per_module": {"eig": [0.11322739700062812, 0.10561881800003903], "scorer": 0.006569998000486521}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 161, "tokens_total": 185}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 112, "tokens_out": 18, "tokens_total": 130}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jake immediately threw his new toy in the trash. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jake decided Dan was the Green Goblin. happen?\nAnswer:"], "scorer": "Observation: Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string\nQuestion: Did Jake immediately threw his new toy in the trash. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jake had a new toy Spiderman silly string web slinger. Dan's mother posted photo's on Facebook of Dan covered in silly string", "hypotheses": ["Jake immediately threw his new toy in the trash.", "Jake decided Dan was the Green Goblin."]}
{"example_id": "154", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Margo left the show. happen?", "a": "yes", "prior_probs": [0.46568290051530103, 0.534317099484699], "posterior_probs": [0.4416966407980646, 0.5583033592019353], "prior_entropy": 0.6907900012428367, "posterior_entropy": 0.686333125937109, "delta_entropy": 0.004456875305727692, "eig_estimate": 0.004115340799034551, "pred": 1, "gold": 1, "confidence": 0.5583033592019353, "accuracy": 1, "tokens_in": 127, "tokens_out": 333, "tokens_total": 460, "latency_total": 0.23274920399944676, "latency_per_module": {"eig": [0.10623463100000663, 0.11563880299945595], "scorer": 0.010875769999984186}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}], "scorer": {"tokens_in": 86, "tokens_out": 13, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Margo sat down to watch. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Margo left the show. happen?\nAnswer:"], "scorer": "Observation: Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.\nQuestion: Did Margo left the show. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Margo was going to the Rocky Horror Picture Show. Eventually, the show was over.", "hypotheses": ["Margo sat down to watch.", "Margo left the show."]}
{"example_id": "155", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Barry went to the bar for a game. happen?", "a": "yes", "prior_probs": [0.4269841303580189, 0.573015869641981], "posterior_probs": [0.3740422133919617, 0.6259577866080384], "prior_entropy": 0.6824463218505652, "posterior_entropy": 0.6610720185220218, "delta_entropy": 0.021374303328543398, "eig_estimate": 0.021055959524105196, "pred": 1, "gold": 1, "confidence": 0.6259577866080384, "accuracy": 1, "tokens_in": 125, "tokens_out": 338, "tokens_total": 463, "latency_total": 0.3178332789993874, "latency_per_module": {"eig": [0.16230728299979091, 0.1463694869999017], "scorer": 0.009156508999694779}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 80, "tokens_out": 19, "tokens_total": 99}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Barry's team won the game today. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Barry went to the bar for a game. happen?\nAnswer:"], "scorer": "Observation: Barry loves playing baseball. Barry also bought a hot dog.\nQuestion: Did Barry went to the bar for a game. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Barry loves playing baseball. Barry also bought a hot dog.", "hypotheses": ["Barry's team won the game today.", "Barry went to the bar for a game."]}
{"example_id": "156", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Erina took too many breaks the first day. happen?", "a": "yes", "prior_probs": [0.49943053747004224, 0.5005694625299577], "posterior_probs": [0.39996863929525167, 0.6000313607047484], "prior_entropy": 0.6931465319826591, "posterior_entropy": 0.6729989492867521, "delta_entropy": 0.020147582695906996, "eig_estimate": 0.020169705392721338, "pred": 1, "gold": 1, "confidence": 0.6000313607047484, "accuracy": 1, "tokens_in": 150, "tokens_out": 340, "tokens_total": 490, "latency_total": 0.2595088300004136, "latency_per_module": {"eig": [0.12190052200003265, 0.12715401900004508], "scorer": 0.010454289000335848}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Erina gave it her all and did well. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Erina took too many breaks the first day. happen?\nAnswer:"], "scorer": "Observation: Erina's first day at her new job was today. Her new boss complimented her on her performance.\nQuestion: Did Erina took too many breaks the first day. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Erina's first day at her new job was today. Her new boss complimented her on her performance.", "hypotheses": ["Erina gave it her all and did well.", "Erina took too many breaks the first day."]}
{"example_id": "157", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Penny dropped her game on accident. happen?", "a": "no", "prior_probs": [0.5264368049007686, 0.4735631950992314], "posterior_probs": [0.5944174596215034, 0.40558254037849656], "prior_entropy": 0.6917487192315404, "posterior_entropy": 0.6752103648191494, "delta_entropy": 0.016538354412390932, "eig_estimate": 0.016676502910435563, "pred": 0, "gold": 1, "confidence": 0.5944174596215034, "accuracy": 0, "tokens_in": 146, "tokens_out": 334, "tokens_total": 480, "latency_total": 0.556387249999716, "latency_per_module": {"eig": [0.2590686219991767, 0.27981018900027266], "scorer": 0.017508439000266662}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 104, "tokens_out": 15, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Penny dropped her game on accident. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She accidentally sold her game console. happen?\nAnswer:"], "scorer": "Observation: Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.\nQuestion: Did Penny dropped her game on accident. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Penny was five Year's old and loved to play video games. The screen cracked and she was unable to play for a month.", "hypotheses": ["Penny dropped her game on accident.", "She accidentally sold her game console."]}
{"example_id": "158", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?", "a": "no", "prior_probs": [0.5403909182322422, 0.45960908176775783], "posterior_probs": [0.6379185995365564, 0.36208140046344356], "prior_entropy": 0.6898807699753513, "posterior_entropy": 0.6546063643409491, "delta_entropy": 0.035274405634402206, "eig_estimate": 0.03570446375952674, "pred": 0, "gold": 1, "confidence": 0.6379185995365564, "accuracy": 0, "tokens_in": 161, "tokens_out": 346, "tokens_total": 507, "latency_total": 0.5486808999994537, "latency_per_module": {"eig": [0.2937808369997583, 0.24026094100008777], "scorer": 0.014639121999607596}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}, {"tokens_in": 26, "tokens_out": 161, "tokens_total": 187}], "scorer": {"tokens_in": 108, "tokens_out": 25, "tokens_total": 133}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Amy was called back to work and her frozen yogurt melted. happen?\nAnswer:"], "scorer": "Observation: Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.\nQuestion: Did Amy robbed the frozen yogurt store and ate all of the yogurt. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amy went to get frozen yogurt on her break. Amy ended up throwing most of the yogurt in the trash.", "hypotheses": ["Amy robbed the frozen yogurt store and ate all of the yogurt.", "Amy was called back to work and her frozen yogurt melted."]}
{"example_id": "159", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ellen was told there was a dress code. happen?", "a": "yes", "prior_probs": [0.5027835081552867, 0.4972164918447132], "posterior_probs": [0.3983186708989878, 0.6016813291010122], "prior_entropy": 0.693131684642603, "posterior_entropy": 0.6723240546501348, "delta_entropy": 0.020807629992468213, "eig_estimate": 0.02063397884452811, "pred": 1, "gold": 1, "confidence": 0.6016813291010122, "accuracy": 1, "tokens_in": 151, "tokens_out": 333, "tokens_total": 484, "latency_total": 0.5251450340001611, "latency_per_module": {"eig": [0.28199157800008834, 0.23066438599926187], "scorer": 0.012489070000810898}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 156, "tokens_total": 176}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 108, "tokens_out": 17, "tokens_total": 125}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ellen realized that she could. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ellen was told there was a dress code. happen?\nAnswer:"], "scorer": "Observation: Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.\nQuestion: Did Ellen was told there was a dress code. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Ellen wanted to know if she could wear leggings while tap dancing. Ellen decided to wear whatever she wanted in the future.", "hypotheses": ["Ellen realized that she could.", "Ellen was told there was a dress code."]}
{"example_id": "160", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Lily decided to make her costume a bear costume. happen?", "a": "no", "prior_probs": [0.6198132078037958, 0.3801867921962042], "posterior_probs": [0.6630818066285097, 0.3369181933714903], "prior_entropy": 0.664155496307155, "posterior_entropy": 0.6389701400853475, "delta_entropy": 0.02518535622180751, "eig_estimate": 0.025126085935870957, "pred": 0, "gold": 1, "confidence": 0.6630818066285097, "accuracy": 0, "tokens_in": 133, "tokens_out": 338, "tokens_total": 471, "latency_total": 0.4712541609987966, "latency_per_module": {"eig": [0.2317093539995767, 0.22311527999954706], "scorer": 0.016429526999672817}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 88, "tokens_out": 18, "tokens_total": 106}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Lily decided to make her costume a bear costume. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did All the costumes were gone though. happen?\nAnswer:"], "scorer": "Observation: Lily wanted a new Halloween costume. She ended up making a rabbit costume.\nQuestion: Did Lily decided to make her costume a bear costume. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Lily wanted a new Halloween costume. She ended up making a rabbit costume.", "hypotheses": ["Lily decided to make her costume a bear costume.", "All the costumes were gone though."]}
{"example_id": "161", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Mia entered the spelling bee but didn't practice. happen?", "a": "yes", "prior_probs": [0.464782121161704, 0.5352178788382961], "posterior_probs": [0.3648656790412365, 0.6351343209587635], "prior_entropy": 0.6906645273748359, "posterior_entropy": 0.6561664544737993, "delta_entropy": 0.03449807290103657, "eig_estimate": 0.03435054291693465, "pred": 1, "gold": 1, "confidence": 0.6351343209587635, "accuracy": 1, "tokens_in": 155, "tokens_out": 340, "tokens_total": 495, "latency_total": 0.5946210249985597, "latency_per_module": {"eig": [0.2594025319995126, 0.32331508999959624], "scorer": 0.011903402999450918}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 108, "tokens_out": 20, "tokens_total": 128}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She studied hard because she wanted to spell. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Mia entered the spelling bee but didn't practice. happen?\nAnswer:"], "scorer": "Observation: Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.\nQuestion: Did Mia entered the spelling bee but didn't practice. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Once there was a girl named Mia who could spell well. Mia won the spelling bee and felt more sure of herself afterwards.", "hypotheses": ["She studied hard because she wanted to spell.", "Mia entered the spelling bee but didn't practice."]}
{"example_id": "162", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did bus showed up early and I was late for class. happen?", "a": "yes", "prior_probs": [0.49817956298321636, 0.5018204370167837], "posterior_probs": [0.3826755227339434, 0.6173244772660567], "prior_entropy": 0.6931405525614377, "posterior_entropy": 0.665358745777568, "delta_entropy": 0.027781806783869656, "eig_estimate": 0.02811767559668337, "pred": 1, "gold": 1, "confidence": 0.6173244772660567, "accuracy": 1, "tokens_in": 158, "tokens_out": 341, "tokens_total": 499, "latency_total": 0.46193840599880787, "latency_per_module": {"eig": [0.2077992959993935, 0.24189694399956352], "scorer": 0.01224216599985084}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 159, "tokens_total": 184}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 108, "tokens_out": 22, "tokens_total": 130}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The bus was late and so was I to school. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did bus showed up early and I was late for class. happen?\nAnswer:"], "scorer": "Observation: Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.\nQuestion: Did bus showed up early and I was late for class. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Everyone at my bus stop were waiting for the bus. My teacher told me to stand outside since I came too late.", "hypotheses": ["The bus was late and so was I to school.", "bus showed up early and I was late for class."]}
{"example_id": "163", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jean ended up having a bad time in Africa. happen?", "a": "yes", "prior_probs": [0.37469656598213685, 0.6253034340178631], "posterior_probs": [0.349298544895746, 0.6507014551042539], "prior_entropy": 0.6614080398426275, "posterior_entropy": 0.6470113290812122, "delta_entropy": 0.014396710761415354, "eig_estimate": 0.014396710761415354, "pred": 1, "gold": 1, "confidence": 0.6507014551042539, "accuracy": 1, "tokens_in": 141, "tokens_out": 337, "tokens_total": 478, "latency_total": 0.3220007899999473, "latency_per_module": {"eig": [0.16961258399987855, 0.14403312400008872], "scorer": 0.00835508199998003}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 96, "tokens_out": 17, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jean booked her trip and went. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jean ended up having a bad time in Africa. happen?\nAnswer:"], "scorer": "Observation: Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.\nQuestion: Did Jean ended up having a bad time in Africa. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jean wanted to travel to Africa. Jean's trip to Africa was better than she had hoped.", "hypotheses": ["Jean booked her trip and went.", "Jean ended up having a bad time in Africa."]}
{"example_id": "164", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The rice Mary put on the stove out over and burned on the stove. happen?", "a": "yes", "prior_probs": [0.5138874089170941, 0.486112591082906], "posterior_probs": [0.3034950800408661, 0.696504919959134], "prior_entropy": 0.6927614106964033, "posterior_entropy": 0.6137966552165628, "delta_entropy": 0.0789647554798405, "eig_estimate": 0.07886804804749703, "pred": 1, "gold": 1, "confidence": 0.696504919959134, "accuracy": 1, "tokens_in": 150, "tokens_out": 343, "tokens_total": 493, "latency_total": 0.23089440200055833, "latency_per_module": {"eig": [0.11393679699995118, 0.10967517600056453], "scorer": 0.007282429000042612}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}, {"tokens_in": 29, "tokens_out": 159, "tokens_total": 188}], "scorer": {"tokens_in": 98, "tokens_out": 24, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Mary ended up overcooking the pasta. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The rice Mary put on the stove out over and burned on the stove. happen?\nAnswer:"], "scorer": "Observation: Mary had never made rice before. She resolved to read directions next time!\nQuestion: Did The rice Mary put on the stove out over and burned on the stove. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Mary had never made rice before. She resolved to read directions next time!", "hypotheses": ["Mary ended up overcooking the pasta.", "The rice Mary put on the stove out over and burned on the stove."]}
{"example_id": "165", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did He was in such a slow mood, he didn't dress correctly. happen?", "a": "yes", "prior_probs": [0.45114413817439564, 0.5488558618256044], "posterior_probs": [0.3193603533066797, 0.6806396466933202], "prior_entropy": 0.6883657645700085, "posterior_entropy": 0.6263863694597975, "delta_entropy": 0.061979395110211, "eig_estimate": 0.06198202263981023, "pred": 1, "gold": 1, "confidence": 0.6806396466933202, "accuracy": 1, "tokens_in": 153, "tokens_out": 339, "tokens_total": 492, "latency_total": 0.22843148700030724, "latency_per_module": {"eig": [0.10938194099981047, 0.11187739600063651], "scorer": 0.007172149999860267}, "tokens_per_module": {"eig": [{"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}, {"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}], "scorer": {"tokens_in": 106, "tokens_out": 19, "tokens_total": 125}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Brad rushed to work. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He was in such a slow mood, he didn't dress correctly. happen?\nAnswer:"], "scorer": "Observation: Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.\nQuestion: Did He was in such a slow mood, he didn't dress correctly. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Brad woke up late this morning. He had worn 2 different pairs of shoes to work this morning.", "hypotheses": ["Brad rushed to work.", "He was in such a slow mood, he didn't dress correctly."]}
{"example_id": "166", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I left all my stuff out on the bed. happen?", "a": "no", "prior_probs": [0.5975131689009477, 0.40248683109905226], "posterior_probs": [0.6641824667828291, 0.33581753321717095], "prior_entropy": 0.6740071150648055, "posterior_entropy": 0.6382222162787503, "delta_entropy": 0.035784898786055175, "eig_estimate": 0.03596751774322384, "pred": 0, "gold": 1, "confidence": 0.6641824667828291, "accuracy": 0, "tokens_in": 148, "tokens_out": 335, "tokens_total": 483, "latency_total": 0.2476775760005694, "latency_per_module": {"eig": [0.12241372500011494, 0.11780095700032689], "scorer": 0.007462894000127562}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}], "scorer": {"tokens_in": 104, "tokens_out": 16, "tokens_total": 120}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I left all my stuff out on the bed. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I heard loud fire alarm. happen?\nAnswer:"], "scorer": "Observation: I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.\nQuestion: Did I left all my stuff out on the bed. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was getting ready to leave my hotel room for the day. I was relieved that it was only a drill.", "hypotheses": ["I left all my stuff out on the bed.", "I heard loud fire alarm."]}
{"example_id": "167", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My shoes made a really loud sound in front of my boss. happen?", "a": "no", "prior_probs": [0.5050753995895266, 0.49492460041047337], "posterior_probs": [0.6468762456464059, 0.35312375435359417], "prior_entropy": 0.693095660311174, "posterior_entropy": 0.6493589488836923, "delta_entropy": 0.0437367114274817, "eig_estimate": 0.04411053021678084, "pred": 0, "gold": 1, "confidence": 0.6468762456464059, "accuracy": 0, "tokens_in": 156, "tokens_out": 339, "tokens_total": 495, "latency_total": 0.23545463199934602, "latency_per_module": {"eig": [0.11786005799967825, 0.11136893900038558], "scorer": 0.0062256349992821924}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}, {"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}], "scorer": {"tokens_in": 108, "tokens_out": 20, "tokens_total": 128}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did My shoes made a really loud sound in front of my boss. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I fell and broke my leg. happen?\nAnswer:"], "scorer": "Observation: For a lark I started dragging my foot behind me at work. He told me to knock it off.\nQuestion: Did My shoes made a really loud sound in front of my boss. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "For a lark I started dragging my foot behind me at work. He told me to knock it off.", "hypotheses": ["My shoes made a really loud sound in front of my boss.", "I fell and broke my leg."]}
{"example_id": "168", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did However, he was not very talented, and his hair did not sell very well. happen?", "a": "no", "prior_probs": [0.6146910714067012, 0.3853089285932988], "posterior_probs": [0.7134848862873541, 0.286515113712646], "prior_entropy": 0.666603394708587, "posterior_entropy": 0.5990018094877949, "delta_entropy": 0.06760158522079207, "eig_estimate": 0.06909257868323522, "pred": 0, "gold": 1, "confidence": 0.7134848862873541, "accuracy": 0, "tokens_in": 162, "tokens_out": 350, "tokens_total": 512, "latency_total": 0.22095731900026294, "latency_per_module": {"eig": [0.10750232600003073, 0.10664263099988602], "scorer": 0.006812362000346184}, "tokens_per_module": {"eig": [{"tokens_in": 31, "tokens_out": 160, "tokens_total": 191}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 104, "tokens_out": 30, "tokens_total": 134}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did However, he was not very talented, and his hair did not sell very well. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tony applies for a lot of art gallery jobs and got rejected. happen?\nAnswer:"], "scorer": "Observation: Tony liked art. Tony then went back to school and found a different major.\nQuestion: Did However, he was not very talented, and his hair did not sell very well. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tony liked art. Tony then went back to school and found a different major.", "hypotheses": ["However, he was not very talented, and his hair did not sell very well.", "Tony applies for a lot of art gallery jobs and got rejected."]}
{"example_id": "169", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I returned the empty envelope to the person it was addressed to. happen?", "a": "yes", "prior_probs": [0.4112140779915971, 0.5887859220084029], "posterior_probs": [0.3200503300324284, 0.6799496699675716], "prior_entropy": 0.6772973833177964, "posterior_entropy": 0.6269073891092749, "delta_entropy": 0.05038999420852153, "eig_estimate": 0.04940652552564704, "pred": 1, "gold": 1, "confidence": 0.6799496699675716, "accuracy": 1, "tokens_in": 147, "tokens_out": 342, "tokens_total": 489, "latency_total": 0.258316222000758, "latency_per_module": {"eig": [0.12436655500005145, 0.1269049870006711], "scorer": 0.0070446800000354415}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 159, "tokens_total": 183}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 96, "tokens_out": 23, "tokens_total": 119}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did I turned the $600 dollars to the authorities. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I returned the empty envelope to the person it was addressed to. happen?\nAnswer:"], "scorer": "Observation: I found a $600 dollar envelope in the mail today. I am honest.\nQuestion: Did I returned the empty envelope to the person it was addressed to. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I found a $600 dollar envelope in the mail today. I am honest.", "hypotheses": ["I turned the $600 dollars to the authorities.", "I returned the empty envelope to the person it was addressed to."]}
{"example_id": "170", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Dominick moved his house away from Becky's house. happen?", "a": "no", "prior_probs": [0.6687183408185025, 0.3312816591814976], "posterior_probs": [0.684078457676746, 0.31592154232325403], "prior_entropy": 0.6350825753029137, "posterior_entropy": 0.6237569233425315, "delta_entropy": 0.011325651960382155, "eig_estimate": 0.011925132997698061, "pred": 0, "gold": 1, "confidence": 0.684078457676746, "accuracy": 0, "tokens_in": 148, "tokens_out": 341, "tokens_total": 489, "latency_total": 0.22359286999926553, "latency_per_module": {"eig": [0.10997439899983874, 0.10728738799934945], "scorer": 0.006331083000077342}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 159, "tokens_total": 184}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 98, "tokens_out": 22, "tokens_total": 120}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Dominick moved his house away from Becky's house. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Dominick shifted their house near the Becky house so. happen?\nAnswer:"], "scorer": "Observation: Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.\nQuestion: Did Dominick moved his house away from Becky's house. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Becky has a nephew named Dominick. Becky and Dominick can now play Uno together.", "hypotheses": ["Dominick moved his house away from Becky's house.", "Dominick shifted their house near the Becky house so."]}
{"example_id": "171", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tom was unable to find work being in a wheelchair. happen?", "a": "yes", "prior_probs": [0.3802296600706219, 0.619770339929378], "posterior_probs": [0.28695952264025276, 0.7130404773597473], "prior_entropy": 0.6641764443158823, "posterior_entropy": 0.5994067919330996, "delta_entropy": 0.06476965238278265, "eig_estimate": 0.06339128769342513, "pred": 1, "gold": 1, "confidence": 0.7130404773597473, "accuracy": 1, "tokens_in": 141, "tokens_out": 338, "tokens_total": 479, "latency_total": 0.2467443679997814, "latency_per_module": {"eig": [0.11671141799979523, 0.1225490480001099], "scorer": 0.007483901999876252}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 94, "tokens_out": 19, "tokens_total": 113}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tom developed emotional problems affecting his division. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tom was unable to find work being in a wheelchair. happen?\nAnswer:"], "scorer": "Observation: Tom was accidentally shot by his teammate in the army. He ends up being homeless.\nQuestion: Did Tom was unable to find work being in a wheelchair. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tom was accidentally shot by his teammate in the army. He ends up being homeless.", "hypotheses": ["Tom developed emotional problems affecting his division.", "Tom was unable to find work being in a wheelchair."]}
{"example_id": "172", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tommy's friends didn't pay attention to him. happen?", "a": "yes", "prior_probs": [0.4065824365907642, 0.5934175634092357], "posterior_probs": [0.37690013254883376, 0.6230998674511662], "prior_entropy": 0.6755905096228538, "posterior_entropy": 0.6625261773213507, "delta_entropy": 0.013064332301503168, "eig_estimate": 0.012694187886507714, "pred": 1, "gold": 1, "confidence": 0.6230998674511662, "accuracy": 1, "tokens_in": 127, "tokens_out": 342, "tokens_total": 469, "latency_total": 0.22160572800021328, "latency_per_module": {"eig": [0.1078079090002575, 0.1071840369995698], "scorer": 0.0066137820003859815}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 161, "tokens_total": 182}, {"tokens_in": 24, "tokens_out": 162, "tokens_total": 186}], "scorer": {"tokens_in": 82, "tokens_out": 19, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tommy's friends cheered him up. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tommy's friends didn't pay attention to him. happen?\nAnswer:"], "scorer": "Observation: Tommy was having a bad day. Tommy had good friends.\nQuestion: Did Tommy's friends didn't pay attention to him. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tommy was having a bad day. Tommy had good friends.", "hypotheses": ["Tommy's friends cheered him up.", "Tommy's friends didn't pay attention to him."]}
{"example_id": "173", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jay wasn't sure how to vote. happen?", "a": "no", "prior_probs": [0.5449985742051293, 0.45500142579487063], "posterior_probs": [0.5733131693796836, 0.4266868306203164], "prior_entropy": 0.689091952601288, "posterior_entropy": 0.6823586855711019, "delta_entropy": 0.006733267030186063, "eig_estimate": 0.006815414084238603, "pred": 0, "gold": 1, "confidence": 0.5733131693796836, "accuracy": 0, "tokens_in": 132, "tokens_out": 335, "tokens_total": 467, "latency_total": 0.23425468999994337, "latency_per_module": {"eig": [0.11516339599984349, 0.10975369300012972], "scorer": 0.009337600999970164}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 88, "tokens_out": 16, "tokens_total": 104}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jay wasn't sure how to vote. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jay swore he'd never vote again. happen?\nAnswer:"], "scorer": "Observation: Jay wanted to vote. That is, until he got older and did it again.\nQuestion: Did Jay wasn't sure how to vote. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jay wanted to vote. That is, until he got older and did it again.", "hypotheses": ["Jay wasn't sure how to vote.", "Jay swore he'd never vote again."]}
{"example_id": "174", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did I threw up on my date. happen?", "a": "yes", "prior_probs": [0.484791977761282, 0.515208022238718], "posterior_probs": [0.4456813772272215, 0.5543186227727785], "prior_entropy": 0.6926845413276974, "posterior_entropy": 0.6872344924852158, "delta_entropy": 0.00545004884248157, "eig_estimate": 0.005382192899078353, "pred": 1, "gold": 1, "confidence": 0.5543186227727785, "accuracy": 1, "tokens_in": 136, "tokens_out": 334, "tokens_total": 470, "latency_total": 0.39006448300096963, "latency_per_module": {"eig": [0.16802182600076776, 0.21201605500027654], "scorer": 0.010026601999925333}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 94, "tokens_out": 14, "tokens_total": 108}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did My date threw up on me. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did I threw up on my date. happen?\nAnswer:"], "scorer": "Observation: I was really nervous before my first middle school dance. Now, she won't even talk to me.\nQuestion: Did I threw up on my date. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I was really nervous before my first middle school dance. Now, she won't even talk to me.", "hypotheses": ["My date threw up on me.", "I threw up on my date."]}
{"example_id": "175", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?", "a": "no", "prior_probs": [0.4577060736616809, 0.5422939263383191], "posterior_probs": [0.7175104258259337, 0.2824895741740662], "prior_entropy": 0.6895653495966495, "posterior_entropy": 0.5952892803901451, "delta_entropy": 0.09427606920650444, "eig_estimate": 0.09390283453708843, "pred": 0, "gold": 1, "confidence": 0.7175104258259337, "accuracy": 0, "tokens_in": 184, "tokens_out": 350, "tokens_total": 534, "latency_total": 0.8163325600007738, "latency_per_module": {"eig": [0.5237648399997852, 0.2793861320005817], "scorer": 0.013181588000406919}, "tokens_per_module": {"eig": [{"tokens_in": 35, "tokens_out": 158, "tokens_total": 193}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 124, "tokens_out": 32, "tokens_total": 156}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did He would jump on our ear to get our attention. happen?\nAnswer:"], "scorer": "Observation: My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.\nQuestion: Did Today I was ready for him. When he came into our room a jumped out and tickled him. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "My four-year-old nephew loves to wake us up. As I screamed, he yelled cold hands.", "hypotheses": ["Today I was ready for him. When he came into our room a jumped out and tickled him.", "He would jump on our ear to get our attention."]}
{"example_id": "176", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did My husband was sad so I thought I would cheer her up. happen?", "a": "yes", "prior_probs": [0.39948043219530643, 0.6005195678046935], "posterior_probs": [0.3362910873785936, 0.6637089126214064], "prior_entropy": 0.6728004379125668, "posterior_entropy": 0.6385446723917367, "delta_entropy": 0.034255765520830095, "eig_estimate": 0.03401970738842637, "pred": 1, "gold": 1, "confidence": 0.6637089126214064, "accuracy": 1, "tokens_in": 149, "tokens_out": 339, "tokens_total": 488, "latency_total": 0.5336978129989802, "latency_per_module": {"eig": [0.2289606949998415, 0.2912281049993908], "scorer": 0.013509012999747938}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}, {"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}], "scorer": {"tokens_in": 102, "tokens_out": 19, "tokens_total": 121}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The my wife became sad. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did My husband was sad so I thought I would cheer her up. happen?\nAnswer:"], "scorer": "Observation: I loved to make my wife laugh. I started making her laugh again and she became happy!\nQuestion: Did My husband was sad so I thought I would cheer her up. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I loved to make my wife laugh. I started making her laugh again and she became happy!", "hypotheses": ["The my wife became sad.", "My husband was sad so I thought I would cheer her up."]}
{"example_id": "177", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She cut her finger when chopping vegetables. happen?", "a": "yes", "prior_probs": [0.5202507998250527, 0.4797492001749473], "posterior_probs": [0.37297043624525356, 0.6270295637547465], "prior_entropy": 0.692326766386478, "posterior_entropy": 0.6605176911357957, "delta_entropy": 0.03180907525068233, "eig_estimate": 0.03199558205065247, "pred": 1, "gold": 1, "confidence": 0.6270295637547465, "accuracy": 1, "tokens_in": 129, "tokens_out": 333, "tokens_total": 462, "latency_total": 0.46373432600012165, "latency_per_module": {"eig": [0.24305608099984966, 0.20721939600025507], "scorer": 0.013458849000016926}, "tokens_per_module": {"eig": [{"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 88, "tokens_out": 13, "tokens_total": 101}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Rachel burned her hand. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She cut her finger when chopping vegetables. happen?\nAnswer:"], "scorer": "Observation: Rachel was cooking dinner. Then she pulled herself together and took care of the cut.\nQuestion: Did She cut her finger when chopping vegetables. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Rachel was cooking dinner. Then she pulled herself together and took care of the cut.", "hypotheses": ["Rachel burned her hand.", "She cut her finger when chopping vegetables."]}
{"example_id": "178", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Sam got lots of complaints about her clothing. happen?", "a": "no", "prior_probs": [0.5855762530292931, 0.414423746970707], "posterior_probs": [0.6439300762833287, 0.35606992371667134], "prior_entropy": 0.6784282315920453, "posterior_entropy": 0.6511233970641828, "delta_entropy": 0.027304834527862587, "eig_estimate": 0.028201894018980166, "pred": 0, "gold": 1, "confidence": 0.6439300762833287, "accuracy": 0, "tokens_in": 130, "tokens_out": 339, "tokens_total": 469, "latency_total": 0.42835592000028555, "latency_per_module": {"eig": [0.20345941499999753, 0.21242365800026164], "scorer": 0.012472847000026377}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}, {"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}], "scorer": {"tokens_in": 82, "tokens_out": 20, "tokens_total": 102}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Sam got lots of complaints about her clothing. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She decided to go out and buy and entire outfit. happen?\nAnswer:"], "scorer": "Observation: Sam loved striped clothes. She began to wear stripes every day!\nQuestion: Did Sam got lots of complaints about her clothing. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sam loved striped clothes. She began to wear stripes every day!", "hypotheses": ["Sam got lots of complaints about her clothing.", "She decided to go out and buy and entire outfit."]}
{"example_id": "179", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jimmy talked on the phone and wouldn't stop, delighting the professor. happen?", "a": "yes", "prior_probs": [0.5009149302761192, 0.49908506972388084], "posterior_probs": [0.3412686163823664, 0.6587313836176336], "prior_entropy": 0.6931455063621907, "posterior_entropy": 0.6418733599665647, "delta_entropy": 0.05127214639562605, "eig_estimate": 0.05065761499827128, "pred": 1, "gold": 1, "confidence": 0.6587313836176336, "accuracy": 1, "tokens_in": 162, "tokens_out": 341, "tokens_total": 503, "latency_total": 0.4333383100001811, "latency_per_module": {"eig": [0.20989937499962252, 0.2095912180002415], "scorer": 0.013847717000317061}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}, {"tokens_in": 29, "tokens_out": 160, "tokens_total": 189}], "scorer": {"tokens_in": 112, "tokens_out": 22, "tokens_total": 134}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He ended up getting in trouble. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jimmy talked on the phone and wouldn't stop, delighting the professor. happen?\nAnswer:"], "scorer": "Observation: Jimmy's phone ringed in class. As a result, she called the security guard to take him away.\nQuestion: Did Jimmy talked on the phone and wouldn't stop, delighting the professor. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jimmy's phone ringed in class. As a result, she called the security guard to take him away.", "hypotheses": ["He ended up getting in trouble.", "Jimmy talked on the phone and wouldn't stop, delighting the professor."]}
{"example_id": "180", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did She severely undercooked the chicken and badly burned the potatoes. happen?", "a": "no", "prior_probs": [0.457286767228584, 0.542713232771416], "posterior_probs": [0.6607183873691748, 0.3392816126308252], "prior_entropy": 0.6894938890322678, "posterior_entropy": 0.6405578253862159, "delta_entropy": 0.048936063646051964, "eig_estimate": 0.04924033748379257, "pred": 0, "gold": 1, "confidence": 0.6607183873691748, "accuracy": 0, "tokens_in": 152, "tokens_out": 334, "tokens_total": 486, "latency_total": 0.42880057300135377, "latency_per_module": {"eig": [0.2104189210003824, 0.20561609900050826], "scorer": 0.012765553000463115}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}, {"tokens_in": 20, "tokens_out": 157, "tokens_total": 177}], "scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did She severely undercooked the chicken and badly burned the potatoes. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Kelly's dinner was tasty. happen?\nAnswer:"], "scorer": "Observation: Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.\nQuestion: Did She severely undercooked the chicken and badly burned the potatoes. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Kelly was cooking dinner for her parents for the first time. Everyone laughed and they joked about it for years.", "hypotheses": ["She severely undercooked the chicken and badly burned the potatoes.", "Kelly's dinner was tasty."]}
{"example_id": "181", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Amber talked to her academic advisor. happen?", "a": "no", "prior_probs": [0.5152120120544086, 0.4847879879455914], "posterior_probs": [0.5538230921646585, 0.4461769078353414], "prior_entropy": 0.6926842985121199, "posterior_entropy": 0.6873420883008954, "delta_entropy": 0.005342210211224496, "eig_estimate": 0.005382612927422348, "pred": 0, "gold": 1, "confidence": 0.5538230921646585, "accuracy": 0, "tokens_in": 126, "tokens_out": 336, "tokens_total": 462, "latency_total": 0.4332396309982869, "latency_per_module": {"eig": [0.21489190499960387, 0.2036613619993659], "scorer": 0.014686363999317109}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 84, "tokens_out": 16, "tokens_total": 100}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Amber talked to her academic advisor. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Amber talked to her psychic advisor. happen?\nAnswer:"], "scorer": "Observation: Amber was scared about her future. She was no longer worried about her future.\nQuestion: Did Amber talked to her academic advisor. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber was scared about her future. She was no longer worried about her future.", "hypotheses": ["Amber talked to her academic advisor.", "Amber talked to her psychic advisor."]}
{"example_id": "182", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Terry practiced for a long time. happen?", "a": "no", "prior_probs": [0.5951790614049827, 0.40482093859501733], "posterior_probs": [0.6306803666696139, 0.36931963333038603], "prior_entropy": 0.6749180336440842, "posterior_entropy": 0.6585925849294575, "delta_entropy": 0.01632544871462671, "eig_estimate": 0.017813533534023505, "pred": 0, "gold": 1, "confidence": 0.6306803666696139, "accuracy": 0, "tokens_in": 137, "tokens_out": 325, "tokens_total": 462, "latency_total": 0.2463559610005177, "latency_per_module": {"eig": [0.11792546900051093, 0.12097217700011242], "scorer": 0.007458314999894355}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}, {"tokens_in": 20, "tokens_out": 153, "tokens_total": 173}], "scorer": {"tokens_in": 96, "tokens_out": 13, "tokens_total": 109}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Terry practiced for a long time. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did terry was so big. happen?\nAnswer:"], "scorer": "Observation: The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.\nQuestion: Did Terry practiced for a long time. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The day of the big game had arrived. Terry scored 2 goals that day and got his team the victory.", "hypotheses": ["Terry practiced for a long time.", "terry was so big."]}
{"example_id": "183", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Sara was outside looking at the neighbor's dog. happen?", "a": "no", "prior_probs": [0.5548732862741387, 0.4451267137258613], "posterior_probs": [0.5708015972471827, 0.42919840275281723], "prior_entropy": 0.6871128780618596, "posterior_entropy": 0.6830876714556835, "delta_entropy": 0.004025206606176113, "eig_estimate": 0.004150846670123975, "pred": 0, "gold": 1, "confidence": 0.5708015972471827, "accuracy": 0, "tokens_in": 148, "tokens_out": 344, "tokens_total": 492, "latency_total": 0.23623542199948133, "latency_per_module": {"eig": [0.1167832940000153, 0.11302655599956779], "scorer": 0.006425571999898239}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 24, "tokens_out": 162, "tokens_total": 186}], "scorer": {"tokens_in": 100, "tokens_out": 22, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Sara was outside looking at the neighbor's dog. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Sara was outside looking at the neighbor's underwear. happen?\nAnswer:"], "scorer": "Observation: Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.\nQuestion: Did Sara was outside looking at the neighbor's dog. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Sara wanted a new puppy really badly. A raccoon sprang out and Sara never wanted an animal again.", "hypotheses": ["Sara was outside looking at the neighbor's dog.", "Sara was outside looking at the neighbor's underwear."]}
{"example_id": "184", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did The librarian noticed Monica and her boyfriend were just hanging out reading. happen?", "a": "yes", "prior_probs": [0.3762754695839405, 0.6237245304160595], "posterior_probs": [0.2777498203381742, 0.7222501796618258], "prior_entropy": 0.6622113117156057, "posterior_entropy": 0.5908155306719938, "delta_entropy": 0.07139578104361188, "eig_estimate": 0.0701527665559544, "pred": 1, "gold": 1, "confidence": 0.7222501796618258, "accuracy": 1, "tokens_in": 161, "tokens_out": 346, "tokens_total": 507, "latency_total": 0.2315421849998529, "latency_per_module": {"eig": [0.11711755299984361, 0.10489736999988963], "scorer": 0.009527262000119663}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 159, "tokens_total": 186}, {"tokens_in": 28, "tokens_out": 160, "tokens_total": 188}], "scorer": {"tokens_in": 106, "tokens_out": 27, "tokens_total": 133}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Children brought food and left their trash laying on the reading table. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The librarian noticed Monica and her boyfriend were just hanging out reading. happen?\nAnswer:"], "scorer": "Observation: Monica was at the library with her boyfriend. She kicked them out because they were loitering.\nQuestion: Did The librarian noticed Monica and her boyfriend were just hanging out reading. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Monica was at the library with her boyfriend. She kicked them out because they were loitering.", "hypotheses": ["Children brought food and left their trash laying on the reading table.", "The librarian noticed Monica and her boyfriend were just hanging out reading."]}
{"example_id": "185", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jen's mother let her put on her own makeup but Jen's friends complimented her. happen?", "a": "yes", "prior_probs": [0.4431772394760715, 0.5568227605239285], "posterior_probs": [0.32102118613651204, 0.6789788138634879], "prior_entropy": 0.6866755555945623, "posterior_entropy": 0.6276368040432336, "delta_entropy": 0.05903875155132865, "eig_estimate": 0.058944877037194796, "pred": 1, "gold": 1, "confidence": 0.6789788138634879, "accuracy": 1, "tokens_in": 166, "tokens_out": 350, "tokens_total": 516, "latency_total": 0.22188160299992887, "latency_per_module": {"eig": [0.11011522199987667, 0.10439270599999873], "scorer": 0.007373675000053481}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 32, "tokens_out": 160, "tokens_total": 192}], "scorer": {"tokens_in": 108, "tokens_out": 30, "tokens_total": 138}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jen applied her mother's makeup and looked like a clown. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Jen's mother let her put on her own makeup but Jen's friends complimented her. happen?\nAnswer:"], "scorer": "Observation: When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.\nQuestion: Did Jen's mother let her put on her own makeup but Jen's friends complimented her. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "When Jen is 10 years old she wants to wear makeup. Jen learns her lesson.", "hypotheses": ["Jen applied her mother's makeup and looked like a clown.", "Jen's mother let her put on her own makeup but Jen's friends complimented her."]}
{"example_id": "186", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Ted got sick of being woken up, so he stayed up all night to moved out of his house. happen?", "a": "yes", "prior_probs": [0.42914630227747175, 0.5708536977225283], "posterior_probs": [0.2753362755469118, 0.7246637244530882], "prior_entropy": 0.6830728109033353, "posterior_entropy": 0.5884944795371039, "delta_entropy": 0.0945783313662314, "eig_estimate": 0.0943225610114375, "pred": 1, "gold": 1, "confidence": 0.7246637244530882, "accuracy": 1, "tokens_in": 188, "tokens_out": 351, "tokens_total": 539, "latency_total": 0.22535486299966578, "latency_per_module": {"eig": [0.11112409500037757, 0.10714766000000964], "scorer": 0.007083107999278582}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 36, "tokens_out": 159, "tokens_total": 195}], "scorer": {"tokens_in": 128, "tokens_out": 32, "tokens_total": 160}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did He would run outside but nobody would be there. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Ted got sick of being woken up, so he stayed up all night to moved out of his house. happen?\nAnswer:"], "scorer": "Observation: Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.\nQuestion: Did Ted got sick of being woken up, so he stayed up all night to moved out of his house. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Every night the alarm would go off at Ted's house. It turns out a stray cat set off the alarm.", "hypotheses": ["He would run outside but nobody would be there.", "Ted got sick of being woken up, so he stayed up all night to moved out of his house."]}
{"example_id": "187", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Neil asked the locals where to find it. happen?", "a": "yes", "prior_probs": [0.4651342558008747, 0.5348657441991254], "posterior_probs": [0.3927171620210196, 0.6072828379789804], "prior_entropy": 0.6907139661688809, "posterior_entropy": 0.669948002560727, "delta_entropy": 0.02076596360815397, "eig_estimate": 0.020715197526478834, "pred": 1, "gold": 1, "confidence": 0.6072828379789804, "accuracy": 1, "tokens_in": 153, "tokens_out": 335, "tokens_total": 488, "latency_total": 0.21527137199973367, "latency_per_module": {"eig": [0.10780168099972798, 0.10144206099994335], "scorer": 0.006027630000062345}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 160, "tokens_total": 180}, {"tokens_in": 23, "tokens_out": 160, "tokens_total": 183}], "scorer": {"tokens_in": 110, "tokens_out": 15, "tokens_total": 125}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did They saw the monster themselves. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Neil asked the locals where to find it. happen?\nAnswer:"], "scorer": "Observation: Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!\nQuestion: Did Neil asked the locals where to find it. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Neil had tried to see the Loch Ness Monster on his trip to Scotland. They said they did not believe in the monster at all!", "hypotheses": ["They saw the monster themselves.", "Neil asked the locals where to find it."]}
{"example_id": "188", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did It was raining and I did not want to get wet. happen?", "a": "yes", "prior_probs": [0.3994401099647912, 0.6005598900352088], "posterior_probs": [0.3370015561987101, 0.6629984438012899], "prior_entropy": 0.672783997955092, "posterior_entropy": 0.6390265660200556, "delta_entropy": 0.03375743193503644, "eig_estimate": 0.03333938077635525, "pred": 1, "gold": 1, "confidence": 0.6629984438012899, "accuracy": 1, "tokens_in": 152, "tokens_out": 337, "tokens_total": 489, "latency_total": 0.21244848199967237, "latency_per_module": {"eig": [0.10289751599975716, 0.10272287899988441], "scorer": 0.006828087000030791}, "tokens_per_module": {"eig": [{"tokens_in": 20, "tokens_out": 159, "tokens_total": 179}, {"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}], "scorer": {"tokens_in": 106, "tokens_out": 18, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did It was a tornado outside. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did It was raining and I did not want to get wet. happen?\nAnswer:"], "scorer": "Observation: I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.\nQuestion: Did It was raining and I did not want to get wet. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "I woke up one morning and went outside to grab the newspaper. I went outside and quickly grabbed my newspaper.", "hypotheses": ["It was a tornado outside.", "It was raining and I did not want to get wet."]}
{"example_id": "189", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Jim's wife knew he was tired and made coffee. happen?", "a": "no", "prior_probs": [0.5614921399756847, 0.4385078600243153], "posterior_probs": [0.6450461098131087, 0.3549538901868913], "prior_entropy": 0.6855654335802226, "posterior_entropy": 0.650459471071086, "delta_entropy": 0.03510596250913656, "eig_estimate": 0.035777118940764116, "pred": 0, "gold": 1, "confidence": 0.6450461098131087, "accuracy": 0, "tokens_in": 152, "tokens_out": 336, "tokens_total": 488, "latency_total": 0.2110486490009862, "latency_per_module": {"eig": [0.10388247700029751, 0.10095171900047717], "scorer": 0.006214453000211506}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 19, "tokens_out": 160, "tokens_total": 179}], "scorer": {"tokens_in": 108, "tokens_out": 16, "tokens_total": 124}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Jim's wife knew he was tired and made coffee. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did jim made breakfast. happen?\nAnswer:"], "scorer": "Observation: Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.\nQuestion: Did Jim's wife knew he was tired and made coffee. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Jim was incredibly tired one morning. After he woke up, he saw a steaming, boiling pot on the stove.", "hypotheses": ["Jim's wife knew he was tired and made coffee.", "jim made breakfast."]}
{"example_id": "190", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Popped a tire and spent their picnic time waiting for a tow-truck. happen?", "a": "yes", "prior_probs": [0.6073004646067773, 0.3926995353932227], "posterior_probs": [0.2892233631183233, 0.7107766368816768], "prior_entropy": 0.6699403183745066, "posterior_entropy": 0.6014548288383141, "delta_entropy": 0.06848548953619249, "eig_estimate": 0.06884863912668934, "pred": 1, "gold": 1, "confidence": 0.7107766368816768, "accuracy": 1, "tokens_in": 159, "tokens_out": 344, "tokens_total": 503, "latency_total": 0.21298795900020195, "latency_per_module": {"eig": [0.10103490500023327, 0.10556279400043422], "scorer": 0.0063902599995344644}, "tokens_per_module": {"eig": [{"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}, {"tokens_in": 31, "tokens_out": 160, "tokens_total": 191}], "scorer": {"tokens_in": 106, "tokens_out": 25, "tokens_total": 131}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did After the picnic, it started raining. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Popped a tire and spent their picnic time waiting for a tow-truck. happen?\nAnswer:"], "scorer": "Observation: The family prepared the food and packed it away. The family had a horrible day.\nQuestion: Did Popped a tire and spent their picnic time waiting for a tow-truck. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "The family prepared the food and packed it away. The family had a horrible day.", "hypotheses": ["After the picnic, it started raining.", "Popped a tire and spent their picnic time waiting for a tow-truck."]}
{"example_id": "191", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Tim was able to break through the window. happen?", "a": "no", "prior_probs": [0.5497691534768302, 0.4502308465231699], "posterior_probs": [0.6053511541023504, 0.39464884589764965], "prior_entropy": 0.6881850301904786, "posterior_entropy": 0.6707822147272394, "delta_entropy": 0.017402815463239185, "eig_estimate": 0.017403760946878566, "pred": 0, "gold": 1, "confidence": 0.6053511541023504, "accuracy": 0, "tokens_in": 133, "tokens_out": 336, "tokens_total": 469, "latency_total": 0.21849775400096405, "latency_per_module": {"eig": [0.10422912100057147, 0.1051637420005136], "scorer": 0.00910489099987899}, "tokens_per_module": {"eig": [{"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}, {"tokens_in": 22, "tokens_out": 160, "tokens_total": 182}], "scorer": {"tokens_in": 88, "tokens_out": 17, "tokens_total": 105}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Tim was able to break through the window. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Tim lost his keys to the house. happen?\nAnswer:"], "scorer": "Observation: Tim never locked his bathroom window. Tim was glad he kept the window unlocked.\nQuestion: Did Tim was able to break through the window. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Tim never locked his bathroom window. Tim was glad he kept the window unlocked.", "hypotheses": ["Tim was able to break through the window.", "Tim lost his keys to the house."]}
{"example_id": "192", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did There is an old dog on the farm who has lost its mind and barks all day. happen?", "a": "yes", "prior_probs": [0.4674719487572201, 0.5325280512427799], "posterior_probs": [0.30614585980201975, 0.6938541401979802], "prior_entropy": 0.6910295370954334, "posterior_entropy": 0.6159820897794255, "delta_entropy": 0.07504744731600788, "eig_estimate": 0.07470250896467698, "pred": 1, "gold": 1, "confidence": 0.6938541401979802, "accuracy": 1, "tokens_in": 182, "tokens_out": 352, "tokens_total": 534, "latency_total": 0.24377765899953374, "latency_per_module": {"eig": [0.12510394099990663, 0.11165225100012321], "scorer": 0.0070214669995039}, "tokens_per_module": {"eig": [{"tokens_in": 29, "tokens_out": 159, "tokens_total": 188}, {"tokens_in": 33, "tokens_out": 159, "tokens_total": 192}], "scorer": {"tokens_in": 120, "tokens_out": 34, "tokens_total": 154}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did The people that live on the old farm has a dog that fears nothing. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did There is an old dog on the farm who has lost its mind and barks all day. happen?\nAnswer:"], "scorer": "Observation: A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.\nQuestion: Did There is an old dog on the farm who has lost its mind and barks all day. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A friend of the family has an old farm. No sane dog chases a pack of wild coyotes.", "hypotheses": ["The people that live on the old farm has a dog that fears nothing.", "There is an old dog on the farm who has lost its mind and barks all day."]}
{"example_id": "193", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did he went inside and got a part time job. happen?", "a": "yes", "prior_probs": [0.5402261165773485, 0.4597738834226515], "posterior_probs": [0.41492685900844456, 0.5850731409915554], "prior_entropy": 0.6899073994065836, "posterior_entropy": 0.6786016396959837, "delta_entropy": 0.011305759710599883, "eig_estimate": 0.012320210249163254, "pred": 1, "gold": 1, "confidence": 0.5850731409915554, "accuracy": 1, "tokens_in": 150, "tokens_out": 340, "tokens_total": 490, "latency_total": 0.21944795900071767, "latency_per_module": {"eig": [0.11140972700013663, 0.10213173300053313], "scorer": 0.005906499000047916}, "tokens_per_module": {"eig": [{"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 102, "tokens_out": 20, "tokens_total": 122}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Oren got a part time job delivering pizza. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did he went inside and got a part time job. happen?\nAnswer:"], "scorer": "Observation: Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.\nQuestion: Did he went inside and got a part time job. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Oren needs to earn extra money for school. Oren feels great that he will be earning extra money.", "hypotheses": ["Oren got a part time job delivering pizza.", "he went inside and got a part time job."]}
{"example_id": "194", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Beth went out and broke her phone with no ride home. happen?", "a": "yes", "prior_probs": [0.5162868080307631, 0.48371319196923696], "posterior_probs": [0.3511353152683664, 0.6488646847316336], "prior_entropy": 0.6926165664692046, "posterior_entropy": 0.648146612270792, "delta_entropy": 0.0444699541984126, "eig_estimate": 0.04443874948795297, "pred": 1, "gold": 1, "confidence": 0.6488646847316336, "accuracy": 1, "tokens_in": 165, "tokens_out": 339, "tokens_total": 504, "latency_total": 0.30987138300042716, "latency_per_module": {"eig": [0.15672946500035323, 0.14553199999954813], "scorer": 0.007609918000525795}, "tokens_per_module": {"eig": [{"tokens_in": 21, "tokens_out": 159, "tokens_total": 180}, {"tokens_in": 26, "tokens_out": 159, "tokens_total": 185}], "scorer": {"tokens_in": 118, "tokens_out": 21, "tokens_total": 139}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Beth passed out from overeating. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Beth went out and broke her phone with no ride home. happen?\nAnswer:"], "scorer": "Observation: Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.\nQuestion: Did Beth went out and broke her phone with no ride home. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Beth was a model in a fashion show at her school. Beth missed the show, and was unable to tell anyone she wasn't coming.", "hypotheses": ["Beth passed out from overeating.", "Beth went out and broke her phone with no ride home."]}
{"example_id": "195", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview. happen?", "a": "yes", "prior_probs": [0.5288894510173028, 0.4711105489826972], "posterior_probs": [0.29496572754376327, 0.7050342724562367], "prior_entropy": 0.6914770498116267, "posterior_entropy": 0.6065382361201079, "delta_entropy": 0.08493881369151879, "eig_estimate": 0.08486782785093956, "pred": 1, "gold": 1, "confidence": 0.7050342724562367, "accuracy": 1, "tokens_in": 200, "tokens_out": 357, "tokens_total": 557, "latency_total": 0.24211090500102728, "latency_per_module": {"eig": [0.11978987000020425, 0.11501532400052383], "scorer": 0.007305711000299198}, "tokens_per_module": {"eig": [{"tokens_in": 26, "tokens_out": 160, "tokens_total": 186}, {"tokens_in": 40, "tokens_out": 159, "tokens_total": 199}], "scorer": {"tokens_in": 134, "tokens_out": 38, "tokens_total": 172}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Everyone realized that the power was out for a long time. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview. happen?\nAnswer:"], "scorer": "Observation: Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.\nQuestion: Did When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Overnight there was a huge lightning storm that knocked out the power. Everyone was late and had to hurry.", "hypotheses": ["Everyone realized that the power was out for a long time.", "When I awoke, I rolled over and saw the alarm clock flashing 6 o'clock and I knew I'd missed my interview."]}
{"example_id": "196", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did They were both very shy, but were attracted to eachother without even knowing!. happen?", "a": "no", "prior_probs": [0.564453773628524, 0.435546226371476], "posterior_probs": [0.7247751687444844, 0.2752248312555156], "prior_entropy": 0.6848154375403865, "posterior_entropy": 0.5883866021491069, "delta_entropy": 0.09642883539127967, "eig_estimate": 0.0971496997293663, "pred": 0, "gold": 1, "confidence": 0.7247751687444844, "accuracy": 0, "tokens_in": 161, "tokens_out": 342, "tokens_total": 503, "latency_total": 0.23476510899945424, "latency_per_module": {"eig": [0.11773734999951557, 0.11032516199975362], "scorer": 0.006702597000185051}, "tokens_per_module": {"eig": [{"tokens_in": 30, "tokens_out": 159, "tokens_total": 189}, {"tokens_in": 21, "tokens_out": 160, "tokens_total": 181}], "scorer": {"tokens_in": 110, "tokens_out": 23, "tokens_total": 133}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did They were both very shy, but were attracted to eachother without even knowing!. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did She went under to say hello. happen?\nAnswer:"], "scorer": "Observation: Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!\nQuestion: Did They were both very shy, but were attracted to eachother without even knowing!. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amelia wanted to be friends with Kiku. They had both wanted to be friends with each other!", "hypotheses": ["They were both very shy, but were attracted to eachother without even knowing!.", "She went under to say hello."]}
{"example_id": "197", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did the man failed to get a job, until recently. happen?", "a": "no", "prior_probs": [0.5572402122655649, 0.44275978773443514], "posterior_probs": [0.6187027058522457, 0.38129729414775426], "prior_entropy": 0.6865799077622886, "posterior_entropy": 0.6646956445053707, "delta_entropy": 0.021884263256917946, "eig_estimate": 0.02153389639724261, "pred": 0, "gold": 1, "confidence": 0.6187027058522457, "accuracy": 0, "tokens_in": 159, "tokens_out": 338, "tokens_total": 497, "latency_total": 0.2278532529999211, "latency_per_module": {"eig": [0.11645369599955302, 0.10472316600043996], "scorer": 0.006676390999928117}, "tokens_per_module": {"eig": [{"tokens_in": 25, "tokens_out": 160, "tokens_total": 185}, {"tokens_in": 22, "tokens_out": 159, "tokens_total": 181}], "scorer": {"tokens_in": 112, "tokens_out": 19, "tokens_total": 131}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did the man failed to get a job, until recently. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did The man eventually gave up on looking. happen?\nAnswer:"], "scorer": "Observation: A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.\nQuestion: Did the man failed to get a job, until recently. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "A man was thrown out of every residence he could find. Now he is homeless, and society at large is poorer for it.", "hypotheses": ["the man failed to get a job, until recently.", "The man eventually gave up on looking."]}
{"example_id": "198", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake. happen?", "a": "no", "prior_probs": [0.5240675038940131, 0.47593249610598687], "posterior_probs": [0.7034604471341279, 0.296539552865872], "prior_entropy": 0.6919882432894742, "posterior_entropy": 0.6079036983005313, "delta_entropy": 0.08408454498894291, "eig_estimate": 0.08463175361886824, "pred": 0, "gold": 1, "confidence": 0.7034604471341279, "accuracy": 0, "tokens_in": 181, "tokens_out": 351, "tokens_total": 532, "latency_total": 0.2818330550007886, "latency_per_module": {"eig": [0.1266686409999238, 0.1467423680005595], "scorer": 0.008422046000305272}, "tokens_per_module": {"eig": [{"tokens_in": 35, "tokens_out": 158, "tokens_total": 193}, {"tokens_in": 24, "tokens_out": 160, "tokens_total": 184}], "scorer": {"tokens_in": 122, "tokens_out": 33, "tokens_total": 155}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Chad didn't eat cause he had the flu. happen?\nAnswer:"], "scorer": "Observation: Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.\nQuestion: Did Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake. happen?\nAnswer: no\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Chad was recently visiting his sister for Christmas. Chad decided he would not be eating the piece of cake.", "hypotheses": ["Chad ate so much at dinner that he was getting heartburn, then his sister brought out a cake.", "Chad didn't eat cause he had the flu."]}
{"example_id": "199", "dataset": "art", "method": "eig_ia", "asked": true, "q": "Did Amber liked having a book in her hand. happen?", "a": "yes", "prior_probs": [0.38821194630071754, 0.6117880536992824], "posterior_probs": [0.34868195339681995, 0.65131804660318], "prior_entropy": 0.667941545366203, "posterior_entropy": 0.6466268961629492, "delta_entropy": 0.021314649203253766, "eig_estimate": 0.02035582950554702, "pred": 1, "gold": 1, "confidence": 0.65131804660318, "accuracy": 1, "tokens_in": 130, "tokens_out": 342, "tokens_total": 472, "latency_total": 0.22520217599867465, "latency_per_module": {"eig": [0.11702654599957896, 0.10180765199947928], "scorer": 0.006367977999616414}, "tokens_per_module": {"eig": [{"tokens_in": 27, "tokens_out": 160, "tokens_total": 187}, {"tokens_in": 23, "tokens_out": 159, "tokens_total": 182}], "scorer": {"tokens_in": 80, "tokens_out": 23, "tokens_total": 103}}, "final_answer": "", "em": 0.0, "f1": 0.0, "prompts": {"eig": ["Answer yes or no.\nQuestion: Did Ambers friends wanted to see her at the library one night. happen?\nAnswer:", "Answer yes or no.\nQuestion: Did Amber liked having a book in her hand. happen?\nAnswer:"], "scorer": "Observation: Amber loves to read. She went to the library instead.\nQuestion: Did Amber liked having a book in her hand. happen?\nAnswer: yes\nHypothesis: "}, "model_ids": {"question": "distilgpt2", "answer": "distilgpt2", "scorer": "distilgpt2"}, "decoding_params": {"question": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "answer": {"max_new_tokens": 32, "temperature": 0.7, "top_p": 0.9, "do_sample": true}, "scorer": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": false}}, "seed": 42, "observation": "Amber loves to read. She went to the library instead.", "hypotheses": ["Ambers friends wanted to see her at the library one night.", "Amber liked having a book in her hand."]}
